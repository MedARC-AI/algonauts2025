{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1943a4b4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-29T14:37:28.418902Z",
     "iopub.status.busy": "2025-04-29T14:37:28.418624Z",
     "iopub.status.idle": "2025-04-29T14:37:39.542451Z",
     "shell.execute_reply": "2025-04-29T14:37:39.541489Z"
    },
    "papermill": {
     "duration": 11.134805,
     "end_time": "2025-04-29T14:37:39.544129",
     "exception": false,
     "start_time": "2025-04-29T14:37:28.409324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        pass\n",
    "        # print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7014452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:37:39.557068Z",
     "iopub.status.busy": "2025-04-29T14:37:39.556693Z",
     "iopub.status.idle": "2025-04-29T14:37:39.561905Z",
     "shell.execute_reply": "2025-04-29T14:37:39.561126Z"
    },
    "papermill": {
     "duration": 0.012314,
     "end_time": "2025-04-29T14:37:39.563264",
     "exception": false,
     "start_time": "2025-04-29T14:37:39.550950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path(\"/home/mihirneal/Developer/algonauts/\")\n",
    "os.path.exists(root_dir / \"algonauts_2025.competitors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace91a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:37:39.574352Z",
     "iopub.status.busy": "2025-04-29T14:37:39.574140Z",
     "iopub.status.idle": "2025-04-29T14:37:43.936860Z",
     "shell.execute_reply": "2025-04-29T14:37:43.936145Z"
    },
    "papermill": {
     "duration": 4.369843,
     "end_time": "2025-04-29T14:37:43.938386",
     "exception": false,
     "start_time": "2025-04-29T14:37:39.568543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import ast \n",
    "\n",
    "class HuggingFaceFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A feature extractor for Hugging Face (or any PyTorch) models that captures\n",
    "    intermediate activations from any layer specified by exact name or glob pattern.\n",
    "\n",
    "    Example usage:\n",
    "        from transformers import BertModel\n",
    "        model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        # Specify layers (using glob patterns is supported)\n",
    "        layers_to_extract = [\"encoder.layer.*.output\"]\n",
    "\n",
    "        # Using the extractor as a context manager ensures hooks are removed automatically.\n",
    "        with HuggingFaceFeatureExtractor(model, layers_to_extract, detach=True) as extractor:\n",
    "            # Perform a forward pass as usual\n",
    "            outputs = model(input_ids, attention_mask=mask)\n",
    "            # Get a copy of the extracted features\n",
    "            features = extractor.features\n",
    "            # Now 'features' is a dict mapping layer names to their activation tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, layers: List[str], detach: bool = True):\n",
    "        self.model = model\n",
    "        self.detach = detach\n",
    "        # Expand layer patterns into full module names\n",
    "        self.layers = self._expand_layers(model, layers)\n",
    "        self._features: Dict[str, Any] = {}\n",
    "        self._handles: Dict[str, Any] = {}\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward hooks on each specified layer.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            sub_module = self.model.get_submodule(layer)\n",
    "            handle = sub_module.register_forward_hook(self._make_hook(layer))\n",
    "            self._handles[layer] = handle\n",
    "\n",
    "    def _make_hook(self, layer_name: str):\n",
    "        def hook(module: nn.Module, inputs: Tuple[Any, ...], output: Any):\n",
    "            # Optionally detach to break the graph and save memory.\n",
    "            self._features[layer_name] = output.detach() if self.detach else output\n",
    "        return hook\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clear the stored features before a new forward pass.\"\"\"\n",
    "        self._features.clear()\n",
    "\n",
    "    @property\n",
    "    def features(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a copy of the captured features.\"\"\"\n",
    "        return dict(self._features)\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Run the model forward. This automatically clears previous features,\n",
    "        then performs a forward pass, capturing intermediate activations.\n",
    "        Returns the model's original output.\n",
    "        \"\"\"\n",
    "        self.clear()\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for handle in self._handles.values():\n",
    "            handle.remove()\n",
    "        self._handles.clear()\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Enter context: hooks are already registered.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"Exit context: remove all hooks.\"\"\"\n",
    "        self.remove_hooks()\n",
    "\n",
    "    @staticmethod\n",
    "    def _expand_layers(model: nn.Module, layers: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Expand a list of layer names and/or glob patterns to all matching module names\n",
    "        in the given model. Raises an error if a specified name or pattern doesn't match.\n",
    "        \"\"\"\n",
    "        all_layers = [name for name, _ in model.named_modules() if name]  # skip the root module ''\n",
    "        all_layers_set = set(all_layers)\n",
    "        expanded = []\n",
    "        special_chars = set(\"*?[]\")\n",
    "        for layer in layers:\n",
    "            if not any(char in layer for char in special_chars):\n",
    "                if layer not in all_layers_set:\n",
    "                    raise ValueError(f\"Layer '{layer}' not found in the model.\")\n",
    "                expanded.append(layer)\n",
    "            else:\n",
    "                matches = fnmatch.filter(all_layers, layer)\n",
    "                if not matches:\n",
    "                    raise ValueError(f\"No layers match the pattern '{layer}'.\")\n",
    "                expanded.extend(matches)\n",
    "        return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c4fe1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:37:43.950304Z",
     "iopub.status.busy": "2025-04-29T14:37:43.949940Z",
     "iopub.status.idle": "2025-04-29T14:37:45.926998Z",
     "shell.execute_reply": "2025-04-29T14:37:45.926309Z"
    },
    "papermill": {
     "duration": 1.984749,
     "end_time": "2025-04-29T14:37:45.928658",
     "exception": false,
     "start_time": "2025-04-29T14:37:43.943909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, List, Callable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_transcript(\n",
    "    path: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a transcript file (TSV) into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the transcript file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the transcript data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='\\t')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading transcript from {path}: {e}\")\n",
    "\n",
    "\n",
    "def load_audio(\n",
    "    path: str,\n",
    "    sampling_rate: int = 48000,\n",
    "    stereo: bool = True\n",
    ") -> (torch.Tensor, int):\n",
    "    \"\"\"\n",
    "    Loads an audio file using torchaudio, converts the waveform to half precision,\n",
    "    optionally converts stereo audio to mono, resamples it to the specified sampling_rate\n",
    "    if needed, and returns the waveform and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the audio file.\n",
    "        sampling_rate (int): Desired sampling rate for the output waveform.\n",
    "        stereo (bool): If False, converts stereo audio to mono.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (waveform_fp16, sampling_rate) where waveform_fp16 is a tensor in float16.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set the backend to 'ffmpeg' if available\n",
    "        torchaudio.set_audio_backend(\"ffmpeg\")\n",
    "        waveform, orig_sr = torchaudio.load(path)\n",
    "        \n",
    "        # Convert to mono if stereo is False and the waveform has multiple channels\n",
    "        if not stereo and waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # Resample if original sample rate is different from the desired sampling rate\n",
    "        if orig_sr != sampling_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=sampling_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert the waveform to half precision (float16)\n",
    "        waveform_fp16 = waveform.half()\n",
    "        del waveform\n",
    "        return waveform_fp16, sampling_rate\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading audio from {path}: {e}\")\n",
    "\n",
    "\n",
    "def load_video(\n",
    "    path: str,\n",
    "    resolution: Tuple[int, int] = (224, 224),\n",
    "    tensor_dtype: torch.dtype = torch.float16,\n",
    "    verbose: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loads a video file, reads its frames, converts each frame from BGR to RGB,\n",
    "    resizes to 224x224, and returns a tensor containing all frames.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape [num_frames, 3, 224, 224] containing the video frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open video file: {}\".format(path))\n",
    "\n",
    "    # Get video FPS and calculate number of frames for 10 seconds\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    num_frames_to_read = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total number of frames in the video:\", cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(\"Original Resolution:\", (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "        print(\"FPS:\", fps)\n",
    "        print(\"Duration (seconds):\", num_frames_to_read / fps)\n",
    "        print(\"Target Resolution:\", resolution)\n",
    "\n",
    "    frames = torch.zeros(num_frames_to_read, 3, 224, 224, dtype=tensor_dtype)\n",
    "\n",
    "    for i in range(num_frames_to_read):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Optionally, convert the frame from BGR to RGB (if needed)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert the frame (numpy array) to a torch tensor and permute dimensions to [C, H, W]\n",
    "        frame_tensor = torch.from_numpy(frame_rgb).permute(2, 0, 1) \n",
    "        # Resize the frame to 224x224\n",
    "        frame_tensor = torch.nn.functional.interpolate(frame_tensor.unsqueeze(0), size=224, mode='bilinear', align_corners=False)\n",
    "        frames[i] = frame_tensor\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Read {len(frames)} frames.\")\n",
    "        print(f\"Frames shape: {frames.shape}\")\n",
    "\n",
    "    return frames, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a192024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:37:45.940726Z",
     "iopub.status.busy": "2025-04-29T14:37:45.940495Z",
     "iopub.status.idle": "2025-04-29T14:37:46.206785Z",
     "shell.execute_reply": "2025-04-29T14:37:46.206095Z"
    },
    "papermill": {
     "duration": 0.273927,
     "end_time": "2025-04-29T14:37:46.208271",
     "exception": false,
     "start_time": "2025-04-29T14:37:45.934344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def extract_features(\n",
    "    parts: List[str],\n",
    "    movies_base: str,\n",
    "    transcripts_base: str,\n",
    "    output_dir: str,\n",
    "    extraction_fn: Callable,\n",
    "    interval: int = 1.49,\n",
    "    verbose: bool = True,\n",
    "    modality: str = 'all',\n",
    "    past_context_in_seconds: int = 30,\n",
    "    splits_overlap: float = 0.5,\n",
    "    ignore_done = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts features from the specified parts of the dataset using the provided extraction function.\n",
    "\n",
    "    Parameters:\n",
    "        parts (List[str]): List of parts to extract features from. This is the subdirectory name under friends and movie10 folders.\n",
    "        movies_base (str): Path to the base directory containing movie files.\n",
    "        transcripts_base (str): Path to the base directory containing transcript files.\n",
    "        output_dir (str): Path to the output directory where features will be saved.\n",
    "        interval (int): Interval (in seconds) at which to extract features. Default is 1.49 seconds (the TR for the dataset).\n",
    "        extraction_fn (function): Function that extracts features from the stimuli. The function should take the following arguments:\n",
    "            - video: torch.Tensor containing video frames (num_frames, 3, 224, 224)\n",
    "            - audio: torch.Tensor containing audio waveform (2, num_samples)\n",
    "            - transcript: array containing strings of words (num_words,)\n",
    "            - verbose: bool indicating whether to print verbose output.\n",
    "            and should return a dictionary mapping layer names to extracted features as torch.Tensor.\n",
    "        verbose (bool): Whether to print verbose output.\n",
    "        modality (str): Modality to extract features from. Default is 'all'. Options are 'video', 'audio', 'transcript'.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    global video_section_g, audio_section_g, transcript_section_g\n",
    "\n",
    "    movies_base = Path(movies_base)\n",
    "    transcripts_base = Path(transcripts_base)\n",
    "\n",
    "    # Verify that the base directories exist.\n",
    "    if not movies_base.exists():\n",
    "        raise FileNotFoundError(f\"Movies directory not found: {movies_base}\")\n",
    "    if not transcripts_base.exists():\n",
    "        raise FileNotFoundError(f\"Transcripts directory not found: {transcripts_base}\")\n",
    "\n",
    "    # Iterate through all directories under movies_base.\n",
    "    for folder in movies_base.rglob('*'):\n",
    "        if folder.is_dir() and folder.name in parts:\n",
    "            # Iterate through mkv files in the matched directory.\n",
    "            for movie_file in folder.glob('*.mkv'):\n",
    "                # Compute the relative path from movies_base.\n",
    "                try:\n",
    "                    rel_folder = folder.relative_to(movies_base)\n",
    "                except ValueError:\n",
    "                    # Skip directories that are not under movies_base.\n",
    "                    continue\n",
    "\n",
    "                print(rel_folder)\n",
    "                if \"friends\" in str(rel_folder):\n",
    "                    # Build the corresponding transcript file path.\n",
    "                    transcript_file = transcripts_base / rel_folder / movie_file.with_suffix('.tsv').name\n",
    "                else:\n",
    "                    transcript_file = transcripts_base / rel_folder / f\"movie10_{movie_file.with_suffix('.tsv').name}\"\n",
    "\n",
    "                print(f\"Movie:      {movie_file}\")\n",
    "                print(f\"Transcript: {transcript_file}\")\n",
    "\n",
    "                if str(movie_file).split('/')[-1].split('.')[0] + '.h5' in ignore_done:\n",
    "                    continue\n",
    "\n",
    "                # Load video frames, audio waveform, and transcript.\n",
    "                video, audio, transcript, sample_rate, fps_video = None, None, None, None, None\n",
    "                if modality == 'all' or modality == 'video':\n",
    "                    video, fps_video = load_video(movie_file, verbose=verbose)\n",
    "                if modality == 'all' or modality == 'audio' or modality == 'video' or modality == 'transcript':\n",
    "                    audio, sample_rate = load_audio(movie_file)\n",
    "                if modality == 'all' or modality == 'transcript':\n",
    "                    transcript = load_transcript(transcript_file)\n",
    "\n",
    "                # round fps video\n",
    "                # if fps_video:\n",
    "                #     fps_video = round(fps_video)\n",
    "\n",
    "                if transcript is not None:\n",
    "                    transcript = resample_transcript(transcript, interval)\n",
    "                    \n",
    "                total_duration = audio.shape[1] / sample_rate\n",
    "                num_intervals_tr = int(total_duration // interval)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Total duration: {total_duration:.2f} seconds\")\n",
    "                    print(f\"Number of intervals: {num_intervals_tr}\")\n",
    "                    print(f\"Sample rate: {sample_rate}\")\n",
    "\n",
    "                # Create the output directory if it doesn't exist.\n",
    "                output_folder = Path(output_dir) / rel_folder\n",
    "                output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Create a h5 file to store the features.\n",
    "                output_file = output_folder / movie_file.with_suffix('.h5').name\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Output file: {output_file}\")\n",
    "\n",
    "                seconds_duration = int(audio.shape[1] / sample_rate)\n",
    "                num_splits = max(1, int(seconds_duration / past_context_in_seconds))\n",
    "                print(f\"Num splits: {num_splits}\")\n",
    "\n",
    "                total_iterations = math.ceil((num_splits / (1 - splits_overlap)) - 1)\n",
    "                # Create a HDF5 file to store the features.\n",
    "                with h5py.File(output_file, 'w') as f:\n",
    "                    features_datasets = {} \n",
    "                    # Extract features at each interval.\n",
    "                    fixed_distance_interval = math.ceil(num_intervals_tr / num_splits)\n",
    "                    for i in tqdm(range(total_iterations)):\n",
    "                        index = math.ceil((i * fixed_distance_interval) - (i * splits_overlap * fixed_distance_interval))\n",
    "\n",
    "                        if index >= num_intervals_tr:        # ← guard clause\n",
    "                            break\n",
    "                    \n",
    "                        # compute future_offset safely\n",
    "                        future_offset = min(fixed_distance_interval - 1,\n",
    "                                            num_intervals_tr - index - 1)\n",
    "                        \n",
    "                        # if i == total_iterations - 1:\n",
    "                        #     future_offset = num_intervals_tr - index - 1\n",
    "                        # else:\n",
    "                        #     future_offset = math.ceil(num_intervals_tr / num_splits) - 1\n",
    "\n",
    "                        end_index = index + future_offset\n",
    "\n",
    "                        # print(\"First \", index, future_offset)\n",
    "                        video_section, audio_section, transcript_section = extract_section(\n",
    "                            video, audio, transcript, interval, index, sample_rate, modality, fps_video, past_offset = 0, future_offset = future_offset, split_by_tr = True\n",
    "                        )\n",
    "                        \n",
    "                        # if i == total_iterations - 1:\n",
    "                        #     video_section_g = video_section\n",
    "                        #     audio_section_g = audio_section\n",
    "                        #     transcript_section_g = transcript_section\n",
    "\n",
    "                        # print(video_section.shape, audio_section.shape, len(transcript_section))\n",
    "                        # # skip the rest of the code for now\n",
    "                        # continue\n",
    "                        \n",
    "                        # if i == 100:\n",
    "                        #     video_section_g = video_section\n",
    "                        #     audio_section_g = audio_section\n",
    "                        #     transcript_section_g = transcript_section\n",
    "    \n",
    "                            \n",
    "                        #     # convert the video_section from int8 to float32\n",
    "                        #     video_section = video_section.int()\n",
    "                        #     # plot the first and the last frame of the video\n",
    "                        #     plt.imshow(video_section[0].permute(1, 2, 0).cpu().numpy())\n",
    "                        #     plt.show()\n",
    "                        #     plt.imshow(video_section[-1].permute(1, 2, 0).cpu().numpy())    \n",
    "                        #     plt.show()\n",
    "    \n",
    "    \n",
    "                        #     # display the audio section as an html audio element\n",
    "                        #     torchaudio.save(\"audio.wav\", audio_section.float(), sample_rate)\n",
    "                        #     from IPython.display import Audio\n",
    "                        #     Audio(\"audio.wav\")\n",
    "    \n",
    "    \n",
    "                        #     # print the transcript section\n",
    "                        #     print(transcript_section)\n",
    "                        \n",
    "                        #     # break for testing\n",
    "                        #     assert False\n",
    "                        \n",
    "                            \n",
    "        \n",
    "                        output_features = extraction_fn(video_section, audio_section, transcript_section, verbose)\n",
    "                        \n",
    "                        for layer_name, tensor in output_features.items():\n",
    "                            assert tensor.shape[0] == video_section.shape[0], f\"Error on layer: {layer_name}, the number of TRs of the output features should be the same as the number of TRs of the video section. Got {tensor.shape[0]} and {video_section.shape[0]}\"\n",
    "\n",
    "                        for layer_name, tensor in output_features.items():\n",
    "                            # Convert the tensor to a numpy array (on CPU) before storing.\n",
    "                            tensor_np = tensor.cpu().numpy() # shape [batch_size, feature_dim1, feature_dim2, ...]\n",
    "                            batch_size = tensor_np.shape[0]\n",
    "                            if layer_name not in features_datasets:\n",
    "                                # Create a new dataset and initialize it with the first interval's data.\n",
    "                                features_datasets[layer_name] = f.create_dataset(\n",
    "                                    layer_name,\n",
    "                                    # data=tensor_np[np.newaxis, ...],\n",
    "                                    data=tensor_np,\n",
    "                                    maxshape=(None,) + tensor_np.shape[1::],\n",
    "                                    dtype=np.float16,\n",
    "                                    chunks=True,\n",
    "                                )\n",
    "                            else:\n",
    "                                ds = features_datasets[layer_name]\n",
    "                                # ds.resize(ds.shape[0] + 1, axis=0)\n",
    "                                last_shape = ds.shape[0]\n",
    "                                ds.resize(end_index + 1, axis=0)\n",
    "                                # ds[-1] = tensor_np\n",
    "                                ds[last_shape:end_index] = tensor_np[-(end_index - last_shape)::]\n",
    "                                \n",
    "                        # if features_dataset is None:\n",
    "                        #     features_max_shape = (None,) + output_features.shape\n",
    "                        #     print(features_max_shape, output_features.shape)\n",
    "                        #     features_dataset = f.create_dataset(\n",
    "                        #         'features', \n",
    "                        #         shape= output_features.unsqueeze(0).shape,\n",
    "                        #         maxshape=features_max_shape,\n",
    "                        #         dtype=np.float16,\n",
    "                        #         chunks=True,    \n",
    "                        #     )\n",
    "                        # else:\n",
    "                        #     features_dataset.resize(features_dataset.shape[0] + 1, axis=0)\n",
    "                        #     features_dataset[-1] = output_features\n",
    "\n",
    "def resample_transcript(transcript: pd.DataFrame, new_interval: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-aggregates transcript data into new time intervals.\n",
    "    \n",
    "    Parameters:\n",
    "        transcript (pd.DataFrame): DataFrame with columns 'words_per_tr', 'onsets_per_tr', and 'durations_per_tr'.\n",
    "        new_interval (float): Desired interval in seconds for grouping.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: New DataFrame where each row aggregates words whose end time (onset + duration)\n",
    "                      falls within the same new interval. Intervals with no transcript words\n",
    "                      are represented with empty text and empty arrays.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    all_onsets = []\n",
    "    all_durations = []\n",
    "\n",
    "    for _, row in transcript.iterrows():\n",
    "        # Skip rows without valid onsets.\n",
    "        if not row['onsets_per_tr'] or row['onsets_per_tr'] == []:\n",
    "            continue\n",
    "\n",
    "        # Convert string representations if needed.\n",
    "        onsets = row['onsets_per_tr']\n",
    "        words = row['words_per_tr']\n",
    "        durations = row['durations_per_tr']\n",
    "        if isinstance(onsets, str):\n",
    "            onsets = ast.literal_eval(onsets)\n",
    "        if isinstance(words, str):\n",
    "            words = ast.literal_eval(words)\n",
    "        if isinstance(durations, str):\n",
    "            durations = ast.literal_eval(durations)\n",
    "\n",
    "        all_words.extend(words)\n",
    "        all_onsets.extend(onsets)\n",
    "        all_durations.extend(durations)\n",
    "\n",
    "    # Create a DataFrame with one row per word.\n",
    "    df = pd.DataFrame({\n",
    "        'word': all_words,\n",
    "        'onset': all_onsets,\n",
    "        'duration': all_durations\n",
    "    })\n",
    "    df['word_end'] = df['onset'] + df['duration']\n",
    "    \n",
    "    # Determine the new interval index for each word (based on word_end)\n",
    "    df['new_index'] = (df['word_end'] // new_interval).astype(int)\n",
    "    \n",
    "    # Group by the new interval index.\n",
    "    grouped = df.groupby('new_index').agg({\n",
    "        'word': list,\n",
    "        'onset': list,\n",
    "        'duration': list,\n",
    "        'word_end': list\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Ensure max_index is an integer. If df is empty, set max_index to 0.\n",
    "    max_index = df['new_index'].max()\n",
    "    if pd.isna(max_index):\n",
    "        max_index = 0\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    \n",
    "    # Create a complete DataFrame with all interval indices from 0 up to the maximum.\n",
    "    complete_intervals = pd.DataFrame({'new_index': range(max_index + 1)})\n",
    "    \n",
    "    # Merge the complete intervals with the grouped data so that empty intervals are kept.\n",
    "    result = complete_intervals.merge(grouped, on='new_index', how='left')\n",
    "    \n",
    "    # Replace any missing values with empty lists.\n",
    "    for col in ['word', 'onset', 'duration', 'word_end']:\n",
    "        result[col] = result[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    \n",
    "    # (Optional) Create a text column that joins the words, resulting in an empty string for empty intervals.\n",
    "    result['text'] = result['word'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26398f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:37:46.219801Z",
     "iopub.status.busy": "2025-04-29T14:37:46.219442Z",
     "iopub.status.idle": "2025-04-29T14:37:46.252274Z",
     "shell.execute_reply": "2025-04-29T14:37:46.251660Z"
    },
    "papermill": {
     "duration": 0.039844,
     "end_time": "2025-04-29T14:37:46.253406",
     "exception": false,
     "start_time": "2025-04-29T14:37:46.213562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Tuple, List\n",
    "import einops\n",
    "\n",
    "def extract_section(\n",
    "    video: torch.Tensor,\n",
    "    audio: torch.Tensor,\n",
    "    transcript: pd.DataFrame,\n",
    "    interval: float,\n",
    "    index: int,\n",
    "    sample_rate: int,\n",
    "    modality: str = 'all',\n",
    "    fps_video: float = 30,\n",
    "    past_offset: int = 0,   # number of intervals (including current) to include from the past\n",
    "    future_offset: int = 0,  # number of intervals after the current one to include\n",
    "    split_by_tr: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n",
    "    \"\"\"\n",
    "    Extracts a section of audio, video, and transcript data based on the interval and index,\n",
    "    with optional offsets for past and future intervals. If part of the requested window is\n",
    "    out of bounds, the missing parts are padded with zeros (or empty strings for transcript).\n",
    "\n",
    "    Parameters:\n",
    "        video (torch.Tensor): Tensor containing video frames (num_frames, 3, 224, 224).\n",
    "        audio (torch.Tensor): Tensor containing audio waveform (channels, num_samples).\n",
    "        transcript (pd.DataFrame): DataFrame containing transcript data (assumed one row per interval).\n",
    "        interval (float): Duration (in seconds) of one segment/interval.\n",
    "        index (int): Index (zero-indexed) of the current interval.\n",
    "        sample_rate (int): Sample rate of the audio waveform.\n",
    "        modality (str): Modality to extract features from. Options are 'video', 'audio', 'transcript', or 'all'.\n",
    "        fps_video (float): Frames per second of the video.\n",
    "        past_offset (int): Number of intervals to include from the past (including the current one).\n",
    "            For example, past_offset=5 with index=100 returns intervals 96-100.\n",
    "        future_offset (int): Number of intervals after the current one to include.\n",
    "            For example, future_offset=2 with index=100 returns intervals 100-102.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (video_section, audio_section, transcript_section) where:\n",
    "            - video_section is a torch.Tensor of shape (requested_frames, *video.shape[1:]),\n",
    "              padded with zeros if necessary.\n",
    "            - audio_section is a torch.Tensor of shape (channels, requested_samples),\n",
    "              padded with zeros if necessary.\n",
    "            - transcript_section is a list of strings of length (number of requested intervals),\n",
    "              where missing intervals are filled with empty strings.\n",
    "    \"\"\"\n",
    "    # Determine the range of intervals to extract.\n",
    "    # If past_offset > 0, we include the current interval and the (past_offset - 1) preceding intervals.\n",
    "    extraction_start_index = index - past_offset + 1 if past_offset > 0 else index\n",
    "    extraction_end_index = index + future_offset  # inclusive\n",
    "    total_intervals = extraction_end_index - extraction_start_index + 1\n",
    "\n",
    "    # print(index, past_offset, future_offset, total_intervals)\n",
    "    # Determine the corresponding time boundaries.\n",
    "    # Note that a given interval i spans [i * interval, (i+1) * interval).\n",
    "    requested_start_time = extraction_start_index * interval\n",
    "    requested_end_time = (extraction_end_index + 1) * interval  # exclusive end\n",
    "\n",
    "    # ---- Audio Extraction ----\n",
    "    audio_section = None\n",
    "    if modality in ['all', 'audio']:\n",
    "        # Total samples requested\n",
    "        total_requested_samples = int(round(total_intervals * interval * sample_rate))\n",
    "        # Create output tensor filled with zeros.\n",
    "        audio_section = torch.zeros(audio.shape[0], total_requested_samples)\n",
    "        \n",
    "        # Compute the global sample indices corresponding to the requested time window.\n",
    "        requested_start_sample = int(round(requested_start_time * sample_rate))\n",
    "        requested_end_sample = int(round(requested_end_time * sample_rate))\n",
    "        \n",
    "        # Determine the part available from the source audio.\n",
    "        source_start = max(0, requested_start_sample)\n",
    "        source_end = min(audio.shape[1], requested_end_sample)\n",
    "        \n",
    "        # Determine where to paste the available audio in the output tensor.\n",
    "        target_offset = 0\n",
    "        if requested_start_sample < 0:\n",
    "            target_offset = -requested_start_sample  # number of samples to pad at beginning\n",
    "        \n",
    "        # Compute the number of samples to copy.\n",
    "        num_samples_to_copy = source_end - source_start\n",
    "        if num_samples_to_copy > 0:\n",
    "            audio_section[:, target_offset:target_offset + num_samples_to_copy] = audio[:, source_start:source_end]\n",
    "        if split_by_tr:\n",
    "            audio_section = einops.rearrange(audio_section, 'c (tr t) -> tr c t', tr=total_intervals)\n",
    "\n",
    "    # ---- Video Extraction ----\n",
    "    video_section = None\n",
    "    if modality in ['all', 'video']:\n",
    "        # Compute requested frame indices.\n",
    "        requested_video_start = int(round(requested_start_time * fps_video))\n",
    "        requested_video_end = int(round(requested_end_time * fps_video))  # exclusive end\n",
    "        total_requested_frames = requested_video_end - requested_video_start\n",
    "        \n",
    "        # Create output tensor filled with zeros.\n",
    "        video_section = torch.zeros(total_requested_frames, *video.shape[1:])\n",
    "        \n",
    "        # Determine the available frames.\n",
    "        source_frame_start = max(0, requested_video_start)\n",
    "        source_frame_end = min(video.shape[0], requested_video_end)\n",
    "        \n",
    "        # Determine target offset in frames.\n",
    "        target_offset_frames = 0\n",
    "        if requested_video_start < 0:\n",
    "            target_offset_frames = -requested_video_start\n",
    "        \n",
    "        num_frames_to_copy = source_frame_end - source_frame_start\n",
    "        if num_frames_to_copy > 0:\n",
    "            video_section[target_offset_frames:target_offset_frames + num_frames_to_copy] = video[source_frame_start:source_frame_end]\n",
    "        if split_by_tr:\n",
    "            B, C, H, W = video_section.shape\n",
    "            tr = total_intervals            # 137\n",
    "            f  = B // tr                    # 35\n",
    "            new_len = f * tr                # 35 * 137 = 4795\n",
    "\n",
    "            # pick new_len indices that span 0 … B-1 evenly\n",
    "            # torch.linspace gives floats; .round().long() makes them integer\n",
    "            inds = torch.linspace(0, B-1, steps=new_len).round().long()\n",
    "\n",
    "            # gather those frames\n",
    "            vs = video_section[inds]\n",
    "\n",
    "            # reshape into (35, 3, 137, H, W)\n",
    "            video_section = einops.rearrange(vs, '(tr f) c w h -> tr f c w h', tr=tr)\n",
    "\n",
    "    # ---- Transcript Extraction ----\n",
    "    transcript_section = []\n",
    "    if modality in ['all', 'transcript']:\n",
    "        # For transcript, we assume one row per interval.\n",
    "        # Build a list of length total_intervals.\n",
    "        for i in range(total_intervals):\n",
    "            global_idx = extraction_start_index + i\n",
    "            if global_idx < 0 or global_idx >= len(transcript):\n",
    "                transcript_section.append(\"\")\n",
    "            else:\n",
    "                if split_by_tr:\n",
    "                    transcript_section = transcript_section + [transcript['word'].iloc[global_idx]]\n",
    "                else:\n",
    "                    transcript_section = transcript_section + transcript['word'].iloc[global_idx]\n",
    "\n",
    "    return video_section, audio_section, transcript_section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7288e4",
   "metadata": {
    "papermill": {
     "duration": 0.004954,
     "end_time": "2025-04-29T14:37:46.263550",
     "exception": false,
     "start_time": "2025-04-29T14:37:46.258596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Play with your model and the feature extractor here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0ec9fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:37:58.214118Z",
     "iopub.status.busy": "2025-04-29T14:37:58.213861Z",
     "iopub.status.idle": "2025-04-29T14:40:43.089841Z",
     "shell.execute_reply": "2025-04-29T14:40:43.088860Z"
    },
    "papermill": {
     "duration": 164.885548,
     "end_time": "2025-04-29T14:40:43.091457",
     "exception": false,
     "start_time": "2025-04-29T14:37:58.205909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [02:33<00:00, 38.28s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from pathlib import Path\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "def find_closest_aspect_ratio(ar, target_ratios, width, height, image_size):\n",
    "    best_diff = float('inf')\n",
    "    best = (1, 1)\n",
    "    area = width * height\n",
    "    for (w, h) in target_ratios:\n",
    "        target_ar = w / h\n",
    "        diff = abs(ar - target_ar)\n",
    "        # pick the ratio with smallest diff; on tie prefer larger original image area\n",
    "        if diff < best_diff or (diff == best_diff and area > 0.5 * image_size**2 * w * h):\n",
    "            best_diff = diff\n",
    "            best = (w, h)\n",
    "    return best\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_w, orig_h = image.size\n",
    "    ar = orig_w / orig_h\n",
    "\n",
    "    # build all (i,j) pairs whose product ∈ [min_num, max_num]\n",
    "    target_ratios = sorted(\n",
    "        {(i, j)\n",
    "         for n in range(min_num, max_num + 1)\n",
    "         for i in range(1, n + 1)\n",
    "         for j in range(1, n + 1)\n",
    "         if min_num <= i * j <= max_num},\n",
    "        key=lambda x: x[0] * x[1]\n",
    "    )\n",
    "\n",
    "    w_mul, h_mul = find_closest_aspect_ratio(ar, target_ratios, orig_w, orig_h, image_size)\n",
    "    target_w, target_h = image_size * w_mul, image_size * h_mul\n",
    "    blocks = w_mul * h_mul\n",
    "\n",
    "    resized = image.resize((target_w, target_h))\n",
    "    cols = target_w // image_size\n",
    "\n",
    "    crops = []\n",
    "    for idx in range(blocks):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        box = (\n",
    "            col * image_size,\n",
    "            row * image_size,\n",
    "            (col + 1) * image_size,\n",
    "            (row + 1) * image_size\n",
    "        )\n",
    "        crops.append(resized.crop(box))\n",
    "\n",
    "    if use_thumbnail and len(crops) != 1:\n",
    "        crops.append(image.resize((image_size, image_size)))\n",
    "\n",
    "    return crops\n",
    "\n",
    "def _preprocess_single_pil(pil_img, transform, input_size, max_num):\n",
    "    crops = dynamic_preprocess(\n",
    "        pil_img,\n",
    "        image_size=input_size,\n",
    "        use_thumbnail=True,\n",
    "        max_num=max_num\n",
    "    )\n",
    "    return [transform(c) for c in crops]   # list of tensors\n",
    "\n",
    "\n",
    "def load_image(\n",
    "    imgs: Union[str, Path, List[Union[str, Path]], torch.Tensor],\n",
    "    input_size: int = 448,\n",
    "    max_num: int = 12,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs :  • str/Path : path to one image file\n",
    "            • list/tuple of paths\n",
    "            • torch.Tensor  [n,3,H,W] or [3,H,W]  (values 0‑255, dtype int / fp16)\n",
    "    input_size : target side length (square patch size)\n",
    "    max_num    : maximum #crops per original image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pixel_values : Tensor  [total_crops, 3, input_size, input_size]\n",
    "                   normalized to ImageNet mean/std  (dtype = float32)\n",
    "    \"\"\"\n",
    "\n",
    "    transform = build_transform(input_size=input_size)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Phase 1: collect all PIL images --------------------------------\n",
    "    # --------------------------------------------------------\n",
    "    pil_images = []\n",
    "\n",
    "    # 1) path or list‑of‑paths\n",
    "    if isinstance(imgs, (str, Path)):\n",
    "        pil_images.append(Image.open(imgs).convert('RGB'))\n",
    "\n",
    "    elif isinstance(imgs, (list, tuple)) and imgs and isinstance(imgs[0], (str, Path)):\n",
    "        for p in imgs:\n",
    "            pil_images.append(Image.open(p).convert('RGB'))\n",
    "\n",
    "    # 2) tensor input\n",
    "    elif isinstance(imgs, torch.Tensor):\n",
    "        if imgs.ndim == 3:                 # [3,H,W]  -> add batch dim\n",
    "            imgs = imgs.unsqueeze(0)\n",
    "        assert imgs.ndim == 4 and imgs.shape[1] == 3, \\\n",
    "            \"Expect tensor shape [n,3,H,W] or [3,H,W]\"\n",
    "\n",
    "        # move to CPU, ensure uint8\n",
    "        imgs_cpu = imgs.detach().to('cpu')\n",
    "        if imgs_cpu.dtype != torch.uint8:\n",
    "            imgs_cpu = imgs_cpu.round().clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "        for i in range(imgs_cpu.size(0)):\n",
    "            pil_images.append(to_pil_image(imgs_cpu[i]))\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"`imgs` must be a path, list of paths, or a [n,3,H,W] tensor\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Phase 2: dynamic tiling + transforms ------------------\n",
    "    # --------------------------------------------------------\n",
    "    pixel_tensors = []\n",
    "    for pil in pil_images:\n",
    "        pixel_tensors.extend(\n",
    "            _preprocess_single_pil(pil, transform, input_size, max_num)\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Phase 3: stack to one tensor  --------------------------\n",
    "    # --------------------------------------------------------\n",
    "    if not pixel_tensors:\n",
    "        raise RuntimeError(\"No images found after preprocessing\")\n",
    "\n",
    "    return torch.stack(pixel_tensors)      # [total_crops, 3, input_size, input_size]\n",
    "\n",
    "def split_model(model_name_or_path):\n",
    "    device_map = {}\n",
    "    world_size = max(1, torch.cuda.device_count())\n",
    "\n",
    "    # load config\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "\n",
    "    # allocate layers (first GPU counts as half)\n",
    "    per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    counts = [per_gpu] * world_size\n",
    "    counts[0] = math.ceil(counts[0] * 0.5)\n",
    "\n",
    "    layer_idx = 0\n",
    "    for gpu_idx, cnt in enumerate(counts):\n",
    "        for _ in range(cnt):\n",
    "            device_map[f'language_model.model.layers.{layer_idx}'] = gpu_idx\n",
    "            layer_idx += 1\n",
    "\n",
    "    # map the rest to GPU 0\n",
    "    for key in [\n",
    "        'vision_model', 'mlp1',\n",
    "        'language_model.model.tok_embeddings',\n",
    "        'language_model.model.embed_tokens',\n",
    "        'language_model.output',\n",
    "        'language_model.model.norm',\n",
    "        'language_model.model.rotary_emb',\n",
    "        'language_model.lm_head'\n",
    "    ]:\n",
    "        device_map[key] = 0\n",
    "\n",
    "    # ensure last layer lands on GPU 0\n",
    "    device_map[f'language_model.model.layers.{num_layers-1}'] = 0\n",
    "\n",
    "    return device_map\n",
    "\n",
    "# now load your model & tokenizer\n",
    "path = 'OpenGVLab/InternVL3-8B'\n",
    "device_map = split_model(path)\n",
    "cache_dir = \"/home/mihirneal/Developer/hf_cache\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=cache_dir,\n",
    "    # load_in_8bit=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad695f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:40:43.112163Z",
     "iopub.status.busy": "2025-04-29T14:40:43.111535Z",
     "iopub.status.idle": "2025-04-29T14:40:43.145594Z",
     "shell.execute_reply": "2025-04-29T14:40:43.144708Z"
    },
    "papermill": {
     "duration": 0.045419,
     "end_time": "2025-04-29T14:40:43.146925",
     "exception": false,
     "start_time": "2025-04-29T14:40:43.101506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Conversation prompt templates.\n",
    "We kindly request that you import fastchat instead of copying this file if you wish to use it.\n",
    "If you have changes in mind, please contribute back so the community can benefit collectively and continue to maintain these valuable templates.\n",
    "Modified from https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n",
    "\"\"\"\n",
    "\n",
    "import dataclasses\n",
    "from enum import IntEnum, auto\n",
    "\n",
    "\n",
    "class SeparatorStyle(IntEnum):\n",
    "    \"\"\"Separator styles.\"\"\"\n",
    "\n",
    "    ADD_COLON_SINGLE = auto()\n",
    "    ADD_COLON_TWO = auto()\n",
    "    ADD_COLON_SPACE_SINGLE = auto()\n",
    "    NO_COLON_SINGLE = auto()\n",
    "    NO_COLON_TWO = auto()\n",
    "    ADD_NEW_LINE_SINGLE = auto()\n",
    "    LLAMA2 = auto()\n",
    "    CHATGLM = auto()\n",
    "    CHATML = auto()\n",
    "    CHATINTERN = auto()\n",
    "    DOLLY = auto()\n",
    "    RWKV = auto()\n",
    "    PHOENIX = auto()\n",
    "    ROBIN = auto()\n",
    "    FALCON_CHAT = auto()\n",
    "    CHATGLM3 = auto()\n",
    "    INTERNVL_ZH = auto()\n",
    "    MPT = auto()\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that manages prompt templates and keeps all conversation history.\"\"\"\n",
    "\n",
    "    # The name of this template\n",
    "    name: str\n",
    "    # The template of the system prompt\n",
    "    system_template: str = '{system_message}'\n",
    "    # The system message\n",
    "    system_message: str = ''\n",
    "    # The names of two roles\n",
    "    roles: Tuple[str] = ('USER', 'ASSISTANT')\n",
    "    # All messages. Each item is (role, message).\n",
    "    messages: List[List[str]] = ()\n",
    "    # The number of few shot examples\n",
    "    offset: int = 0\n",
    "    # The separator style and configurations\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.ADD_COLON_SINGLE\n",
    "    sep: str = '\\n'\n",
    "    sep2: str = None\n",
    "    # Stop criteria (the default one is EOS token)\n",
    "    stop_str: Union[str, List[str]] = None\n",
    "    # Stops generation if meeting any token in this list\n",
    "    stop_token_ids: List[int] = None\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        \"\"\"Get the prompt for generation.\"\"\"\n",
    "        system_prompt = self.system_template.format(system_message=self.system_message)\n",
    "        if self.sep_style == SeparatorStyle.ADD_COLON_SINGLE:\n",
    "            ret = system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + ': ' + message + self.sep\n",
    "                else:\n",
    "                    ret += role + ':'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = system_prompt + seps[0]\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += role + ': ' + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += role + ':'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.ADD_COLON_SPACE_SINGLE:\n",
    "            ret = system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + ': ' + message + self.sep\n",
    "                else:\n",
    "                    ret += role + ': '  # must be end with a space\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.ADD_NEW_LINE_SINGLE:\n",
    "            ret = '' if system_prompt == '' else system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + '\\n' + message + self.sep\n",
    "                else:\n",
    "                    ret += role + '\\n'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.NO_COLON_SINGLE:\n",
    "            ret = system_prompt\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + message + self.sep\n",
    "                else:\n",
    "                    ret += role\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.NO_COLON_TWO:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = system_prompt\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += role + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += role\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.RWKV:\n",
    "            ret = system_prompt\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += (\n",
    "                        role\n",
    "                        + ': '\n",
    "                        + message.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n')\n",
    "                    )\n",
    "                    ret += '\\n\\n'\n",
    "                else:\n",
    "                    ret += role + ':'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.LLAMA2:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            if self.system_message:\n",
    "                ret = system_prompt\n",
    "            else:\n",
    "                ret = '[INST] '\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                tag = self.roles[i % 2]\n",
    "                if message:\n",
    "                    if i == 0:\n",
    "                        ret += message + ' '\n",
    "                    else:\n",
    "                        ret += tag + ' ' + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += tag\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.CHATGLM:\n",
    "            # source: https://huggingface.co/THUDM/chatglm-6b/blob/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py#L1302-L1308\n",
    "            # source2: https://huggingface.co/THUDM/chatglm2-6b/blob/e186c891cf64310ac66ef10a87e6635fa6c2a579/modeling_chatglm.py#L926\n",
    "            round_add_n = 1 if self.name == 'chatglm2' else 0\n",
    "            if system_prompt:\n",
    "                ret = system_prompt + self.sep\n",
    "            else:\n",
    "                ret = ''\n",
    "\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if i % 2 == 0:\n",
    "                    ret += f'[Round {i//2 + round_add_n}]{self.sep}'\n",
    "\n",
    "                if message:\n",
    "                    ret += f'{role}：{message}{self.sep}'\n",
    "                else:\n",
    "                    ret += f'{role}：'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.CHATML:\n",
    "            ret = '' if system_prompt == '' else system_prompt + self.sep + '\\n'\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + '\\n' + message + self.sep + '\\n'\n",
    "                else:\n",
    "                    ret += role + '\\n'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.CHATGLM3:\n",
    "            ret = ''\n",
    "            if self.system_message:\n",
    "                ret += system_prompt\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + '\\n' + ' ' + message\n",
    "                else:\n",
    "                    ret += role\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.CHATINTERN:\n",
    "            # source: https://huggingface.co/internlm/internlm-chat-7b-8k/blob/bd546fa984b4b0b86958f56bf37f94aa75ab8831/modeling_internlm.py#L771\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = system_prompt\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                # if i % 2 == 0:\n",
    "                #     ret += \"<s>\"\n",
    "                if message:\n",
    "                    ret += role + ':' + message + seps[i % 2] + '\\n'\n",
    "                else:\n",
    "                    ret += role + ':'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.DOLLY:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = system_prompt\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += role + ':\\n' + message + seps[i % 2]\n",
    "                    if i % 2 == 1:\n",
    "                        ret += '\\n\\n'\n",
    "                else:\n",
    "                    ret += role + ':\\n'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.PHOENIX:\n",
    "            ret = system_prompt\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + ': ' + '<s>' + message + '</s>'\n",
    "                else:\n",
    "                    ret += role + ': ' + '<s>'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.ROBIN:\n",
    "            ret = system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + ':\\n' + message + self.sep\n",
    "                else:\n",
    "                    ret += role + ':\\n'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.FALCON_CHAT:\n",
    "            ret = ''\n",
    "            if self.system_message:\n",
    "                ret += system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + ': ' + message + self.sep\n",
    "                else:\n",
    "                    ret += role + ':'\n",
    "\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.INTERNVL_ZH:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = self.system_message + seps[0]\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += role + ': ' + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += role + ':'\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.MPT:\n",
    "            ret = system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += role + message + self.sep\n",
    "                else:\n",
    "                    ret += role\n",
    "            return ret\n",
    "        else:\n",
    "            raise ValueError(f'Invalid style: {self.sep_style}')\n",
    "\n",
    "    def set_system_message(self, system_message: str):\n",
    "        \"\"\"Set the system message.\"\"\"\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def append_message(self, role: str, message: str):\n",
    "        \"\"\"Append a new message.\"\"\"\n",
    "        self.messages.append([role, message])\n",
    "\n",
    "    def update_last_message(self, message: str):\n",
    "        \"\"\"Update the last output.\n",
    "        The last message is typically set to be None when constructing the prompt,\n",
    "        so we need to update it in-place after getting the response from a model.\n",
    "        \"\"\"\n",
    "        self.messages[-1][1] = message\n",
    "\n",
    "    def to_gradio_chatbot(self):\n",
    "        \"\"\"Convert the conversation to gradio chatbot format.\"\"\"\n",
    "        ret = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n",
    "            if i % 2 == 0:\n",
    "                ret.append([msg, None])\n",
    "            else:\n",
    "                ret[-1][-1] = msg\n",
    "        return ret\n",
    "\n",
    "    def to_openai_api_messages(self):\n",
    "        \"\"\"Convert the conversation to OpenAI chat completion format.\"\"\"\n",
    "        ret = [{'role': 'system', 'content': self.system_message}]\n",
    "\n",
    "        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n",
    "            if i % 2 == 0:\n",
    "                ret.append({'role': 'user', 'content': msg})\n",
    "            else:\n",
    "                if msg is not None:\n",
    "                    ret.append({'role': 'assistant', 'content': msg})\n",
    "        return ret\n",
    "\n",
    "    def copy(self):\n",
    "        return Conversation(\n",
    "            name=self.name,\n",
    "            system_template=self.system_template,\n",
    "            system_message=self.system_message,\n",
    "            roles=self.roles,\n",
    "            messages=[[x, y] for x, y in self.messages],\n",
    "            offset=self.offset,\n",
    "            sep_style=self.sep_style,\n",
    "            sep=self.sep,\n",
    "            sep2=self.sep2,\n",
    "            stop_str=self.stop_str,\n",
    "            stop_token_ids=self.stop_token_ids,\n",
    "        )\n",
    "\n",
    "    def dict(self):\n",
    "        return {\n",
    "            'template_name': self.name,\n",
    "            'system_message': self.system_message,\n",
    "            'roles': self.roles,\n",
    "            'messages': self.messages,\n",
    "            'offset': self.offset,\n",
    "        }\n",
    "\n",
    "\n",
    "# A global registry for all conversation templates\n",
    "conv_templates: Dict[str, Conversation] = {}\n",
    "\n",
    "\n",
    "def register_conv_template(template: Conversation, override: bool = False):\n",
    "    \"\"\"Register a new conversation template.\"\"\"\n",
    "    if not override:\n",
    "        assert (\n",
    "            template.name not in conv_templates\n",
    "        ), f'{template.name} has been registered.'\n",
    "\n",
    "    conv_templates[template.name] = template\n",
    "\n",
    "\n",
    "def get_conv_template(name: str) -> Conversation:\n",
    "    \"\"\"Get a conversation template.\"\"\"\n",
    "    return conv_templates[name].copy()\n",
    "\n",
    "\n",
    "# Both Hermes-2 and internlm2-chat are chatml-format conversation templates. The difference\n",
    "# is that during training, the preprocessing function for the Hermes-2 template doesn't add\n",
    "# <s> at the beginning of the tokenized sequence, while the internlm2-chat template does.\n",
    "# Therefore, they are completely equivalent during inference.\n",
    "register_conv_template(\n",
    "    Conversation(\n",
    "        name='Hermes-2',\n",
    "        system_template='<|im_start|>system\\n{system_message}',\n",
    "        # note: The new system prompt was not used here to avoid changes in benchmark performance.\n",
    "        # system_message='我是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。',\n",
    "        system_message='你是由上海人工智能实验室联合商汤科技开发的书生多模态大模型，英文名叫InternVL, 是一个有用无害的人工智能助手。',\n",
    "        roles=('<|im_start|>user\\n', '<|im_start|>assistant\\n'),\n",
    "        sep_style=SeparatorStyle.MPT,\n",
    "        sep='<|im_end|>',\n",
    "        stop_str='<|endoftext|>',\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "register_conv_template(\n",
    "    Conversation(\n",
    "        name='internlm2-chat',\n",
    "        system_template='<|im_start|>system\\n{system_message}',\n",
    "        # note: The new system prompt was not used here to avoid changes in benchmark performance.\n",
    "        # system_message='我是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。',\n",
    "        system_message='你是由上海人工智能实验室联合商汤科技开发的书生多模态大模型，英文名叫InternVL, 是一个有用无害的人工智能助手。',\n",
    "        roles=('<|im_start|>user\\n', '<|im_start|>assistant\\n'),\n",
    "        sep_style=SeparatorStyle.MPT,\n",
    "        sep='<|im_end|>',\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "register_conv_template(\n",
    "    Conversation(\n",
    "        name='phi3-chat',\n",
    "        system_template='<|system|>\\n{system_message}',\n",
    "        # note: The new system prompt was not used here to avoid changes in benchmark performance.\n",
    "        # system_message='我是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。',\n",
    "        system_message='你是由上海人工智能实验室联合商汤科技开发的书生多模态大模型，英文名叫InternVL, 是一个有用无害的人工智能助手。',\n",
    "        roles=('<|user|>\\n', '<|assistant|>\\n'),\n",
    "        sep_style=SeparatorStyle.MPT,\n",
    "        sep='<|end|>',\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "register_conv_template(\n",
    "    Conversation(\n",
    "        name='internvl2_5',\n",
    "        system_template='<|im_start|>system\\n{system_message}',\n",
    "        system_message='你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。',\n",
    "        roles=('<|im_start|>user\\n', '<|im_start|>assistant\\n'),\n",
    "        sep_style=SeparatorStyle.MPT,\n",
    "        sep='<|im_end|>\\n',\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000168b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:40:43.165399Z",
     "iopub.status.busy": "2025-04-29T14:40:43.165156Z",
     "iopub.status.idle": "2025-04-29T14:40:46.811899Z",
     "shell.execute_reply": "2025-04-29T14:40:46.810852Z"
    },
    "papermill": {
     "duration": 3.657488,
     "end_time": "2025-04-29T14:40:46.813437",
     "exception": false,
     "start_time": "2025-04-29T14:40:43.155949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded ./examples/image1.jpg\n",
      "Downloaded ./examples/image2.jpg\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "os.makedirs('examples', exist_ok=True)\n",
    "urls = [\n",
    "    \"https://picsum.photos/512\",\n",
    "    \"https://picsum.photos/512\"\n",
    "]\n",
    "for idx, url in enumerate(urls, start=1):\n",
    "    resp = requests.get(url)\n",
    "    img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "    path = f\"./examples/image{idx}.jpg\"\n",
    "    img.save(path)\n",
    "    print(f\"Downloaded {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd1e4192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:40:46.833145Z",
     "iopub.status.busy": "2025-04-29T14:40:46.832870Z",
     "iopub.status.idle": "2025-04-29T14:40:48.999463Z",
     "shell.execute_reply": "2025-04-29T14:40:48.998443Z"
    },
    "papermill": {
     "duration": 2.177742,
     "end_time": "2025-04-29T14:40:49.000995",
     "exception": false,
     "start_time": "2025-04-29T14:40:46.823253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './examples/image1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs.logits        \u001b[38;5;66;03m# (1, seq_len, vocab)\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# ① Load/prepare crops just like in your working chat example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m pixel_values1 = \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./examples/image1.jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m.to(torch.bfloat16).cuda()\n\u001b[32m     74\u001b[39m pixel_values2 = load_image(\u001b[33m'\u001b[39m\u001b[33m./examples/image2.jpg\u001b[39m\u001b[33m'\u001b[39m, max_num=\u001b[32m12\u001b[39m).to(torch.bfloat16).cuda()\n\u001b[32m     75\u001b[39m pixel_values  = torch.cat((pixel_values1, pixel_values2), dim=\u001b[32m0\u001b[39m)   \u001b[38;5;66;03m# (24, 3, 448, 448)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mload_image\u001b[39m\u001b[34m(imgs, input_size, max_num)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# 1) path or list‑of‑paths\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(imgs, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     pil_images.append(\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(imgs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m imgs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(imgs[\u001b[32m0\u001b[39m], (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m imgs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/algonauts/ckadirt_feat/.venv/lib/python3.13/site-packages/PIL/Image.py:3505\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3502\u001b[39m     filename = os.fspath(fp)\n\u001b[32m   3504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3506\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './examples/image1.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility: prepare a single prompt exactly as `model.chat` does\n",
    "# ------------------------------------------------------------------\n",
    "def build_prompt(model, tokenizer, question, num_patches_list,\n",
    "                 IMG_START_TOKEN='<img>', IMG_END_TOKEN='</img>',\n",
    "                 IMG_CONTEXT_TOKEN='<IMG_CONTEXT>'):\n",
    "    \"\"\"\n",
    "    Returns prompt text + (img_context_token_id already set in model)\n",
    "    \"\"\"\n",
    "    if '<image>' not in question:\n",
    "        question = '<image>\\n' + question\n",
    "    template = get_conv_template(model.template)\n",
    "    template.system_message = model.system_message\n",
    "    template.append_message(template.roles[0], question)   # user\n",
    "    template.append_message(template.roles[1], None)       # assistant (to be filled)\n",
    "    prompt = template.get_prompt()\n",
    "\n",
    "    # replace each <image> placeholder with the correct number of IMG_CONTEXT_TOKENs\n",
    "    for n_patches in num_patches_list:\n",
    "        image_tokens = (\n",
    "            IMG_START_TOKEN\n",
    "            + IMG_CONTEXT_TOKEN * model.num_image_token * n_patches\n",
    "            + IMG_END_TOKEN\n",
    "        )\n",
    "        prompt = prompt.replace('<image>', image_tokens, 1)\n",
    "\n",
    "    # store the special‑token id inside the model (needed by forward)\n",
    "    model.img_context_token_id = tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Main helper: run forward() and return logits\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_logits(model, tokenizer, pixel_values, question, num_patches_list, device='cuda'):\n",
    "    \"\"\"\n",
    "    pixel_values       : (total_image_crops, 3, H, W)  — concatenated crops\n",
    "    num_patches_list   : e.g. [12, 12]  (one entry per *original* image)\n",
    "    returns logits     : shape (B, seq_len, vocab)\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(model, tokenizer, question, num_patches_list)\n",
    "    print(prompt)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    model_inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    input_ids      = model_inputs['input_ids'     ].to(device)\n",
    "    attention_mask = model_inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # forward() wants an image‑level flag; 1 = this crop is used\n",
    "    image_flags = torch.ones(pixel_values.size(0), 1,\n",
    "                             dtype=torch.long, device=device)\n",
    "    global am\n",
    "    am = attention_mask.detach()\n",
    "    outputs = model(\n",
    "        pixel_values=pixel_values.to(device),\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        image_flags=image_flags,\n",
    "        return_dict=True\n",
    "    )\n",
    "    return outputs.logits        # (1, seq_len, vocab)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Example usage\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# ① Load/prepare crops just like in your working chat example\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values  = torch.cat((pixel_values1, pixel_values2), dim=0)   # (24, 3, 448, 448)\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]  # [12, 12]\n",
    "\n",
    "# ② Ask your question\n",
    "question = \"<image>\\nImage‑2: <image>\\nDescribe the two images in detail.\"\n",
    "\n",
    "# ③ Forward pass → logits\n",
    "logits = get_logits(model, tokenizer, pixel_values, question, num_patches_list)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233fc7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:40:49.023022Z",
     "iopub.status.busy": "2025-04-29T14:40:49.022740Z",
     "iopub.status.idle": "2025-04-29T14:40:49.032911Z",
     "shell.execute_reply": "2025-04-29T14:40:49.032260Z"
    },
    "papermill": {
     "duration": 0.021559,
     "end_time": "2025-04-29T14:40:49.034217",
     "exception": false,
     "start_time": "2025-04-29T14:40:49.012658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_pair_chunk(words: List[str],\n",
    "                    num_img_tokens: int,\n",
    "                    IMG_START_TOKEN='<img>',\n",
    "                    IMG_END_TOKEN='</img>',\n",
    "                    IMG_CONTEXT_TOKEN='<IMG_CONTEXT>'):\n",
    "    return (\n",
    "        IMG_START_TOKEN\n",
    "        + IMG_CONTEXT_TOKEN * num_img_tokens\n",
    "        + IMG_END_TOKEN\n",
    "        + ' '\n",
    "        + ' '.join(words)\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Main helper\n",
    "# ------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def logits_by_pair(extractor, tokenizer,\n",
    "                   pixel_values: torch.Tensor,              # [n, 3, H, W]\n",
    "                   sentences: List[List[str]],              # len == n\n",
    "                   use_template: bool = False               # wrap in chat template?\n",
    "                  ) -> Tuple[torch.Tensor,\n",
    "                             List[Tuple[int,int]],\n",
    "                             torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        logits        : [1, seq_len, vocab]\n",
    "        token_ranges  : list of (start, end) indices per pair\n",
    "        avg_logits    : [n, vocab]  (avg over sequence tokens of each pair)\n",
    "    \"\"\"\n",
    "    assert len(pixel_values) == len(sentences), \"n images must match n sentences\"\n",
    "    n = len(sentences)\n",
    "    device = next(extractor.model.parameters()).device\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) Build the text prompt\n",
    "    # ------------------------------------------------------------------\n",
    "    num_img_tokens = extractor.model.num_image_token          # K: one image -> K tokens\n",
    "    pair_chunks = [\n",
    "        make_pair_chunk(sentences[i], num_img_tokens)\n",
    "        for i in range(n)\n",
    "    ]\n",
    "    body = '\\n'.join(pair_chunks)                   # \"<img>... </img>  words\\n...\"\n",
    "\n",
    "    if use_template:\n",
    "        tpl = get_conv_template(extractor.model.template)\n",
    "        tpl.system_message = extractor.model.system_message\n",
    "        tpl.append_message(tpl.roles[0], body)\n",
    "        tpl.append_message(tpl.roles[1], None)      # assistant placeholder\n",
    "        prompt = tpl.get_prompt()\n",
    "    else:\n",
    "        prompt = body\n",
    "\n",
    "    # Replace <image> placeholders (if any) already expanded above → nothing to do\n",
    "    # but we still need the model to know what <IMG_CONTEXT> id is\n",
    "    IMG_CONTEXT_TOKEN = '<IMG_CONTEXT>'\n",
    "    extractor.model.img_context_token_id = tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) Tokenise\n",
    "    # ------------------------------------------------------------------\n",
    "    tokenizer.padding_side = 'left'\n",
    "    enc = tokenizer(prompt, return_tensors='pt')\n",
    "    input_ids      = enc['input_ids'     ].to(device)\n",
    "    attention_mask = enc['attention_mask'].to(device)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Build image_flags (all ones, one per image)\n",
    "    # ------------------------------------------------------------------\n",
    "    image_flags = torch.ones(pixel_values.size(0), 1, dtype=torch.long, device=device)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) Forward pass  -> logits\n",
    "    # ------------------------------------------------------------------\n",
    "    logits = extractor(\n",
    "        pixel_values=pixel_values.to(device),\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        image_flags=image_flags,\n",
    "        return_dict=True\n",
    "    ).logits                                               # [1, seq_len, vocab]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) Find token‑range for each pair  (start index of every <img>)\n",
    "    # ------------------------------------------------------------------\n",
    "    img_start_id = tokenizer.convert_tokens_to_ids('<img>')\n",
    "    token_ids = input_ids[0]                               # (seq_len,)\n",
    "    start_idxs = (token_ids == img_start_id).nonzero(as_tuple=False).flatten().tolist()\n",
    "    assert len(start_idxs) == n, \"didn't find <img> marker for every image\"\n",
    "\n",
    "    token_ranges = []\n",
    "    for i in range(n):\n",
    "        s = start_idxs[i]\n",
    "        e = start_idxs[i+1]-1 if i < n-1 else len(token_ids)-1\n",
    "        token_ranges.append((s, e))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6) Average logits over each range -> [n, vocab]\n",
    "    # ------------------------------------------------------------------\n",
    "    avg_logits = torch.stack([\n",
    "        logits[0, s:e+1].mean(dim=0)              # mean over sequence dimension\n",
    "        for (s, e) in token_ranges\n",
    "    ], dim=0)                                     # [n, vocab]\n",
    "\n",
    "    return logits, token_ranges, avg_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c8ec437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:40:49.054017Z",
     "iopub.status.busy": "2025-04-29T14:40:49.053755Z",
     "iopub.status.idle": "2025-04-29T14:40:49.063921Z",
     "shell.execute_reply": "2025-04-29T14:40:49.063021Z"
    },
    "papermill": {
     "duration": 0.021985,
     "end_time": "2025-04-29T14:40:49.065872",
     "exception": false,
     "start_time": "2025-04-29T14:40:49.043887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternVLChatModel(\n",
       "  (vision_model): InternVisionModel(\n",
       "    (embeddings): InternVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InternVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.004)\n",
       "          (drop_path2): DropPath(drop_prob=0.004)\n",
       "        )\n",
       "        (2): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "        (3): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.013)\n",
       "          (drop_path2): DropPath(drop_prob=0.013)\n",
       "        )\n",
       "        (4): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.017)\n",
       "          (drop_path2): DropPath(drop_prob=0.017)\n",
       "        )\n",
       "        (5): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.022)\n",
       "          (drop_path2): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "        (6): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.026)\n",
       "          (drop_path2): DropPath(drop_prob=0.026)\n",
       "        )\n",
       "        (7): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.031)\n",
       "          (drop_path2): DropPath(drop_prob=0.031)\n",
       "        )\n",
       "        (8): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.035)\n",
       "          (drop_path2): DropPath(drop_prob=0.035)\n",
       "        )\n",
       "        (9): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.039)\n",
       "          (drop_path2): DropPath(drop_prob=0.039)\n",
       "        )\n",
       "        (10): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.044)\n",
       "          (drop_path2): DropPath(drop_prob=0.044)\n",
       "        )\n",
       "        (11): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.048)\n",
       "          (drop_path2): DropPath(drop_prob=0.048)\n",
       "        )\n",
       "        (12): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.052)\n",
       "          (drop_path2): DropPath(drop_prob=0.052)\n",
       "        )\n",
       "        (13): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.056)\n",
       "          (drop_path2): DropPath(drop_prob=0.056)\n",
       "        )\n",
       "        (14): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (15): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.065)\n",
       "          (drop_path2): DropPath(drop_prob=0.065)\n",
       "        )\n",
       "        (16): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.069)\n",
       "          (drop_path2): DropPath(drop_prob=0.069)\n",
       "        )\n",
       "        (17): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.074)\n",
       "          (drop_path2): DropPath(drop_prob=0.074)\n",
       "        )\n",
       "        (18): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (19): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.083)\n",
       "          (drop_path2): DropPath(drop_prob=0.083)\n",
       "        )\n",
       "        (20): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.087)\n",
       "          (drop_path2): DropPath(drop_prob=0.087)\n",
       "        )\n",
       "        (21): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "        (22): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (23): InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151674, 3584)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2Attention(\n",
       "            (q_proj): Linear8bitLt(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear8bitLt(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear8bitLt(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear8bitLt(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear8bitLt(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear8bitLt(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear8bitLt(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3584, out_features=151674, bias=False)\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear8bitLt(in_features=4096, out_features=3584, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear8bitLt(in_features=3584, out_features=3584, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af7892f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:40:49.086662Z",
     "iopub.status.busy": "2025-04-29T14:40:49.086449Z",
     "iopub.status.idle": "2025-04-29T14:40:50.499482Z",
     "shell.execute_reply": "2025-04-29T14:40:50.498461Z"
    },
    "papermill": {
     "duration": 1.425197,
     "end_time": "2025-04-29T14:40:50.501028",
     "exception": false,
     "start_time": "2025-04-29T14:40:49.075831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 532, 151674])\n",
      "[(0, 264), (265, 531)]\n",
      "torch.Size([2, 151674])\n",
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 532, 3584]), Averaged feature shape: torch.Size([2, 3584]), Sample: tensor([-0.0608, -0.1104, -0.6680, -0.2949,  0.1553], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 532, 3584]), Averaged feature shape: torch.Size([2, 3584]), Sample: tensor([-0.1865, -0.0942, -0.5039, -0.1406, -0.1523], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 532, 3584]), Averaged feature shape: torch.Size([2, 3584]), Sample: tensor([-0.0854, -0.1182, -0.8359, -0.2988, -0.3379], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 532, 3584]), Averaged feature shape: torch.Size([2, 3584]), Sample: tensor([ 0.3848,  0.1035, -1.7891,  1.4688, -0.5078], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "layers_to_extract = layers_to_extract = [\n",
    "    \"language_model.model.layers.10.post_attention_layernorm\",\n",
    "    \"language_model.model.layers.15.post_attention_layernorm\",\n",
    "    \"language_model.model.layers.20.post_attention_layernorm\",\n",
    "    \"language_model.model.norm\"\n",
    "]\n",
    "\n",
    "# Use the feature extractor as a context manager so that hooks are cleaned up automatically.\n",
    "with HuggingFaceFeatureExtractor(model, layers_to_extract, detach=True) as extractor:\n",
    "    with torch.no_grad():\n",
    "        # # Run a forward pass. The extractor clears previous features automatically.\n",
    "        # outputs = extractor(inputs['pixel_values'].to('cuda'))\n",
    "    \n",
    "        # # Retrieve the extracted features.\n",
    "        # features = extractor.features\n",
    "    \n",
    "        # # Print out the shape of the main model output.\n",
    "        # print(\"Main model output (last_hidden_state) shape:\", outputs.shape)\n",
    "    \n",
    "        # # Iterate over the extracted features and print their shapes.\n",
    "        # for layer_name, activation in features.items():\n",
    "        #     print(f\"Layer: {layer_name}, Feature shape: {activation.shape}\")\n",
    "        pixel_values = pixel_values\n",
    "        sentences = [['primera', 'oracion', 'nnn en'], ['segunda', 'oracion', 'a veces 222']]\n",
    "        \n",
    "        logits, ranges, avg_logits = logits_by_pair(\n",
    "            extractor, tokenizer,\n",
    "            pixel_values, sentences,\n",
    "            use_template=False   # or True\n",
    "        )\n",
    "        print(logits.shape)        # [1, seq_len, 151674]\n",
    "        print(ranges)              # e.g. [(0,160), (161,280), ...]\n",
    "        print(avg_logits.shape)    # [n, 151674]\n",
    "\n",
    "        features = extractor.features\n",
    "    \n",
    "        # Iterate over the extracted features and print their shapes.\n",
    "        for layer_name, activation in features.items():\n",
    "            avg_activation = torch.stack([\n",
    "                activation[0, s:e+1].mean(dim=0)              # mean over sequence dimension\n",
    "                for (s, e) in ranges\n",
    "            ], dim=0)  \n",
    "            print(f\"Layer: {layer_name}, Feature shape: {activation.shape}, Averaged feature shape: {avg_activation.shape}, Sample: {avg_activation[0,0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff5a4b",
   "metadata": {
    "papermill": {
     "duration": 0.009896,
     "end_time": "2025-04-29T14:40:50.521538",
     "exception": false,
     "start_time": "2025-04-29T14:40:50.511642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Write the extract_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51fa14f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:40:50.542880Z",
     "iopub.status.busy": "2025-04-29T14:40:50.542565Z",
     "iopub.status.idle": "2025-04-29T14:40:50.551739Z",
     "shell.execute_reply": "2025-04-29T14:40:50.550921Z"
    },
    "papermill": {
     "duration": 0.021129,
     "end_time": "2025-04-29T14:40:50.553039",
     "exception": false,
     "start_time": "2025-04-29T14:40:50.531910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "# Define the extractor.\n",
    "extractor = HuggingFaceFeatureExtractor(model, layers_to_extract, detach=True)\n",
    "\n",
    "\n",
    "def select_16_frames(video_section, target_frames=16):\n",
    "    num_frames = video_section.shape[0]\n",
    "    if num_frames >= target_frames:\n",
    "        # Uniformly sample 16 indices from 0 to num_frames - 1\n",
    "        indices = torch.linspace(0, num_frames - 1, steps=target_frames).long()\n",
    "        selected_frames = video_section[indices]\n",
    "    else:\n",
    "        # Repeat frames to reach exactly 16 frames\n",
    "        repeats = target_frames // num_frames\n",
    "        remainder = target_frames % num_frames\n",
    "        repeated_frames = video_section.repeat(repeats, 1, 1, 1)\n",
    "        if remainder > 0:\n",
    "            extra_frames = video_section[:remainder]\n",
    "            repeated_frames = torch.cat([repeated_frames, extra_frames], dim=0)\n",
    "        selected_frames = repeated_frames\n",
    "    return selected_frames\n",
    "\n",
    "def extract_fn(\n",
    "    video: torch.Tensor, \n",
    "    audio: torch.Tensor, \n",
    "    transcript: List[List[str]], \n",
    "    verbose: bool\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    # Modify this function using the feature extractor\n",
    "    # video is a tensor with shape [fps * interval, 3, heigth, width] on fp16 from 0-255\n",
    "    # audio is a tensor with shape [1 if mono 2 if stereo, sampling_rate * interval] on fp16\n",
    "    # transcript is list of strings of words.\n",
    "\n",
    "    dict_return = {}\n",
    "    with torch.no_grad():\n",
    "        pixel_values = video[:,0] # select the first frame of each chunk\n",
    "        pixel_values = load_image(pixel_values).to(torch.bfloat16).cuda()\n",
    "        sentences = transcript\n",
    "        \n",
    "        logits, ranges, avg_logits = logits_by_pair(\n",
    "            extractor, tokenizer,\n",
    "            pixel_values, sentences,\n",
    "            use_template=False   # or True\n",
    "        )\n",
    "        \n",
    "        # print(logits.shape)        # [1, seq_len, 151674]\n",
    "        # print(ranges)              # e.g. [(0,160), (161,280), ...]\n",
    "        # print(avg_logits.shape)    # [n, 151674]\n",
    "\n",
    "        features = extractor.features\n",
    "    \n",
    "        # Iterate over the extracted features and print their shapes.\n",
    "        for layer_name, activation in features.items():\n",
    "            avg_activation = torch.stack([\n",
    "                activation[0, s:e+1].mean(dim=0)              # mean over sequence dimension\n",
    "                for (s, e) in ranges\n",
    "            ], dim=0)  \n",
    "            print(f\"Layer: {layer_name}, Feature shape: {activation.shape}, Averaged feature shape: {avg_activation.shape}, Sample: {avg_activation[0,0:5]}\")\n",
    "            dict_return[layer_name] = avg_activation.to(torch.float16).cpu()\n",
    "    return dict_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea302b",
   "metadata": {
    "papermill": {
     "duration": 0.01077,
     "end_time": "2025-04-29T14:40:50.573645",
     "exception": false,
     "start_time": "2025-04-29T14:40:50.562875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Start extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa34918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:40:50.594231Z",
     "iopub.status.busy": "2025-04-29T14:40:50.593989Z",
     "iopub.status.idle": "2025-04-29T16:37:34.964747Z",
     "shell.execute_reply": "2025-04-29T16:37:34.963593Z"
    },
    "papermill": {
     "duration": 7004.383738,
     "end_time": "2025-04-29T16:37:34.967325",
     "exception": false,
     "start_time": "2025-04-29T14:40:50.583587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e21b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e21b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e04a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e04a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e08b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e08b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e19a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e19a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e23b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e23b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e19b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e19b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e02b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e02b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e10a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e10a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e20b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e20b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e02a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e02a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e13a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e13a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e18a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e18a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e09a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e09a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e17b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e17b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e13b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e13b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e20a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e20a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e22a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e22a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e18b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e18b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e07a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e07a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e11a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e11a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e06a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e06a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e12a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e12a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e22b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e22b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e03b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e03b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e05a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e05a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e24a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e24a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e24b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e24b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e23a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e23a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e14a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e14a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e01b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e01b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e05b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e05b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e09b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e09b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e06b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e06b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e01a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e01a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e15b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e15b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e12b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e12b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e07b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e07b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e04b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e04b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e10b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e10b.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e16a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e16a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e21a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e21a.tsv\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e14b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e14b.tsv\n",
      "Total number of frames in the video: 21931.0\n",
      "Original Resolution: (720.0, 480.0)\n",
      "FPS: 29.968454258675077\n",
      "Duration (seconds): 731.8028421052632\n",
      "Target Resolution: (224, 224)\n",
      "Read 21931 frames.\n",
      "Frames shape: torch.Size([21931, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6f7064069995>:50: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 731.81 seconds\n",
      "Number of intervals: 491\n",
      "Sample rate: 48000\n",
      "Output file: /kaggle/working/friends/s1/friends_s01e14b.h5\n",
      "Num splits: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/71 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 1/71 [00:11<13:51, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1260, -0.0693,  0.0708, -0.5938, -0.4180], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832, -0.2949, -0.0640, -0.1943, -0.0228], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0315,  0.0432, -0.5391, -0.1826, -0.6172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.3594, -0.3477, -1.4453, -0.0173, -1.4688], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/71 [00:23<13:49, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.0977, -0.1426, -0.1973,  0.1089], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3340,  0.1865, -0.2383, -0.1934, -0.0327], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1230,  0.1953, -0.8945, -0.1729, -0.5000], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6172,  1.0078, -0.5508,  0.9414, -0.0767], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/71 [00:36<13:55, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.1875, -0.0996, -0.1670,  0.0796], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.2031,  0.0339, -0.1055, -0.0684], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.1309, -0.7266, -0.0947, -0.6953], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5508,  0.3926, -0.7227,  0.8945, -0.4375], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/71 [00:49<13:49, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2012,  0.0566, -0.2324, -0.1182,  0.2891], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2891,  0.0903, -0.1895, -0.0649, -0.0315], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1133,  0.1826, -0.9492, -0.0181, -0.6172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7188, -0.0317, -0.8594,  1.2891, -0.6758], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/71 [01:01<13:32, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0317,  0.1816,  0.0762, -0.1396,  0.0447], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.1641, -0.0444, -0.1084,  0.0703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1611,  0.1982, -0.7422, -0.1025, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1387,  0.2930, -0.3652,  1.0859,  0.4023], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 6/71 [01:13<13:18, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0640,  0.1484, -0.1943, -0.1416,  0.1050], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3359,  0.1011, -0.0170, -0.0247, -0.0630], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.1279, -0.8828,  0.0278, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.2891e-01, -1.6797e-01, -2.0020e-01,  1.6797e+00,  7.4863e-05],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 7/71 [01:25<13:09, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1592,  0.2129, -0.0815, -0.1992,  0.1060], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2314,  0.2412,  0.0152, -0.1289,  0.0057], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3418,  0.1221, -0.7461, -0.1211, -0.6797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4297,  0.5000, -0.6562,  1.2422, -0.3867], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 8/71 [01:39<13:11, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.4824,  0.0035, -0.1602,  0.3203], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2314,  0.3086, -0.2266, -0.1396, -0.1089], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.2061, -0.8320, -0.1211, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5078,  0.4238, -1.3516,  1.6016, -0.1279], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/71 [01:52<13:10, 12.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3320,  0.1680, -0.1074, -0.1191,  0.4160], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3730,  0.0150, -0.1113, -0.1396, -0.0015], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1875,  0.0723, -0.8516, -0.1152, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1128,  0.4707, -0.3320,  1.8828, -0.5352], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10/71 [02:04<12:57, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0654,  0.0408, -0.2715, -0.1504,  0.1113], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.1099, -0.1035, -0.1777, -0.0161], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.1904, -0.8516, -0.1299, -0.7812], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.6562, -0.7500,  1.1016, -0.6484], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 11/71 [02:17<12:47, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.2178, -0.1436, -0.2695,  0.1865], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.1299, -0.1338, -0.0737,  0.0025], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1147,  0.1543, -0.7812, -0.0708, -0.5898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0203, -0.2676, -1.0547,  1.3594, -0.1826], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/71 [02:30<12:35, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3535,  0.2129, -0.2295, -0.2305,  0.0703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1992,  0.1846, -0.3164, -0.2002, -0.0688], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0457,  0.1973, -0.8633, -0.2158, -0.5469], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1445,  1.2188, -1.2891,  1.1797, -0.3965], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 13/71 [02:43<12:26, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3635, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3848,  0.2354, -0.1768, -0.1748,  0.1128], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3635, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2021,  0.1367, -0.3828, -0.0767, -0.0388], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3635, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0356,  0.1982, -0.9219, -0.1069, -0.4375], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3635, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7578,  0.5977, -1.2188,  1.7109, -0.0693], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 14/71 [02:56<12:17, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3639, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2148,  0.1035, -0.3281, -0.2871,  0.0273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3639, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1406,  0.0996, -0.1738, -0.2383, -0.0649], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3639, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1187,  0.1514, -0.7852, -0.2695, -0.4941], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3639, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0762,  0.5117, -0.1875,  0.6211, -0.3184], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 15/71 [03:10<12:15, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.1787, -0.2910, -0.2520,  0.0762], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2451,  0.0957, -0.1641, -0.2139, -0.0432], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1699,  0.2100, -0.8945, -0.1455, -0.4902], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.7734, -0.5859,  0.8594,  0.2314], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/71 [03:23<12:05, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3705, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.1523, -0.4238, -0.2832,  0.0352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3705, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-2.9297e-01,  1.7834e-04, -4.3457e-02, -1.8555e-01,  9.7168e-02],\n",
      "       device='cuda:1', dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3705, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2129,  0.1846, -0.8008, -0.1318, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3705, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949,  0.5703, -0.3457,  0.9258, -0.0447], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 17/71 [03:36<11:51, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2051,  0.1270, -0.4199, -0.2637, -0.0388], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1846,  0.1553, -0.1953, -0.1504, -0.0688], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1064,  0.1167, -0.9570, -0.1650, -0.4648], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0427,  0.7461, -0.3633,  1.5391, -0.3711], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 18/71 [03:49<11:37, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1611,  0.1484, -0.3164, -0.2988, -0.0153], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1221,  0.0679, -0.2461, -0.1631,  0.0383], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1348,  0.1045, -0.8594, -0.0762, -0.3926], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1621,  0.6992, -0.2578,  1.1016, -0.3008], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 19/71 [04:03<11:26, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2227,  0.1816, -0.4238, -0.3027, -0.0267], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1953,  0.0947, -0.0913, -0.2061,  0.0198], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1089,  0.2236, -0.8789, -0.1187, -0.4434], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0066,  0.3262, -0.6523,  0.8477, -0.4141], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 20/71 [04:16<11:12, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.1943, -0.2734, -0.2910,  0.0889], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.1357, -0.1445, -0.2334,  0.0106], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0986,  0.1846, -0.8555, -0.2246, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1006,  0.2930, -0.6484,  0.8047, -0.3086], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 21/71 [04:29<10:59, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2256,  0.1582, -0.1768, -0.2734,  0.0014], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.1709, -0.0035, -0.2197,  0.0432], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1631,  0.1650, -0.8242, -0.1768, -0.4688], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3027,  0.4082, -0.5430,  0.8672, -0.1416], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 22/71 [04:42<10:48, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1191,  0.1904, -0.1084, -0.3105, -0.0493], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.1855, -0.0388, -0.2402,  0.1270], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.1572, -0.8555, -0.1377, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0850,  0.0479, -0.7500,  0.7383, -0.4688], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 23/71 [04:56<10:38, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3262,  0.0693, -0.5625, -0.2383, -0.1494], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1641,  0.1309, -0.2383, -0.0884, -0.1621], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1670,  0.2109, -0.8867, -0.0254, -0.4531], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.3516, -0.5156,  0.8945, -0.5391], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 24/71 [05:09<10:24, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2031,  0.1465, -0.4199, -0.2930,  0.0576], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1592,  0.2031, -0.0693, -0.2129,  0.0068], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.2012, -0.8320, -0.1758, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5586,  0.7617, -0.1484,  1.0234, -0.5664], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 25/71 [05:23<10:14, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2422,  0.1504, -0.2188, -0.2100,  0.1260], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1128, -0.1357, -0.1650, -0.0121], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0593,  0.1187, -0.8711, -0.1689, -0.6484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1235,  0.1787, -0.5508,  0.6680, -0.2578], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 26/71 [05:36<10:06, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2285, -0.0457, -0.0996,  0.0282,  0.1553], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2168,  0.2422, -0.0913, -0.0184, -0.0129], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.2441, -0.5391, -0.0806, -0.6406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1426, -0.1318, -0.9062,  0.6719,  0.1611], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 27/71 [05:50<09:51, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.0579, -0.1885, -0.0884,  0.3887], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1602,  0.1118, -0.2236, -0.0864, -0.1709], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.1982, -0.5977, -0.1270, -0.4531], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7109, -0.6680, -0.5703,  1.2344,  0.6641], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 28/71 [06:03<09:39, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2344,  0.1719, -0.0554, -0.0903,  0.1797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2012,  0.1152, -0.1670, -0.0123, -0.1748], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1514,  0.0117, -0.9180, -0.0085, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0334,  0.3379, -0.9648,  1.1172,  0.2119], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 29/71 [06:17<09:28, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1396,  0.2559, -0.1069, -0.0233,  0.3535], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1846,  0.1377, -0.0317,  0.0140, -0.1494], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1377,  0.1436, -0.8867,  0.0898, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7734,  0.0830, -0.6641,  1.6094,  0.4922], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 30/71 [06:30<09:11, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0796,  0.0491, -0.2129, -0.0165,  0.1187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.1738, -0.2012, -0.0344,  0.0962], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.2930, -0.8008,  0.0070, -0.5547], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0723,  0.1387, -1.1562,  1.0469,  0.1338], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 31/71 [06:44<08:57, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4395,  0.0723, -0.1836, -0.1060,  0.2031], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1206,  0.0227, -0.3164, -0.1201, -0.0884], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1211,  0.1289, -0.7461, -0.1108, -0.4355], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4551, -1.3516, -0.3008,  0.6445,  0.9922], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 32/71 [06:57<08:44, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3633,  0.1128, -0.1152, -0.0131,  0.1934], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.0276, -0.2949, -0.1279, -0.0723], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1074,  0.1895, -0.7617, -0.0649, -0.4258], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6094, -1.6172, -0.3301,  0.8633,  0.9648], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 33/71 [07:11<08:31, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1904,  0.0327, -0.2832, -0.1328,  0.2051], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3535,  0.2793, -0.2617, -0.0320,  0.1289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2256,  0.2637, -0.7891,  0.0991, -0.4668], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3398,  0.6055, -1.0625,  1.7500, -0.1533], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 34/71 [07:24<08:16, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2500,  0.2832, -0.3164, -0.1719,  0.2207], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3203,  0.3398, -0.3145, -0.0566,  0.0479], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1738,  0.3594, -0.9258,  0.1040, -0.5078], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1118,  1.4375, -1.2031,  2.4688, -0.2402], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 35/71 [07:37<08:02, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2490,  0.1709, -0.3574, -0.2754,  0.2422], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2373,  0.2637, -0.3086, -0.1816,  0.0977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0317,  0.2754, -0.8555, -0.1416, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3359,  1.1172, -1.2344,  1.5000, -0.2178], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 36/71 [07:51<07:47, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2324,  0.2148, -0.1289, -0.1133,  0.2275], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.2656, -0.1973,  0.0088,  0.1348], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1289,  0.2637, -0.9141,  0.0991, -0.5039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0136,  1.4844, -1.6016,  2.4531,  0.2109], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 37/71 [08:04<07:36, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.1367, -0.1572, -0.0801,  0.2227], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.1924, -0.3008,  0.0132,  0.0737], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0417,  0.2676, -0.9141,  0.0957, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2158,  1.2422, -1.3594,  2.3438, -0.0356], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 38/71 [08:17<07:20, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3047,  0.1875, -0.2695, -0.1943,  0.2373], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1738,  0.2891, -0.3125, -0.1123,  0.0574], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0515,  0.2402, -0.9258, -0.0391, -0.6094], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3125,  1.3125, -1.0859,  1.7188,  0.0479], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 39/71 [08:31<07:06, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1934,  0.2061, -0.1514, -0.1660,  0.2031], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949,  0.2695, -0.1104, -0.0471,  0.2051], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2031,  0.3145, -0.8750,  0.1035, -0.4414], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2119,  1.0469, -1.6094,  2.5156, -0.1064], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 40/71 [08:44<06:53, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1621,  0.2109, -0.1797, -0.1592,  0.0928], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3066,  0.2295, -0.1680, -0.0374,  0.2207], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0698,  0.2812, -0.8359,  0.0265, -0.4707], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1187,  1.1484, -1.7266,  1.9766,  0.0342], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 41/71 [08:57<06:37, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0547,  0.2949, -0.1992, -0.2002,  0.1289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3125,  0.3398,  0.0192, -0.1270,  0.3047], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2119,  0.2539, -0.8516, -0.0320, -0.4512], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4316,  1.3516, -1.2500,  2.0938, -0.2754], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 42/71 [09:11<06:26, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1084,  0.2158, -0.2139, -0.1953,  0.2158], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3594,  0.2910, -0.1147, -0.0520,  0.2197], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1328,  0.2695, -0.8633,  0.0413, -0.4922], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0058,  1.2266, -1.3125,  2.1719,  0.1177], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 43/71 [09:24<06:10, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3632, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.1572, -0.1187, -0.1855,  0.3301], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3632, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3379,  0.2793, -0.1338, -0.0840,  0.1572], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3632, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0737,  0.2695, -0.9648, -0.0073, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3632, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4102,  1.5469, -1.5859,  2.2344, -0.0520], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 44/71 [09:37<05:56, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1387,  0.1875, -0.0884, -0.2969,  0.1270], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1533,  0.2812, -0.3496, -0.1992,  0.0013], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0283,  0.2305, -0.9805, -0.1309, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949,  0.3652, -0.9805,  1.0000,  0.3125], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 45/71 [09:50<05:46, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1855, -0.0588, -0.2988, -0.1816,  0.0874], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1416,  0.2432, -0.1631, -0.0273, -0.0781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0884,  0.2412, -0.8711,  0.0908, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.0938,  0.8164, -0.2676,  1.1250,  0.3555], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 46/71 [10:04<05:36, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.0503, -0.0427, -0.3047,  0.0679], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2490,  0.0898, -0.1299, -0.2393,  0.0366], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2402,  0.1138, -0.8984, -0.2168, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1875,  0.3906, -1.0781,  0.5078, -0.7852], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 47/71 [10:18<05:23, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2432,  0.0123, -0.2168, -0.2832,  0.1230], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.1650, -0.2969, -0.2041,  0.1187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1816,  0.1787, -0.8320, -0.2090, -0.5117], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3398,  0.1895, -1.1250,  0.4961, -0.6367], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 48/71 [10:31<05:10, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.0223, -0.3047, -0.3516, -0.0603], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2080,  0.0742, -0.1270, -0.2207,  0.0496], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1504,  0.1196, -0.8164, -0.2197, -0.4199], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1021,  0.4551, -0.5195,  0.6289, -0.4375], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 49/71 [10:45<04:56, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.0608, -0.1572, -0.2148,  0.0698], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.1328, -0.1138, -0.1807,  0.1050], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1455,  0.1523, -0.7891, -0.1631, -0.4727], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2480,  0.6211, -0.5898,  0.5586, -0.5430], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 50/71 [10:58<04:45, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2373,  0.0009, -0.2559, -0.2090,  0.1934], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1396, -0.1602, -0.1406,  0.0097], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1318,  0.1729, -0.8516, -0.1436, -0.4766], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2988,  0.6680, -1.0078,  0.9922, -0.4141], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 51/71 [11:12<04:31, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2383,  0.0967, -0.1436, -0.3184,  0.1426], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.0859,  0.0339, -0.2246,  0.1309], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2168,  0.1709, -0.8398, -0.2256, -0.4141], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0967,  0.6797, -1.0234,  0.5977, -0.7344], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 52/71 [11:26<04:19, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2002, -0.0952, -0.2197, -0.2715,  0.0864], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1836,  0.0728, -0.1602, -0.1523,  0.0410], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0574,  0.2363, -0.8203, -0.0806, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3340,  0.7539, -0.3828,  0.8398, -0.7070], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 53/71 [11:40<04:08, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2988,  0.0435, -0.1611, -0.3145,  0.0854], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.1064, -0.2168, -0.2061,  0.0942], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1650,  0.1807, -0.8516, -0.2021, -0.4004], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2197,  0.4297, -1.1562,  0.4453, -0.7344], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 54/71 [11:54<03:55, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.0598, -0.2461, -0.3184,  0.0820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.1016, -0.1895, -0.2363,  0.1196], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1484,  0.1797, -0.8594, -0.2451, -0.4375], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.5703, -0.9102,  0.5508, -0.8320], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 55/71 [12:08<03:42, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.1357, -0.1611, -0.3125,  0.1079], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.1006, -0.1182, -0.2578,  0.1187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2188,  0.1738, -0.8398, -0.2490, -0.4668], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0474,  0.2578, -0.9141,  0.7500, -0.5430], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 56/71 [12:22<03:27, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3535,  0.1289, -0.4258, -0.2871,  0.0447], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1602,  0.0068, -0.2773, -0.1650, -0.0129], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1787,  0.1377, -0.8867, -0.1680, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6094,  0.6094, -0.3652,  0.8438, -0.5156], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 57/71 [12:35<03:12, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3438,  0.1299, -0.4023, -0.3027,  0.0776], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1709,  0.0776, -0.2246, -0.1602,  0.0249], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1416,  0.2051, -0.8789, -0.1377, -0.5547], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2021,  0.8984, -0.4766,  0.9922, -0.8086], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 58/71 [12:49<02:57, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.0056, -0.4492, -0.2393,  0.0884], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.0359, -0.2930, -0.1885,  0.0693], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2461,  0.1836, -0.7227, -0.2373, -0.5117], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.3984,  0.8711, -0.9492,  0.3828, -1.2188], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 59/71 [13:03<02:44, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2363,  0.1260, -0.4238, -0.2432,  0.1035], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1562,  0.0967, -0.4160, -0.2148,  0.0903], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1387,  0.1719, -0.8516, -0.2559, -0.4609], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2002,  1.1562, -0.4355,  0.3438, -0.5508], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 60/71 [13:16<02:30, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2988,  0.1338, -0.4590, -0.2422,  0.0693], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1289,  0.1235, -0.3379, -0.1328,  0.0496], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1484,  0.2695, -0.8672, -0.0957, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1816,  0.8242, -0.6914,  1.4297, -0.7734], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 61/71 [13:30<02:18, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1416,  0.1729, -0.1079, -0.3145,  0.0996], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.1016, -0.0762, -0.2422,  0.1270], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2158,  0.1660, -0.8086, -0.1963, -0.4824], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3496,  0.5078, -0.9648,  0.6953, -0.8906], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 62/71 [13:45<02:05, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3638, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.1021, -0.2285, -0.2520,  0.1689], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3638, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2051,  0.2871, -0.0913, -0.1699, -0.1436], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3638, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1699,  0.1738, -0.8008, -0.1309, -0.6680], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3638, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.4375, -0.5234,  1.0234, -0.3770], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 63/71 [13:58<01:50, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1299,  0.2354, -0.2402, -0.0840, -0.3262], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0483, -0.0854, -0.3926, -0.0121, -0.1777], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1196,  0.1050, -0.7852,  0.2129, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0859, -0.0537, -0.7656,  1.1250,  0.4531], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 64/71 [14:12<01:36, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1406,  0.0339, -0.1992, -0.1719,  0.1670], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2314,  0.3535, -0.0942, -0.0623, -0.0684], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2363,  0.2949, -0.7773,  0.0117, -0.6797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1074,  0.2949, -1.5781,  1.3516, -0.5664], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 65/71 [14:26<01:23, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1689,  0.0188, -0.2285, -0.1992,  0.2676], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.1021, -0.0918, -0.1455,  0.0942], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.2236, -0.8086, -0.2158, -0.5547], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.2773, -0.4785,  0.6797,  0.9492], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 66/71 [14:40<01:09, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0005,  0.1182, -0.0674, -0.1895,  0.1934], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2422,  0.3301, -0.2188, -0.0737,  0.0791], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2109,  0.2598, -0.8984, -0.0439, -0.5078], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5352,  0.3262, -1.2812,  0.9336, -0.1982], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 67/71 [14:54<00:55, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1104,  0.0010, -0.1855, -0.1592,  0.1992], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2256,  0.2988, -0.0269, -0.0718, -0.0928], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1592,  0.3086, -0.7812,  0.0118, -0.6289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0864,  0.0447, -1.2500,  1.1562, -0.3457], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 68/71 [15:08<00:41, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.2715,  0.0513, -0.1025,  0.1021], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.2969, -0.2471, -0.0415, -0.0874], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.2285, -0.6992, -0.0018, -0.6016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5117,  0.2734, -0.7500,  1.4922,  0.0547], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 69/71 [15:22<00:27, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0205,  0.1416, -0.0981, -0.0515, -0.0957], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2500,  0.1904, -0.0840, -0.0413, -0.0703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3418, -0.0029, -0.6172, -0.0168, -0.6172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0369,  0.2676, -0.6172,  1.2656, -0.2354], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 70/71 [15:28<00:11, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 2092, 3584]), Averaged feature shape: torch.Size([8, 3584]), Sample: tensor([-0.3828,  0.0287, -0.6484, -0.0796,  0.1992], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 2092, 3584]), Averaged feature shape: torch.Size([8, 3584]), Sample: tensor([-0.2227,  0.1016, -0.1406, -0.1245, -0.2217], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 2092, 3584]), Averaged feature shape: torch.Size([8, 3584]), Sample: tensor([-0.1699,  0.2656, -0.7305, -0.1230, -0.4785], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 2092, 3584]), Averaged feature shape: torch.Size([8, 3584]), Sample: tensor([-0.4688, -0.2754, -1.1953,  0.1777,  0.6133], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [15:29<00:00, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 259, 3584]), Averaged feature shape: torch.Size([1, 3584]), Sample: tensor([-0.1079, -0.0688,  0.0481, -0.6094, -0.4043], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 259, 3584]), Averaged feature shape: torch.Size([1, 3584]), Sample: tensor([-0.2930, -0.3066, -0.0199, -0.2002, -0.0427], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 259, 3584]), Averaged feature shape: torch.Size([1, 3584]), Sample: tensor([ 0.0251,  0.0084, -0.5312, -0.1885, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 259, 3584]), Averaged feature shape: torch.Size([1, 3584]), Sample: tensor([ 1.4219, -0.2637, -1.5469,  0.0292, -1.4922], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e11b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e11b.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames in the video: 21280.0\n",
      "Original Resolution: (720.0, 480.0)\n",
      "FPS: 29.968454258675077\n",
      "Duration (seconds): 710.08\n",
      "Target Resolution: (224, 224)\n",
      "Read 21280 frames.\n",
      "Frames shape: torch.Size([21280, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6f7064069995>:50: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 710.09 seconds\n",
      "Number of intervals: 476\n",
      "Sample rate: 48000\n",
      "Output file: /kaggle/working/friends/s1/friends_s01e11b.h5\n",
      "Num splits: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 1/69 [00:13<15:09, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1133, -0.0549,  0.0742, -0.5938, -0.3945], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793, -0.2852, -0.0020, -0.1895, -0.0292], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0405,  0.0381, -0.5312, -0.1504, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.3047, -0.2598, -1.4922,  0.1309, -1.5000], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/69 [00:27<15:06, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3535,  0.2236, -0.3105, -0.2305,  0.1758], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2402,  0.2656,  0.0967, -0.1934, -0.2041], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1826,  0.3945, -0.8242, -0.2617, -0.6562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.0078,  0.7617, -0.6875,  0.4648,  0.0077], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/69 [00:40<14:53, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.1953, -0.2754, -0.0361,  0.0503], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1846,  0.2012, -0.1348,  0.0034, -0.0850], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1299,  0.1631, -0.6367, -0.0664, -0.4902], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0029,  0.0674, -1.1719,  0.6445, -0.6133], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/69 [00:53<14:30, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0967,  0.1680, -0.1768, -0.1211,  0.2422], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2373,  0.1953, -0.1533, -0.0085, -0.0347], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1445,  0.2500, -0.6289, -0.0100, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0728, -0.3262, -0.8242,  1.3984, -0.7227], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/69 [01:07<14:20, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0762,  0.1865, -0.1021, -0.2266,  0.2871], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.1475,  0.1289, -0.1807,  0.0581], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1807,  0.0981, -0.8516, -0.1270, -0.5469], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7109,  0.1533, -0.2949,  1.2188, -0.2266], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/69 [01:20<14:04, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2383,  0.1006, -0.0635, -0.0840,  0.2637], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1875,  0.0972, -0.1299, -0.0142, -0.1104], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.1157, -0.7617, -0.0786, -0.2754], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.0312,  0.5234, -1.1719,  1.3672, -0.0420], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/69 [01:34<13:53, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1045,  0.1787, -0.1250,  0.0850,  0.1689], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2480,  0.0762, -0.0771,  0.0292, -0.0036], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1113,  0.1611, -0.8438,  0.0603, -0.6406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6523,  0.7109, -1.1719,  1.1328,  0.5000], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/69 [01:47<13:44, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1240,  0.1367,  0.0903,  0.0209,  0.1348], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.0791, -0.1147,  0.0102, -0.0287], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1885,  0.2285, -0.8398,  0.0171, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6172,  0.7148, -1.0625,  1.2344,  0.4863], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/69 [02:01<13:37, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217,  0.0674,  0.0728, -0.0249,  0.1436], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3320,  0.0540, -0.1621, -0.0586, -0.0029], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0483,  0.2246, -0.8398, -0.0547, -0.5547], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6094,  1.3672, -0.8750,  1.0234,  0.1748], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10/69 [02:15<13:19, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.0977,  0.1973,  0.0116,  0.3320], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2461,  0.0137, -0.1318,  0.0601, -0.1055], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1187,  0.1895, -0.8750,  0.0767, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7031,  1.1328, -0.7383,  1.7578,  0.4863], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 11/69 [02:28<13:09, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217,  0.2812, -0.0391, -0.1973,  0.0366], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.0820,  0.1338, -0.1416, -0.0410], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1318,  0.1719, -0.8164, -0.1084, -0.4941], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5547,  0.0198, -0.9180,  0.1953, -0.2285], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/69 [02:42<13:01, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1279,  0.1699, -0.4062, -0.1167,  0.2227], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3945,  0.1660, -0.1465, -0.0737,  0.0427], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3125,  0.2812, -0.7344, -0.1104, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0786, -0.1108, -1.4766,  0.0090, -0.1846], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 13/69 [02:56<12:54, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.2598, -0.0233, -0.2578,  0.1572], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1865,  0.0148,  0.1572, -0.1709, -0.1069], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0047,  0.0757, -0.8047, -0.2178, -0.4941], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6992,  0.3867, -1.4844,  0.2754,  0.0610], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 14/69 [03:10<12:44, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4805,  0.2471, -0.1377, -0.2393,  0.3145], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.0928, -0.0952, -0.2012, -0.1147], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1108,  0.2109, -0.8555, -0.2676, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4980,  0.7461, -1.1406,  0.5039, -0.1338], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 15/69 [03:25<12:34, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3723, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1543,  0.2500, -0.0991, -0.1094,  0.1641], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3723, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2471,  0.1235, -0.2363, -0.0430,  0.0264], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3723, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.1270, -0.7969, -0.0053, -0.6172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3723, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217, -0.0781, -0.6914,  0.7109,  0.1953], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/69 [03:39<12:21, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3750,  0.1797,  0.0571, -0.2793,  0.2295], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2383,  0.0039, -0.0388, -0.2490, -0.0942], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1699,  0.1226, -0.8125, -0.2969, -0.4902], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8633,  0.2314, -0.7695,  0.2354, -0.1641], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 17/69 [03:52<12:03, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1094,  0.1562, -0.0767, -0.0874, -0.0172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.1396, -0.0309, -0.0520, -0.1279], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2148,  0.2578, -0.6914, -0.0791, -0.6992], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.4414, -1.1406,  0.2812, -0.4258], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 18/69 [04:06<11:45, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1855,  0.0947, -0.2910, -0.1338, -0.1187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.1572,  0.0036, -0.1504, -0.1396], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2422,  0.1973, -0.7539, -0.1621, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4336,  0.8203, -0.8203,  0.3145, -0.2559], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 19/69 [04:20<11:29, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.2930,  0.0640, -0.1602,  0.2734], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.2109, -0.3848, -0.0796, -0.0986], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.1621e-01,  1.9727e-01, -9.2578e-01, -4.5776e-04, -5.2344e-01],\n",
      "       device='cuda:1', dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1367,  0.6094, -0.8828,  1.8906,  0.1309], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 20/69 [04:33<11:10, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1953,  0.2451, -0.0903, -0.1777,  0.2852], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2148,  0.2910, -0.1118, -0.0659, -0.0439], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1562,  0.3047, -0.8008,  0.0045, -0.4414], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1426,  0.3281, -0.8125,  1.8203,  0.0227], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 21/69 [04:47<10:59, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1650,  0.1631, -0.2461, -0.0613,  0.0413], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.2295, -0.1895, -0.0248, -0.0442], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1523,  0.2256, -0.8242,  0.1289, -0.5625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0089, -0.0903, -1.2734,  1.9688, -0.3555], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 22/69 [05:00<10:42, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3105,  0.2578, -0.1025, -0.1914,  0.3926], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3262,  0.2441, -0.3574, -0.0698, -0.0267], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0386,  0.2402, -0.8203, -0.0138, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.3418,  0.5391, -0.7734,  1.9062,  0.1416], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 23/69 [05:14<10:21, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2344,  0.0938, -0.0522, -0.2012,  0.3047], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.2402, -0.3262, -0.0571, -0.0806], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1699,  0.2354, -0.8125,  0.0425, -0.5469], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1553,  0.3379, -0.5859,  1.9062, -0.4883], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 24/69 [05:27<10:10, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2061,  0.1982, -0.1084, -0.1895,  0.2832], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.2354, -0.0422, -0.0811,  0.0623], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1953,  0.2461, -0.9219,  0.0806, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5195,  0.6562, -1.6094,  2.1875,  0.3711], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 25/69 [05:41<09:59, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1338,  0.1396, -0.1211, -0.0527,  0.2246], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2471,  0.3301, -0.2715,  0.0028, -0.0554], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.2578, -0.6875,  0.1436, -0.5898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4629,  0.5781, -0.6758,  1.7266, -0.2949], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 26/69 [05:55<09:47, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0564,  0.0791, -0.2266,  0.0510,  0.2578], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.2051, -0.1562,  0.0233,  0.0442], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.2422, -0.7812,  0.1279, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5586,  0.4180, -1.5469,  1.9844, -0.2402], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 27/69 [06:08<09:33, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1396,  0.0908, -0.1924,  0.0282,  0.3984], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3594,  0.2617, -0.1934,  0.0242,  0.0669], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3223,  0.2432, -0.7031,  0.1079, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4102,  0.4688, -1.0391,  1.5078, -0.6680], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 28/69 [06:22<09:21, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2021,  0.2041, -0.1318, -0.1709,  0.0835], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.2871, -0.1436, -0.0549,  0.0781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1504,  0.2129, -0.8359,  0.0269, -0.4004], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2773,  0.4629, -1.4062,  1.6250, -0.4336], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 29/69 [06:36<09:12, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1416,  0.0820, -0.1738, -0.1953,  0.1270], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2334,  0.2695,  0.0189, -0.1562,  0.0259], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1660,  0.2930, -0.9023, -0.0623, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1299,  0.3496, -0.9375,  1.5156, -0.6406], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 30/69 [06:50<09:01, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2363,  0.1660, -0.2754, -0.1875,  0.0801], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1924,  0.2178, -0.2852, -0.0635, -0.0664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1118,  0.2578, -0.7578, -0.0162, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0334,  0.4023, -1.4219,  1.5156, -0.1289], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 31/69 [07:04<08:45, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3712, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.1201, -0.1387, -0.1709,  0.2715], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3712, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2197,  0.1436, -0.0613, -0.0493, -0.0059], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3712, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0417,  0.1777, -0.8984,  0.0811, -0.6484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3712, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4062,  0.6719, -1.2266,  1.6406,  0.1895], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 32/69 [07:18<08:32, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2256,  0.0938, -0.2656, -0.1455,  0.0771], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.3125, -0.2656, -0.0122, -0.0481], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.2275, -0.7578,  0.0933, -0.4961], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1533,  0.9219, -1.6719,  1.6953, -0.3613], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 33/69 [07:32<08:18, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3701, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.3242,  0.0554, -0.3242,  0.2559], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3701, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2471,  0.0977, -0.0554, -0.1465, -0.0101], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3701, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0913,  0.2158, -0.8281, -0.1094, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3701, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3398,  0.6367, -1.5547,  1.4766, -0.2832], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 34/69 [07:46<08:11, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3718, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1641,  0.3223,  0.0317, -0.2988,  0.2207], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3718, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.0669, -0.0393, -0.1885, -0.0127], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3718, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1543,  0.1768, -0.8281, -0.1895, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3718, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4199,  0.6953, -1.3516,  0.4805, -0.2295], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 35/69 [08:00<07:55, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2451,  0.4102,  0.0103, -0.2832,  0.3066], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2266,  0.0693, -0.0342, -0.1611,  0.0415], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0513,  0.2236, -0.7812, -0.1396, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5156,  0.9219, -1.4844,  0.7773, -0.2344], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 36/69 [08:14<07:39, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2422,  0.0830, -0.4238, -0.2256,  0.2246], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.3359, -0.3496, -0.1006,  0.0820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1855,  0.4082, -0.7031,  0.0302, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  1.0391, -1.1875,  1.3203, -0.6211], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 37/69 [08:28<07:30, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1992,  0.3789, -0.2266, -0.0791,  0.2656], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2383,  0.0635,  0.0708, -0.0540, -0.0079], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0869,  0.1631, -0.9336,  0.0242, -0.4453], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8008,  0.2305, -1.0938,  1.4766, -0.0840], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 38/69 [08:43<07:19, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3713, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3301,  0.1543, -0.1367, -0.0859,  0.0234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3713, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2021,  0.2129,  0.0957, -0.0366, -0.2412], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3713, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.2773, -0.6641, -0.0383, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3713, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6484, -0.0019, -0.8984,  1.3828,  0.0236], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 39/69 [08:57<07:04, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3613,  0.2969, -0.0033,  0.0065,  0.3633], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1260, -0.1455, -0.0013, -0.2129], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0938,  0.1670, -0.8516, -0.0598, -0.5000], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4453,  0.3242, -0.8047,  1.6641,  0.3008], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 40/69 [09:10<06:44, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3047,  0.3711, -0.0408,  0.0303,  0.3477], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3027,  0.1060, -0.0796,  0.0337, -0.2070], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0977,  0.1514, -0.8594,  0.0248, -0.5000], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7852,  0.6875, -0.8750,  1.4844,  0.2314], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 41/69 [09:24<06:26, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3711,  0.3750, -0.1504,  0.0057,  0.3730], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3066,  0.0820, -0.0588,  0.0199, -0.1562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1406,  0.1953, -0.8945, -0.0027, -0.5195], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5664,  0.4805, -0.8867,  1.6094,  0.2598], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 42/69 [09:37<06:11, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.3359, -0.1562,  0.0444,  0.3574], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.0381,  0.0454,  0.0231, -0.1152], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0957,  0.1885, -0.8906,  0.0064, -0.4766], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7422,  0.4355, -0.7422,  1.5078,  0.3145], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 43/69 [09:51<05:59, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.1934, -0.2676, -0.0510,  0.1172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.2852, -0.1099, -0.0108, -0.1426], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3652,  0.3301, -0.6758,  0.0197, -0.5000], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.0938,  0.8086, -0.7969,  1.3828, -0.2812], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 44/69 [10:06<05:48, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4258,  0.2695, -0.0576,  0.0388,  0.3379], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.0471, -0.1104,  0.0226, -0.1504], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494,  0.2012, -0.8789,  0.0170, -0.4473], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4863,  0.2373, -1.1250,  1.7344,  0.0359], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 45/69 [10:19<05:32, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3535,  0.2148, -0.4121, -0.1494,  0.2969], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2383,  0.0195, -0.1572, -0.0693, -0.1992], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0430,  0.0601, -1.0703,  0.0405, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4102, -0.3301, -1.0938,  1.2031,  0.1318], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 46/69 [10:33<05:18, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3711, -0.1602, -0.4629, -0.2520,  0.0811], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3945, -0.0869,  0.0236, -0.3262, -0.2500], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1299,  0.1011, -1.0391, -0.4922, -0.4902], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.7266, -0.5938, -1.1797,  0.8789, -0.3340], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 47/69 [10:47<05:02, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1465,  0.0109, -0.1611, -0.1553,  0.0130], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3340,  0.0903,  0.0229, -0.1157, -0.1416], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1650,  0.1631, -0.8594, -0.0806, -0.7891], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3340,  0.8945, -0.5156,  0.6133,  0.0282], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 48/69 [11:00<04:44, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1758,  0.1816, -0.3496, -0.1514,  0.0796], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3496,  0.0957, -0.0276, -0.0576,  0.0464], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1924,  0.1436, -0.7383,  0.0498, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4512,  0.2969, -0.7422,  0.9297, -0.2480], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 49/69 [11:14<04:33, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1338,  0.2412, -0.4355, -0.2559, -0.0513], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.2656,  0.1621, -0.1118, -0.0967], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1807,  0.2207, -0.6484, -0.1318, -0.5664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3945,  0.5195, -0.8555, -0.2119, -0.0742], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 50/69 [11:27<04:18, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0371,  0.1650, -0.0623, -0.3750,  0.1396], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3125,  0.0544, -0.1406, -0.3164,  0.0942], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2285,  0.1924, -0.7188, -0.3574, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4570,  0.5352, -1.0156,  0.1123,  0.6250], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 51/69 [11:41<04:05, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1226,  0.2412, -0.1050, -0.2354,  0.1533], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3691,  0.0317, -0.0576, -0.1177,  0.1797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0977,  0.1187, -0.7812, -0.0796, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0278,  0.9102, -1.2734,  0.6484,  0.3379], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 52/69 [11:54<03:51, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.2148, -0.0947, -0.2090,  0.0693], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2285,  0.0085,  0.2656, -0.1465, -0.0415], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0879,  0.1240, -0.8633, -0.1201, -0.6758], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3594,  0.0271, -0.9766,  0.6719,  0.2002], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 53/69 [12:08<03:39, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3702, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520,  0.1455, -0.3828, -0.1914, -0.0583], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3702, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520,  0.1328,  0.1318, -0.1807, -0.1797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3702, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0938,  0.1211, -0.7148, -0.2773, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3702, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6836,  0.0349, -0.8672,  0.3008, -0.2031], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 54/69 [12:22<03:24, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4160,  0.2451, -0.1157, -0.2236,  0.2520], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3223,  0.0820,  0.0977, -0.1729, -0.0518], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0830,  0.1846, -0.8398, -0.2256, -0.4785], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5703,  1.1328, -1.0938,  0.4688,  0.3477], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 55/69 [12:36<03:12, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3496,  0.3223, -0.1689, -0.1670,  0.2637], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.0291, -0.0757, -0.1445, -0.0957], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0371,  0.1235, -0.8477, -0.1953, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5352,  0.5312, -1.3281,  1.1016, -0.1738], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 56/69 [12:50<02:59, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.3594,  0.0104, -0.2246,  0.1855], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.0608,  0.2334, -0.1504, -0.0383], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1270,  0.1758, -0.8750, -0.1777, -0.4922], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6562,  0.4727, -1.2344,  1.0312,  0.1992], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 57/69 [13:03<02:44, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3652,  0.2715, -0.3652, -0.3848, -0.0659], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.1021,  0.1660, -0.2539, -0.0391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1709,  0.1777, -0.6875, -0.2871, -0.4980], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.4570, -1.2344,  0.0153,  0.5078], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 58/69 [13:17<02:30, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3750,  0.2314, -0.2129, -0.3984, -0.1143], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3047,  0.1196,  0.0212, -0.2754, -0.0579], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.1484, -0.5859, -0.3301, -0.5000], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3828,  0.8125, -0.8789, -0.2656,  0.4727], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 59/69 [13:30<02:16, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1328,  0.1953,  0.0654, -0.2432,  0.2852], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.1660, -0.0272, -0.1396,  0.0330], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1436,  0.2314, -0.7812, -0.0679, -0.6172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5234,  1.0312, -0.5078,  1.2969,  0.1758], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 60/69 [13:44<02:02, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.2246, -0.4473, -0.2910, -0.1069], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3223,  0.1377,  0.2734, -0.2637, -0.1318], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1279,  0.1660, -0.7969, -0.3145, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.3262, -0.9062, -0.4766,  0.1631], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 61/69 [13:58<01:49, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5078,  0.1260, -0.3125, -0.1826,  0.3145], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3438,  0.0493, -0.0080, -0.1436, -0.1309], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2500,  0.1128, -0.7852, -0.2852, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7852,  0.5039, -0.4023, -0.0070, -0.0058], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 62/69 [14:11<01:35, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3926,  0.2969, -0.1226, -0.2051,  0.1602], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2090, -0.0073,  0.0081, -0.1309, -0.1953], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0537,  0.1621, -0.8711, -0.1914, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7734,  0.2266, -1.3203,  0.7773,  0.4004], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 63/69 [14:25<01:21, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3027,  0.2285,  0.0913, -0.1885,  0.0962], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2344,  0.0928, -0.0576, -0.1592, -0.0938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0496,  0.2275, -0.8906, -0.1982, -0.5625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6211,  0.3418, -1.0781,  0.4219,  0.7891], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 64/69 [14:39<01:08, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0342,  0.0386, -0.1660, -0.2061,  0.0562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1621,  0.4102, -0.1030, -0.0476, -0.0596], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3340,  0.2910, -0.7656,  0.0049, -0.6445], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4453,  0.8203, -1.1484,  1.2656, -0.6953], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 65/69 [14:52<00:54, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1069,  0.1934, -0.1914, -0.1348,  0.0889], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2363,  0.1045,  0.0211, -0.1089, -0.1099], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1582,  0.0864, -0.7617, -0.0430, -0.6758], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6445,  0.4590, -0.0869,  1.2109, -0.3047], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 66/69 [15:06<00:41, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1050,  0.2080, -0.3633, -0.1118,  0.0869], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3633,  0.2324, -0.2266, -0.0583,  0.0820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4219,  0.2637, -0.7031,  0.0199, -0.6914], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3867,  0.4531, -1.0938,  1.1172, -0.3164], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 67/69 [15:19<00:27, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0352,  0.1826, -0.3184, -0.0625,  0.0815], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2090,  0.2090, -0.2334, -0.0566, -0.0143], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.2002, -0.6953, -0.0176, -0.5039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0601,  0.7578, -1.3906,  1.1484, -0.0522], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 68/69 [15:25<00:13, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 1815, 3584]), Averaged feature shape: torch.Size([7, 3584]), Sample: tensor([ 0.0464,  0.1309, -0.0894, -0.1875, -0.1377], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 1815, 3584]), Averaged feature shape: torch.Size([7, 3584]), Sample: tensor([-0.1885,  0.3223, -0.2773, -0.1592, -0.0219], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 1815, 3584]), Averaged feature shape: torch.Size([7, 3584]), Sample: tensor([-0.3848,  0.1592, -0.6797, -0.0510, -0.6445], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 1815, 3584]), Averaged feature shape: torch.Size([7, 3584]), Sample: tensor([-0.3301,  0.7617, -0.5898,  1.3906, -0.3301], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e15a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e15a.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames in the video: 21288.0\n",
      "Original Resolution: (720.0, 480.0)\n",
      "FPS: 29.968454258675077\n",
      "Duration (seconds): 710.3469473684211\n",
      "Target Resolution: (224, 224)\n",
      "Read 21288 frames.\n",
      "Frames shape: torch.Size([21288, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6f7064069995>:50: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 710.35 seconds\n",
      "Number of intervals: 476\n",
      "Sample rate: 48000\n",
      "Output file: /kaggle/working/friends/s1/friends_s01e15a.h5\n",
      "Num splits: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 1/69 [00:12<14:35, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1191, -0.0571,  0.0569, -0.6211, -0.3887], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773, -0.2930, -0.0059, -0.2100, -0.0250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0364,  0.0579, -0.5000, -0.1943, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.2734, -0.1211, -1.5234,  0.1328, -1.4453], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/69 [00:25<14:21, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.1196, -0.2441, -0.2539,  0.1196], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1826,  0.2773, -0.3535, -0.0762,  0.1113], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0869,  0.1895, -0.8789,  0.0574, -0.4961], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1826,  0.6680, -0.5820,  1.8047, -0.2910], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/69 [00:38<14:20, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2178,  0.1904, -0.1514, -0.1328,  0.0977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.3262, -0.3613, -0.0728,  0.0874], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1953,  0.2617, -0.8906, -0.0124, -0.4238], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3672,  0.8516, -0.9258,  1.8672,  0.2383], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/69 [00:52<14:14, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0908, -0.0466,  0.0498, -0.6445, -0.2480], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3262, -0.2109,  0.0352, -0.1748, -0.0026], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0248,  0.0227, -0.5938, -0.1553, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.2734,  0.2080, -1.2969,  0.3672, -1.3203], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/69 [01:05<14:02, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184, -0.0354,  0.0157, -0.1621,  0.1875], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1196,  0.2041,  0.0486,  0.0145, -0.0530], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2197,  0.1865, -0.7930, -0.0708, -0.4219], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.3125,  0.7656, -0.4473,  0.8633,  0.1992], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/69 [01:19<14:01, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0359,  0.1396,  0.3203, -0.1025,  0.0762], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1895,  0.0708,  0.1865, -0.1455,  0.0254], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0076,  0.1475, -0.9375, -0.0265, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4375,  0.4277, -0.2734,  1.6406,  0.0216], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/69 [01:32<13:53, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4219,  0.0659,  0.0010, -0.1162,  0.1738], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-2.7344e-01,  2.2461e-01, -1.2695e-01,  2.2316e-04, -8.7402e-02],\n",
      "       device='cuda:1', dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1748,  0.2178, -0.7109, -0.0422, -0.4668], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2354,  0.3770, -0.7539,  0.8867, -0.0762], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/69 [01:46<13:41, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3730, -0.0483, -0.3906, -0.4551,  0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3750, -0.0024,  0.0125, -0.2910, -0.1416], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1260, -0.0503, -0.9023, -0.2949, -0.4668], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1631, -0.4727, -1.1719,  0.1904, -0.9336], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/69 [01:59<13:28, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1348,  0.2383, -0.2891, -0.1182,  0.0884], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2109,  0.3477, -0.1865, -0.0723,  0.0498], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1118,  0.3398, -0.9102,  0.0189, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4297,  0.8711, -0.5391,  2.5000, -0.4414], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10/69 [02:13<13:18, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1992,  0.3223, -0.1328, -0.1118,  0.3242], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.1079, -0.2002, -0.0879,  0.1787], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1816,  0.1348, -0.7500, -0.0786, -0.4336], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0187,  0.4062, -0.6250,  1.0547, -0.0442], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 11/69 [02:27<13:15, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.1455, -0.3496, -0.0027,  0.2236], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.1533, -0.2393,  0.0540, -0.0569], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0608,  0.2168, -1.1328,  0.1177, -0.6641], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1001,  0.6406, -1.1328,  2.8594, -0.2256], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/69 [02:41<13:05, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.2334, -0.0757, -0.1602,  0.2617], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2236,  0.0669, -0.1367, -0.0243,  0.0664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0859,  0.1104, -0.6328,  0.0077, -0.5117], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1377,  0.2539, -0.7305,  1.3281, -0.0688], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 13/69 [02:55<12:49, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.0077, -0.0815, -0.0496,  0.2266], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.0815, -0.2520,  0.0254, -0.1138], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0962,  0.1074, -1.0547,  0.1201, -0.6875], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.5391, -0.7578,  2.8750, -0.0674], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 14/69 [03:08<12:30, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1758,  0.1328, -0.3027, -0.1172,  0.0503], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217,  0.2412, -0.1289, -0.0030,  0.1172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1455,  0.2002, -0.8359,  0.1758, -0.3555], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6641,  1.0703, -1.2969,  2.6094, -0.0129], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 15/69 [03:21<12:11, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0544,  0.1074, -0.0894, -0.0613,  0.3066], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.1953, -0.1592, -0.0732,  0.1777], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0017,  0.2432, -0.8398, -0.1128, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.4688, -0.8711,  0.8164,  0.0259], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/69 [03:35<11:55, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1426,  0.2021, -0.3672, -0.1357,  0.1045], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2031,  0.1167, -0.1011, -0.0601,  0.0500], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1147,  0.0703, -0.9375,  0.0286, -0.6406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.9258,  0.3926, -0.7852,  1.8203, -0.2520], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 17/69 [03:48<11:38, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2471,  0.0500, -0.3086, -0.1069,  0.3301], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3301,  0.2061, -0.2559,  0.0164, -0.0742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1748,  0.3105, -0.9805,  0.0718, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1719,  1.0859, -0.6836,  2.5312,  0.1035], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 18/69 [04:02<11:27, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0466,  0.1904, -0.1689, -0.1069,  0.1699], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2158,  0.1270, -0.0148, -0.0698,  0.1182], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1338,  0.0869, -0.8867, -0.0082, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8320,  0.2754, -0.8750,  1.6719,  0.0493], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 19/69 [04:15<11:15, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1689,  0.0596, -0.2695, -0.0427,  0.3164], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3320,  0.2871, -0.3262,  0.0089, -0.0137], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1641,  0.2949, -0.9062,  0.0630, -0.5898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1680,  0.6641, -0.6875,  2.6875,  0.2969], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 20/69 [04:29<11:03, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0618,  0.1641, -0.1631, -0.1904,  0.2373], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2480,  0.1475, -0.1572, -0.1328,  0.1270], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1445,  0.2451, -0.9375, -0.0791, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8633,  0.4180, -0.6562,  1.5000,  0.3184], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 21/69 [04:43<10:52, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.2295, -0.3008, -0.2441,  0.2031], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2490,  0.1729, -0.0029, -0.1914, -0.0801], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1562,  0.2100, -0.9766, -0.0206, -0.7070], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2793, -0.1719, -1.5078,  1.4609, -0.4277], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 22/69 [04:56<10:40, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0461, -0.0527, -0.1592, -0.1504,  0.0854], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2393,  0.3320, -0.0219, -0.0603,  0.0005], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1807,  0.3203, -0.8125,  0.0381, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5859,  1.0781, -0.8711,  0.8477, -0.0591], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 23/69 [05:10<10:27, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1562,  0.0630, -0.1426, -0.0952,  0.2578], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3105,  0.0757,  0.1377, -0.0718,  0.0415], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2168,  0.1406, -0.8711,  0.0140, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5977,  0.7227, -0.2891,  1.1172, -0.1943], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 24/69 [05:23<10:09, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1934,  0.0889, -0.1934, -0.0171,  0.2012], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.2109, -0.1152, -0.0593,  0.0771], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2021,  0.2363, -0.8906, -0.0079, -0.6484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3711,  0.6953, -0.9727,  0.9453, -0.1729], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 25/69 [05:36<09:50, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0332,  0.1309, -0.1279, -0.1758,  0.0361], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2002,  0.3457, -0.0820, -0.0596,  0.0245], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1128,  0.2422, -0.6836, -0.0432, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0262,  1.0391, -0.7344,  0.9336,  0.1553], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 26/69 [05:50<09:36, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2227,  0.2910, -0.1396, -0.1592,  0.2734], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637, -0.0145, -0.2988, -0.0645, -0.0864], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1875,  0.0664, -0.8398, -0.0310, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0133,  0.4297, -0.8594,  1.2344, -0.3242], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 27/69 [06:04<09:30, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3729, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1641,  0.1807, -0.1309, -0.1602,  0.2285], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3729, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3496,  0.0082, -0.2656, -0.0874,  0.0137], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3729, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1953,  0.1089, -0.7500, -0.0713, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3729, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1934,  0.7031, -0.0747,  1.0000, -0.3242], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 28/69 [06:18<09:19, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3714, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1377,  0.1060, -0.1748, -0.2656,  0.0098], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3714, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-3.1836e-01,  2.1582e-01,  1.1492e-04, -1.5723e-01,  1.3794e-02],\n",
      "       device='cuda:1', dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3714, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.2451, -0.7344, -0.1021, -0.6680], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3714, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3730,  0.5625, -1.3906,  1.0859, -0.9453], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 29/69 [06:31<09:07, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0006,  0.0193,  0.0781,  0.0151,  0.0957], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3848,  0.1475, -0.2344,  0.0378,  0.0106], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2158,  0.2012, -0.8398,  0.0095, -0.7266], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754, -0.3047, -0.8867,  1.3594, -0.2432], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 30/69 [06:45<08:51, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1465,  0.0903, -0.1680, -0.0645,  0.0254], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.1689, -0.1279, -0.0859, -0.0018], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.1226, -0.8828, -0.0530, -0.6055], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0889,  0.4512, -0.9844,  1.1250, -0.2969], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 31/69 [06:59<08:39, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0605,  0.3340, -0.1377, -0.1953,  0.1836], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.2422, -0.0825, -0.1104,  0.1143], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3457,  0.1035, -0.8984, -0.1138, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1602,  0.5039, -0.8672,  1.3672, -0.2520], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 32/69 [07:12<08:27, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0518,  0.1406, -0.2617, -0.2275,  0.2832], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.2520, -0.1230, -0.1943,  0.0635], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1279,  0.2539, -0.8828, -0.1270, -0.6914], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4082,  1.0859, -0.8555,  1.0000,  0.4258], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 33/69 [07:26<08:14, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1079,  0.2012, -0.2090, -0.0986,  0.1147], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.1504, -0.2109, -0.0393,  0.0139], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.1328, -0.7031, -0.0232, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4258,  0.5625, -1.4219,  0.7148, -0.6406], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 34/69 [07:40<07:59, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0884, -0.0053, -0.2539, -0.1611,  0.2314], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.1191, -0.0776, -0.0854,  0.0267], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2461,  0.1562, -0.8984, -0.0295, -0.6484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6797,  0.5273, -0.5859,  0.9375, -0.0928], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 35/69 [07:53<07:43, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1128, -0.0208, -0.2617, -0.1250,  0.1797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3730,  0.2930, -0.1445, -0.0442, -0.0322], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949,  0.3633, -0.7891, -0.0874, -0.6953], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1289,  0.7812, -0.3828,  1.3125, -0.2139], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 36/69 [08:07<07:32, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.0613, -0.0410, -0.1621,  0.2393], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2090,  0.1406, -0.1699, -0.1094, -0.0835], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1934,  0.1963, -0.8789, -0.1533, -0.6680], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7148,  0.7227, -0.9219,  0.9648, -0.1963], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 37/69 [08:21<07:18, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1777,  0.1416, -0.2061, -0.2197,  0.1699], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.0884, -0.2021, -0.0859, -0.0030], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0986,  0.1689, -0.8281, -0.1680, -0.7031], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0442,  0.5156, -1.3594,  0.7773,  0.4766], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 38/69 [08:35<07:03, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.3867,  0.0044, -0.1021,  0.2148], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.1689, -0.2402, -0.1240,  0.0591], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0405,  0.3906, -0.9102, -0.0688, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2451,  1.2656, -0.3203,  1.3594,  0.1768], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 39/69 [08:48<06:51, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2158,  0.2656, -0.2227, -0.0544,  0.2188], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3867,  0.1895, -0.4180, -0.1260, -0.0172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.3262, -1.1016, -0.1025, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0854,  0.4102, -0.5312,  0.8672, -0.2637], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 40/69 [09:02<06:39, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0752,  0.0835, -0.3379, -0.1406,  0.1113], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3262,  0.3438, -0.0030, -0.0270,  0.0273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3047,  0.3906, -0.8633, -0.0469, -0.4980], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1641,  0.5078, -0.7539,  1.5547, -0.7734], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 41/69 [09:16<06:25, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1221,  0.3398, -0.0654, -0.1494,  0.0981], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3477,  0.2158, -0.1885, -0.1777,  0.1455], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.2051, -0.7930, -0.1357, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.5664, -0.1270,  1.5625, -0.4297], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 42/69 [09:30<06:10, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1621,  0.2168, -0.4258, -0.2197,  0.2617], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3438,  0.2676, -0.0889, -0.1084,  0.1543], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2266,  0.2988, -0.7852, -0.0693, -0.5547], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0488,  0.5234, -0.5703,  1.6094, -0.3809], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 43/69 [09:43<05:57, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494,  0.2471, -0.1953, -0.2139,  0.1543], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.0762, -0.0469, -0.0713, -0.0344], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1826,  0.1641, -0.7812, -0.0564, -0.3945], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8359, -0.3613, -0.8711,  0.9219, -0.1133], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 44/69 [09:57<05:43, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0491,  0.0874, -0.2363, -0.2373,  0.1182], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3535,  0.3574,  0.2119, -0.1206,  0.0238], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2334,  0.2656, -0.7891, -0.1025, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4805,  0.2578, -0.8906,  1.5781, -0.6367], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 45/69 [10:11<05:30, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.2832, -0.1318, -0.3262,  0.1050], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1436, -0.0228, -0.1611,  0.0752], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0796,  0.0977, -0.8555, -0.1367, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1069, -0.0947, -0.6758,  0.5977, -0.0317], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 46/69 [10:25<05:17, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.3145, -0.2344, -0.1494,  0.0942], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1729,  0.1279, -0.0272, -0.1279,  0.0415], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1143,  0.1465, -0.7695, -0.1592, -0.4824], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3633,  0.8008, -0.8789,  1.1328, -0.3926], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 47/69 [10:39<05:03, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.2695, -0.1953, -0.2832,  0.0962], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.1963,  0.0283, -0.1836,  0.0674], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1191,  0.1475, -0.8555, -0.1621, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.4492, -0.7812,  1.0078, -0.4961], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 48/69 [10:52<04:49, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.2305, -0.1396, -0.2754,  0.0957], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.1914, -0.0825, -0.1846,  0.0342], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1299,  0.1279, -0.8242, -0.2080, -0.6094], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.1670, -0.6992,  1.0859, -0.3887], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 49/69 [11:06<04:33, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0015,  0.3164, -0.2500, -0.1396,  0.2578], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3301,  0.2119, -0.0505, -0.0708,  0.1260], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3867,  0.2363, -0.8516, -0.0145, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0693,  0.3984, -0.5312,  0.9609, -0.3438], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 50/69 [11:20<04:19, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.2461, -0.2109, -0.0192,  0.1348], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3457,  0.2500, -0.0801, -0.0645,  0.0610], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3730,  0.2432, -0.8008, -0.0483, -0.5000], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2021,  0.5664, -0.2441,  1.2734,  0.3145], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 51/69 [11:33<04:05, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0179,  0.0342, -0.0344, -0.1572,  0.0510], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.0518, -0.0361, -0.0776, -0.1367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.1260, -0.9102, -0.0854, -0.4336], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2236,  0.5352, -0.8398,  0.7148,  0.2275], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 52/69 [11:47<03:50, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2080,  0.2305, -0.0172, -0.1069,  0.3984], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.1025, -0.1147, -0.1211,  0.0059], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1230,  0.2168, -0.9023, -0.1562, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.6953, -0.3691,  1.3281,  0.0371], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 53/69 [12:00<03:36, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.1816,  0.0310, -0.1523,  0.2500], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2988,  0.2070,  0.1172, -0.1377, -0.0520], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1895,  0.2021, -0.9453, -0.0938, -0.6562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2422,  1.1250, -0.3008,  1.8750, -0.1846], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 54/69 [12:14<03:23, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1270,  0.1074, -0.2471, -0.2334,  0.0776], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.3145,  0.1455, -0.1201, -0.0260], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.1904, -0.8086, -0.0510, -0.6484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7227,  0.0649, -0.7734,  1.2109, -0.5234], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 55/69 [12:27<03:10, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2129,  0.1201, -0.2500, -0.1436,  0.1885], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2441,  0.3086, -0.0253, -0.0732, -0.0352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2891,  0.2129, -0.7812, -0.0806, -0.6445], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8125,  0.2969, -0.7227,  1.3516, -0.2910], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 56/69 [12:41<02:57, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4258,  0.0571, -0.2812, -0.1631,  0.1865], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.0645, -0.2617, -0.0255,  0.0530], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2471,  0.3281, -0.6172, -0.0265, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0156,  1.3125, -0.5469,  1.2031, -0.5859], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 57/69 [12:54<02:42, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3750,  0.2734, -0.2520, -0.1260,  0.1611], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2266,  0.1689, -0.1235, -0.1035,  0.1187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0903,  0.1963, -0.8555, -0.0359, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1196,  1.4219, -0.7266,  1.6328, -0.2852], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 58/69 [13:08<02:29, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3984,  0.1631, -0.1729, -0.1924,  0.0588], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.1602, -0.0072, -0.1318,  0.1060], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1377,  0.3242, -0.7734, -0.0510, -0.5664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2451,  1.7734, -0.5781,  1.2578, -0.0635], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 59/69 [13:22<02:16, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3613,  0.2236, -0.1455, -0.1299,  0.0996], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1592, -0.1196, -0.0374,  0.0874], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0659,  0.2461, -0.7578,  0.0562, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1475,  1.3281, -0.7891,  2.0000, -0.1846], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 60/69 [13:35<02:01, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3555,  0.1875, -0.1406, -0.2217,  0.0649], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.1826, -0.1670, -0.1357,  0.0742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1050,  0.2500, -0.7539, -0.0947, -0.6016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0503,  1.6328, -0.3262,  1.7109, -0.1147], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 61/69 [13:49<01:48, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3574, -0.1309, -0.7109, -0.1719,  0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734, -0.0583,  0.1138, -0.1621,  0.0045], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2061, -0.0962, -0.9023, -0.1074, -0.2910], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0566, -0.4785, -1.8203,  1.1641, -0.4434], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 62/69 [14:02<01:35, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3702, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0425,  0.0554, -0.2891, -0.1157,  0.1504], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3702, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.2119,  0.0630, -0.0359,  0.0198], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3702, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3574,  0.2520, -0.6602, -0.0488, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3702, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.2910, -1.0312,  0.1836, -0.1602], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 63/69 [14:16<01:21, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1992,  0.0864, -0.1211, -0.2715,  0.0957], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2373,  0.0938, -0.0376, -0.1973, -0.0204], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1543,  0.2656, -0.7852, -0.2988, -0.4941], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4551,  0.4707, -0.7500,  0.0170,  0.3145], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 64/69 [14:30<01:08, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.1621, -0.1348, -0.1709,  0.2715], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3203,  0.0559,  0.0012, -0.1079,  0.0187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2119,  0.1768, -0.7578, -0.1631, -0.4336], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6875,  0.5312, -1.5391,  0.3496, -0.1025], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 65/69 [14:43<00:54, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2373,  0.0918, -0.0674, -0.2734,  0.0840], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.1001,  0.1797, -0.1299, -0.0439], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0569,  0.2051, -0.7812, -0.2637, -0.5664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3965,  1.1094, -0.7344,  0.0703,  0.4531], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 66/69 [14:57<00:40, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1709,  0.0928, -0.0378, -0.2500,  0.1523], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1357,  0.0364,  0.0947, -0.1216, -0.0566], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0894,  0.1416, -0.8633, -0.2080, -0.5625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4727,  1.0781, -0.8242,  0.1572,  0.4395], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 67/69 [15:10<00:27, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2363,  0.1426,  0.0603, -0.2139,  0.1064], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1934,  0.1191, -0.1025, -0.1299, -0.1079], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0752,  0.1592, -0.8125, -0.2266, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3672,  0.7148, -1.0547,  0.0869,  0.4551], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 68/69 [15:16<00:13, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 1826, 3584]), Averaged feature shape: torch.Size([7, 3584]), Sample: tensor([-0.1680,  0.2363, -0.0903, -0.1309,  0.2178], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 1826, 3584]), Averaged feature shape: torch.Size([7, 3584]), Sample: tensor([-0.2754,  0.0864,  0.0159, -0.0864,  0.1270], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 1826, 3584]), Averaged feature shape: torch.Size([7, 3584]), Sample: tensor([-0.2354,  0.0405, -0.7617, -0.0483, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 1826, 3584]), Averaged feature shape: torch.Size([7, 3584]), Sample: tensor([-0.2852, -0.2217, -1.7656,  0.4062, -0.4004], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e08a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e08a.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames in the video: 21222.0\n",
      "Original Resolution: (720.0, 480.0)\n",
      "FPS: 29.968454258675077\n",
      "Duration (seconds): 708.1446315789474\n",
      "Target Resolution: (224, 224)\n",
      "Read 21222 frames.\n",
      "Frames shape: torch.Size([21222, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6f7064069995>:50: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 708.14 seconds\n",
      "Number of intervals: 475\n",
      "Sample rate: 48000\n",
      "Output file: /kaggle/working/friends/s1/friends_s01e08a.h5\n",
      "Num splits: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 1/69 [00:13<14:53, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1221, -0.0732,  0.0447, -0.6094, -0.4023], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832, -0.3184, -0.0435, -0.2031, -0.0300], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0287,  0.0267, -0.5312, -0.1865, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.4219, -0.4004, -1.5156,  0.0928, -1.5078], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/69 [00:26<14:36, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3887,  0.0537, -0.1025, -0.1689,  0.2002], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2891,  0.2012,  0.0344, -0.0089,  0.1147], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3242,  0.3320, -0.8086,  0.0654, -0.4746], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3477,  0.6797, -1.1250,  0.8320, -0.6055], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/69 [00:39<14:25, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.1553,  0.0835, -0.2715,  0.1904], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3262,  0.1338, -0.0752, -0.1338,  0.1152], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1777,  0.3105, -0.8164, -0.1309, -0.4258], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.2275, -0.7422,  0.4375, -0.1514], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/69 [00:52<14:20, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1689,  0.0464,  0.0400, -0.1113,  0.1377], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.1768, -0.2012, -0.0505, -0.0311], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.0498, -0.8125, -0.0378, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.0154, -0.3535,  0.8945,  0.1396], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/69 [01:05<14:06, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.0815,  0.0339, -0.1572, -0.0845], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2383,  0.1504, -0.0410, -0.0417, -0.1602], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1084,  0.1719, -0.7969, -0.0518, -0.6094], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2305,  0.6680, -0.5742,  1.4453, -0.0981], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/69 [01:19<13:55, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1245, -0.0513,  0.0576, -0.6172, -0.3984], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637, -0.2871, -0.0054, -0.2002, -0.0315], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0189,  0.0356, -0.5430, -0.1855, -0.6602], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.3750, -0.4277, -1.6406,  0.1182, -1.6328], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/69 [01:32<13:50, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0198,  0.2217, -0.2061, -0.1416, -0.0791], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.2520, -0.1904, -0.1504,  0.1836], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2441,  0.2344, -0.7188, -0.0374, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2490,  0.8516, -0.6914,  1.1641,  0.5547], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/69 [01:46<13:39, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0114,  0.0635, -0.1992, -0.1338,  0.1953], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3770,  0.1807,  0.0322, -0.0544,  0.1631], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2500,  0.1631, -0.8242,  0.0471, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949,  0.1079, -1.3906,  0.9414,  0.1050], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/69 [02:00<13:32, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0115,  0.1147, -0.3438, -0.1689, -0.0444], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.1992, -0.1357, -0.1689,  0.1572], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.2129, -0.7344, -0.0588, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1895,  1.1016, -1.0391,  1.4844, -0.0913], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10/69 [02:13<13:16, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2354,  0.0410, -0.0393, -0.0737,  0.2520], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3340,  0.1289,  0.1670, -0.0728,  0.0835], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2490,  0.1553, -0.7852, -0.0493, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5078,  0.1943, -1.4062,  0.7969, -0.2891], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 11/69 [02:27<13:00, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3594, -0.0029, -0.1377, -0.0884,  0.1396], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3418,  0.1279,  0.1455, -0.0767,  0.0864], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.0908, -0.8242, -0.0757, -0.5078], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7031,  0.1094, -1.4531,  0.4531, -0.1572], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/69 [02:41<13:00, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0781,  0.0879, -0.1660, -0.1836,  0.0869], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3945,  0.2598,  0.0518, -0.1187,  0.1689], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2197,  0.2598, -0.7422, -0.1104, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6367,  1.2188, -0.6719,  1.2031,  0.2314], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 13/69 [02:55<12:54, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1543, -0.0028, -0.1128, -0.1738,  0.1738], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3398,  0.2129, -0.0181, -0.1104,  0.0654], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3379,  0.0776, -0.8359, -0.0496, -0.5898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4473, -0.1006, -1.3750,  0.8672, -0.4570], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 14/69 [03:09<12:44, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1016,  0.0859, -0.2139, -0.1562,  0.1709], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3086,  0.2754,  0.0566, -0.0859,  0.1030], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1514,  0.2236, -0.7852, -0.0295, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4531,  1.7031, -0.6328,  1.1484, -0.1729], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 15/69 [03:23<12:26, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.1260, -0.0055, -0.1963,  0.0801], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1758,  0.1592, -0.1699, -0.1279, -0.1572], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1074,  0.1602, -0.7539, -0.1953, -0.5391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1328,  0.0952, -0.4355,  0.0179, -0.2715], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/69 [03:37<12:17, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2324,  0.0339, -0.1445, -0.0957,  0.1514], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2432,  0.0977,  0.1670, -0.0913,  0.1001], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2412,  0.1973, -0.7891, -0.0413, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4043,  0.4688, -1.4297,  0.6328, -0.5273], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 17/69 [03:51<12:05, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2148,  0.2178, -0.3828, -0.0742,  0.1699], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.2832,  0.0192, -0.0101,  0.1289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2197,  0.3066, -0.8789,  0.0947, -0.5898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.3047, -0.9531,  1.8516, -0.3418], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 18/69 [04:05<11:56, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1016,  0.0972, -0.4102, -0.0630,  0.0187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.2773,  0.0972, -0.0366, -0.0552], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3320,  0.1177, -0.8789, -0.0009, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.3066, -1.0000,  0.8906, -0.7227], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 19/69 [04:19<11:46, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1553,  0.0869, -0.2373, -0.1108,  0.1328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.1494, -0.0122, -0.0918,  0.0469], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1650,  0.1143, -0.7227, -0.0732, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5898,  0.4453, -1.7109,  0.6250, -0.9062], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 20/69 [04:34<11:32, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1318,  0.1738, -0.2773, -0.0393,  0.2617], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3203,  0.2295, -0.1021, -0.0154,  0.0498], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2188,  0.2500, -0.8477, -0.0303, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2246,  0.6055, -0.9844,  1.2500, -0.1768], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 21/69 [04:47<11:14, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3770,  0.0144, -0.4023, -0.2275,  0.1631], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.0605, -0.2891, -0.1777, -0.2773], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0598,  0.1260, -0.8516, -0.1299, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.5312, -0.5352, -1.3750,  0.9102, -0.0679], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 22/69 [05:02<11:01, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.1074, -0.1436, -0.1689, -0.0238], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1982,  0.1963, -0.1108, -0.1533, -0.1592], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4082,  0.1484, -0.7891, -0.1055, -0.5078], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1113,  0.6406, -0.7617,  1.2812, -0.0654], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 23/69 [05:16<10:46, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4297,  0.1855,  0.0786, -0.1328,  0.2373], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.1475, -0.2354, -0.0243, -0.2598], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1260,  0.1768, -0.9805, -0.0286, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.6797, -0.3379,  1.6406,  0.5352], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 24/69 [05:29<10:28, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.2148,  0.0762, -0.2578,  0.1475], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2314,  0.1592, -0.3184, -0.0811, -0.2168], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494,  0.1973, -0.9531, -0.0674, -0.5469], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8945,  0.8359, -0.5234,  1.1797,  0.0459], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 25/69 [05:43<10:14, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4570,  0.2236,  0.0552, -0.2852,  0.0449], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3047,  0.0854, -0.2637, -0.1553, -0.0972], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1104,  0.0586, -0.7500, -0.1484, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2676,  0.7734, -0.9219,  1.2344, -0.2266], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 26/69 [05:57<09:56, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2188, -0.1328, -0.2793, -0.1406,  0.0864], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2480,  0.2969, -0.1270,  0.0317, -0.0239], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1631,  0.2988, -0.8047,  0.0933, -0.5664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0898,  0.6055, -0.6484,  1.8203, -0.4082], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 27/69 [06:12<09:52, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2354, -0.0654, -0.1660, -0.1348,  0.1206], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2471,  0.3281, -0.0103,  0.0197, -0.0210], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1650,  0.2852, -0.7617,  0.0640, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3691,  0.3828, -0.9492,  1.7578, -0.5156], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 28/69 [06:26<09:40, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0850, -0.0952, -0.1758, -0.1426,  0.0747], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3379,  0.2949, -0.0674, -0.0669, -0.0767], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1875,  0.3223, -0.7812, -0.0488, -0.6602], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7188,  0.7227, -0.8359,  1.5391, -0.5742], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 29/69 [06:41<09:41, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2041,  0.2637, -0.1377, -0.1123,  0.0449], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.3281, -0.1162, -0.0583,  0.1074], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1670,  0.2832, -0.6133, -0.0457, -0.4805], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7656,  0.9062, -1.1172,  1.1406,  0.2871], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 30/69 [06:56<09:32, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3555,  0.2051,  0.0148, -0.0684,  0.1846], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.1670, -0.0771, -0.0064, -0.0591], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0109,  0.1475, -0.7109,  0.0047, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6641,  0.8477, -0.7930,  1.5312,  0.1602], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 31/69 [07:11<09:20, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3633, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3633,  0.3379, -0.1167, -0.0820,  0.2197], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3633, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3105,  0.1748, -0.3496, -0.0786,  0.0898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3633, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1250,  0.2754, -0.8594, -0.0449, -0.4980], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3633, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4043,  1.2969, -0.7383,  1.5703,  0.2793], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 32/69 [07:25<08:59, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494,  0.1748, -0.1963, -0.1592,  0.1416], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.2832, -0.1543, -0.1206,  0.0574], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1982,  0.3281, -0.6641, -0.1357, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.0312,  1.0469, -1.1016,  1.1562, -0.2812], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 33/69 [07:40<08:43, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1533,  0.0090, -0.0447, -0.1328, -0.0796], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.2412,  0.1099, -0.0103, -0.0140], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2158,  0.2852, -0.6445, -0.0201, -0.5469], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1748,  0.7969, -0.8477,  1.4453, -0.1108], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 34/69 [07:53<08:19, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2441,  0.0957,  0.2324, -0.0464,  0.0540], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2441,  0.0520,  0.1836, -0.0234, -0.0123], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.0620, -0.8203,  0.0038, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4355,  0.0625, -0.6289,  1.0391, -0.1504], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 35/69 [08:07<08:01, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1914,  0.1670,  0.0033, -0.1328, -0.0957], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3086,  0.3105, -0.0322, -0.0273, -0.0422], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1367,  0.2734, -0.6484, -0.0811, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2188,  1.1953, -0.9297,  1.0156, -0.0256], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 36/69 [08:21<07:42, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3359,  0.3379, -0.1201, -0.1338,  0.1641], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.1621, -0.2432, -0.0698, -0.0234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1660,  0.2344, -0.7656, -0.0742, -0.4434], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5273,  1.1953, -0.7734,  1.3828,  0.5039], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 37/69 [08:35<07:25, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1709,  0.1309, -0.1021, -0.1641,  0.0214], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2012,  0.2373,  0.0576,  0.0056, -0.1201], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1406,  0.2812, -0.6641,  0.0212, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0396,  1.2500, -0.7070,  1.5078, -0.0491], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 38/69 [08:48<07:07, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1553, -0.0100, -0.5742, -0.1865,  0.1235], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1216,  0.1367, -0.3320, -0.0515, -0.0300], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1982,  0.0466, -0.6758, -0.0664, -0.3711], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.5469, -0.4258, -0.0117,  1.0000, -0.0801], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 39/69 [09:02<06:52, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1670,  0.2393,  0.0908, -0.2051,  0.3398], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2461,  0.2334, -0.1787, -0.0752,  0.0830], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0864,  0.2734, -0.6836, -0.0107, -0.4062], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0134,  0.2676, -0.5703,  1.7656,  0.2949], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 40/69 [09:16<06:42, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2334,  0.0130, -0.2812, -0.2129, -0.0015], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2324,  0.2871, -0.2344, -0.0311,  0.0067], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2402,  0.2871, -0.7969,  0.1133, -0.5078], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1084,  0.8711, -1.0234,  1.9922, -0.1143], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 41/69 [09:31<06:33, 14.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2441,  0.2930, -0.2148, -0.2148,  0.2080], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.3164, -0.1445, -0.0830,  0.0381], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1230,  0.1719, -0.7070,  0.0131, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0693,  0.1973, -0.5352,  1.3047,  0.0684], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 42/69 [09:45<06:23, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.2197, -0.1758, -0.0903,  0.0461], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.1846, -0.0562, -0.0737,  0.0508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0669,  0.1758, -0.7773, -0.0439, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4082,  0.5664, -1.0547,  1.2500, -0.2793], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 43/69 [09:58<06:03, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1709,  0.1836, -0.2080, -0.1006,  0.2480], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2256,  0.3262, -0.0084, -0.0598,  0.1016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1167,  0.3711, -0.6641,  0.0664, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0747,  0.9023, -0.6523,  1.6016,  0.1035], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 44/69 [10:12<05:44, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1621,  0.1758, -0.1260, -0.1924,  0.2285], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949,  0.3477,  0.0211, -0.0947,  0.0874], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1060,  0.4258, -0.7109,  0.0117, -0.5039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1719,  1.2109, -0.5234,  1.3047,  0.4316], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 45/69 [10:25<05:28, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.0928, -0.2490, -0.0894,  0.0928], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3086,  0.2002, -0.1484, -0.0859,  0.0698], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2031,  0.1943, -0.9141,  0.0018, -0.4609], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.8984, -1.0234,  1.2188, -0.4336], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 46/69 [10:39<05:13, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2451,  0.2578, -0.0613, -0.0623,  0.1973], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.2949, -0.3574,  0.0238, -0.0249], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1611,  0.1514, -0.8086,  0.1089, -0.5547], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1162,  0.5703, -1.2422,  2.1875,  0.0864], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 47/69 [10:53<05:00, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0039,  0.2168, -0.2676, -0.0972,  0.2080], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2363,  0.1465, -0.1943, -0.0227,  0.0991], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1309,  0.2002, -0.8398,  0.0530, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1807,  0.5234, -1.3359,  1.1250, -0.0659], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 48/69 [11:07<04:49, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0282,  0.2451,  0.0476, -0.0923,  0.1348], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1836,  0.2773, -0.1523, -0.0034,  0.0845], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.2178, -0.7578,  0.1475, -0.4766], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5312,  0.3105, -1.3438,  1.9375, -0.0894], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 49/69 [11:20<04:35, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0623,  0.0625, -0.2207, -0.2871,  0.1309], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3379,  0.3281,  0.0593, -0.1016, -0.0177], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.3887, -0.8281, -0.0549, -0.6680], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7734,  0.4238, -1.0078,  1.0703, -0.2031], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 50/69 [11:34<04:21, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0525,  0.2754, -0.2236, -0.1514,  0.1377], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3926,  0.2461, -0.2070, -0.0811,  0.0586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.2451, -0.7266, -0.0732, -0.6562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.6680, -0.8672,  0.9023,  0.1050], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 51/69 [11:48<04:07, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0027,  0.2178, -0.0297, -0.2139,  0.0752], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2031,  0.1689, -0.0596, -0.1875,  0.1260], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0065,  0.2090, -1.0000, -0.1826, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1914,  0.6445, -0.8789,  1.1719,  0.1748], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 52/69 [12:02<03:54, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.0232, -0.1367, -0.1582,  0.1699], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949,  0.2168, -0.1465, -0.0918,  0.0815], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0408,  0.2070, -0.9961, -0.1719, -0.6250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0132,  0.8789, -1.2031,  1.3906,  0.0898], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 53/69 [12:15<03:39, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0703, -0.0820, -0.1816, -0.1523,  0.0869], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1982,  0.1875, -0.2070, -0.0620, -0.1699], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3262,  0.0854, -0.7734, -0.1279, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7969,  0.2207, -0.3867,  0.9453,  0.1631], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 54/69 [12:29<03:25, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3634, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1758, -0.0034,  0.0047, -0.1367,  0.0728], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3634, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2402,  0.1748, -0.0986, -0.0786, -0.0101], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3634, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1396,  0.2197, -0.7812, -0.1396, -0.7070], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3634, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2490,  0.5156, -0.3164,  1.0078,  0.0493], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 55/69 [12:43<03:12, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494,  0.1807, -0.4414, -0.2256, -0.0449], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1689,  0.2354, -0.2363, -0.0952, -0.1836], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3105,  0.1030, -0.6953, -0.1758, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3631, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4824,  0.0908, -1.0391,  0.8438, -0.1221], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 56/69 [12:57<02:59, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3647, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1230,  0.0286, -0.4570, -0.2490, -0.0172], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3647, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.1318, -0.0486, -0.0850, -0.2441], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3647, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2432,  0.0801, -0.7812, -0.2334, -0.5664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3647, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6914,  0.0933, -0.8008,  0.6445, -0.0491], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 57/69 [13:10<02:45, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5391, -0.1934, -0.3203, -0.1338,  0.0347], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3262, -0.0342, -0.1494, -0.1182, -0.1973], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1504,  0.0938, -0.9023, -0.2432, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.4570, -0.1338, -1.1875,  0.1465, -0.0078], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 58/69 [13:24<02:30, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1777,  0.0420, -0.1348, -0.1040,  0.0354], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1367,  0.1680, -0.1338, -0.0093, -0.0186], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0889,  0.1069, -0.9375, -0.0608, -0.6016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4570,  0.0427, -0.6758,  0.9844,  0.1729], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 59/69 [13:37<02:16, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0312, -0.0017, -0.0679, -0.0801,  0.2754], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2246,  0.1738, -0.0228, -0.0068,  0.0048], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0537,  0.1787, -1.0000, -0.0198, -0.6953], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8281,  1.1875, -0.0801,  2.1719,  0.5508], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 60/69 [13:51<02:02, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0156,  0.1650, -0.1147, -0.0996,  0.2676], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2432,  0.2168, -0.0547, -0.0698, -0.0522], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1338,  0.2305, -0.8867, -0.0815, -0.7227], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3669, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6719,  1.1484, -0.5195,  1.7969,  0.0089], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 61/69 [14:04<01:47, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1123,  0.1016, -0.0786, -0.1191,  0.0520], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1230,  0.1748, -0.2363, -0.0918, -0.0039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1533,  0.1270, -0.9297, -0.0557, -0.6289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5898, -0.0063, -0.7812,  1.1406,  0.1963], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 62/69 [14:18<01:34, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0097,  0.0708, -0.0928, -0.0625,  0.3477], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.1396, -0.0282,  0.0004,  0.0474], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1089,  0.1328, -0.9219, -0.0046, -0.6523], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8086,  1.0391, -0.3359,  1.8281,  0.2891], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 63/69 [14:31<01:21, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.1084,  0.1709, -0.1836,  0.1895], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1777,  0.0957, -0.1514, -0.0386, -0.0840], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217, -0.0742, -0.6797, -0.0123, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0537,  0.1016, -1.1562,  0.9023, -0.2207], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 64/69 [14:45<01:07, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0415,  0.0659, -0.0757,  0.0008,  0.0806], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.1270,  0.0757,  0.0544, -0.0728], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1553,  0.2295, -0.8242,  0.0786, -0.5391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-4.3945e-01,  6.5994e-04, -8.9453e-01,  7.5000e-01, -2.6953e-01],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 65/69 [14:58<00:54, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0327,  0.1875,  0.2793, -0.1943,  0.1943], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.0757,  0.1348, -0.1875,  0.1768], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2441,  0.2197, -0.8672, -0.1699, -0.4180], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.4766, -0.9688,  1.1016, -0.1641], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 66/69 [15:12<00:40, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.3516,  0.0349, -0.1592,  0.0654], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.1221, -0.1328, -0.0938,  0.0245], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2334, -0.0559, -0.8398, -0.0260, -0.5391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5625, -0.5703, -0.8906,  0.7305, -0.3887], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 67/69 [15:24<00:26, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3415, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.2051,  0.3652, -0.0026, -0.2480,  0.1953], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3415, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.2695,  0.1089,  0.0928, -0.1338, -0.0625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3415, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.2178,  0.0371, -0.7773, -0.1143, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3415, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.5664, -0.4980, -1.1875,  0.8594, -0.3184], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 68/69 [15:29<00:13, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 1562, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.0801,  0.0277,  0.0420, -0.0425,  0.2188], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 1562, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.2637,  0.0496,  0.0080, -0.0168,  0.1875], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 1562, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.1396,  0.2891, -0.8438,  0.0293, -0.4863], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 1562, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.3867,  0.6289, -0.9414,  0.8906,  0.0024], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e03a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e03a.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames in the video: 21079.0\n",
      "Original Resolution: (720.0, 480.0)\n",
      "FPS: 29.968454258675077\n",
      "Duration (seconds): 703.3729473684211\n",
      "Target Resolution: (224, 224)\n",
      "Read 21079 frames.\n",
      "Frames shape: torch.Size([21079, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6f7064069995>:50: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 703.37 seconds\n",
      "Number of intervals: 472\n",
      "Sample rate: 48000\n",
      "Output file: /kaggle/working/friends/s1/friends_s01e03a.h5\n",
      "Num splits: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 1/69 [00:13<14:49, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1226, -0.0549,  0.0422, -0.6016, -0.3887], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910, -0.3086, -0.0339, -0.1973, -0.0045], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0347,  0.0378, -0.5469, -0.1816, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.3438, -0.3125, -1.5625,  0.1504, -1.5078], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/69 [00:26<14:33, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2266,  0.1108, -0.0559, -0.1250,  0.3555], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.1562, -0.1836, -0.0190, -0.0039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1895,  0.2041, -0.7227,  0.0091, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5898,  0.7500, -0.8867,  1.0000, -0.1416], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/69 [00:39<14:21, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1289,  0.0282, -0.0996, -0.1475,  0.1787], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.3242, -0.1816, -0.0552,  0.0444], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2305,  0.2451, -0.7969,  0.0157, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0076,  1.0547, -1.0312,  1.5000, -0.2793], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/69 [00:52<14:13, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1021,  0.1562, -0.0620, -0.0625,  0.3262], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.2090, -0.0674, -0.0742,  0.0452], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0579,  0.2246, -0.8516,  0.0262, -0.7070], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.2109,  1.2969, -0.4590,  1.7188,  0.2061], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/69 [01:05<14:02, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2227,  0.1416, -0.2188, -0.0791,  0.1709], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.2930, -0.1260, -0.0056,  0.1416], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1416,  0.2617, -0.8320,  0.0850, -0.5078], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3223,  0.8555, -1.4062,  1.7812, -0.2520], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/69 [01:19<13:53, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.1475, -0.2412, -0.0811,  0.1406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520,  0.3320, -0.0991, -0.0232,  0.1328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.3008, -0.8203,  0.1050, -0.5391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1396,  0.7578, -1.4609,  2.1094, -0.1709], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/69 [01:32<13:37, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1465, -0.0294, -0.1226, -0.1553,  0.1768], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.3066, -0.0281, -0.0374,  0.0752], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.3828, -0.8672,  0.0991, -0.5664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3535,  1.2969, -0.9141,  2.3438,  0.1562], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/69 [01:45<13:26, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0703,  0.0972,  0.0134, -0.0767,  0.2578], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217,  0.2256,  0.0089, -0.1338,  0.0698], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1064,  0.3105, -0.8320, -0.0728, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5234,  1.0859, -0.8672,  1.4297,  0.5938], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/69 [01:59<13:21, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1196, -0.0654,  0.0486, -0.6055, -0.3789], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734, -0.3008, -0.0178, -0.2090, -0.0089], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0383,  0.0214, -0.5273, -0.1914, -0.6406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.3906, -0.3223, -1.4844,  0.1318, -1.5547], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10/69 [02:12<13:12, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1377, -0.0042, -0.1196, -0.1836,  0.1553], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2041, -0.0059, -0.3184, -0.1187,  0.0104], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0869,  0.0588, -0.8789, -0.0977, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1211, -0.7617, -0.7188,  1.2031, -0.3027], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 11/69 [02:26<13:01, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520,  0.3145, -0.0479, -0.1436,  0.2051], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2305,  0.2344, -0.1338, -0.0649, -0.0693], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2334,  0.1719, -0.6797, -0.0459, -0.6250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6367,  0.0442, -0.8047,  0.5547,  0.0228], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/69 [02:39<12:40, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4121,  0.0879, -0.0864, -0.1680,  0.2891], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2021,  0.1621, -0.1777, -0.0461, -0.1045], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0967,  0.1992, -0.8555, -0.0645, -0.6406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0493, -0.1387, -1.7188,  1.2422,  0.1553], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 13/69 [02:52<12:28, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1982, -0.0791, -0.3008, -0.0664,  0.1777], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.1982, -0.0422, -0.0398, -0.1426], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.2139, -0.7539, -0.0552, -0.7539], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.5039, -1.4375,  0.5430, -0.1079], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 14/69 [03:05<12:12, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4160,  0.1768, -0.0571, -0.1240,  0.0119], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1924,  0.1934, -0.2275, -0.0850, -0.0894], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.2002, -0.8398, -0.0859, -0.6914], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3730, -0.1416, -1.4609,  1.0391, -0.0542], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 15/69 [03:19<12:01, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0962, -0.0557, -0.2578, -0.0820,  0.1211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.1982, -0.0071, -0.0254, -0.1040], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3301,  0.3164, -0.7695, -0.0684, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.7188, -1.4453,  0.3516, -0.2773], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/69 [03:32<11:47, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.3633, -0.0820, -0.1953,  0.1758], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1914,  0.2168, -0.1885, -0.1104, -0.0376], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2383,  0.0791, -0.7305, -0.0513, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2480, -0.1494, -0.9727,  0.4707, -0.3789], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 17/69 [03:46<11:36, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1621, -0.0874, -0.2490, -0.1035,  0.1123], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2344,  0.2080, -0.0571, -0.0581, -0.1104], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.1904, -0.7617, -0.0723, -0.6055], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0620,  0.5234, -0.8828,  0.3887, -0.3613], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 18/69 [03:59<11:24, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3965,  0.1611, -0.1650, -0.1680,  0.2949], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.1846, -0.0713, -0.0630, -0.0408], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1953,  0.0991, -0.7461, -0.0173, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5117, -0.1211, -0.6289,  0.5781, -0.1133], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 19/69 [04:13<11:12, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1182,  0.0082, -0.2354, -0.1475, -0.0081], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2891,  0.2061,  0.0476, -0.1006, -0.1226], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.2734, -0.7539, -0.0640, -0.7148], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2314,  1.0781, -0.8047,  0.5703, -0.0874], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 20/69 [04:26<10:57, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1523,  0.1001, -0.4199, -0.0942,  0.0977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1089, -0.1621,  0.0376, -0.0234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1562,  0.2090, -0.7148,  0.0371, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.6484,  0.3594, -0.7188,  0.7227,  0.5625], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 21/69 [04:40<10:47, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4023,  0.2734, -0.0378, -0.1846,  0.2266], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2490,  0.0376, -0.0981, -0.0811, -0.0811], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0996,  0.1406, -0.7383, -0.1592, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0591,  0.5234, -1.3594,  0.4336,  0.4141], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 22/69 [04:53<10:36, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1689, -0.0889, -0.4316, -0.1216,  0.0415], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2441,  0.3691, -0.2812,  0.0063,  0.0210], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.3008, -0.8359,  0.1367, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0199,  0.6875, -0.8086,  1.8125, -0.8086], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 23/69 [05:07<10:22, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1377,  0.0596, -0.1279, -0.2012,  0.1426], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3750,  0.3789, -0.0427, -0.1406,  0.0432], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.4102, -0.7461, -0.0623, -0.5000], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1670,  1.1328, -1.3281,  1.4062,  0.0320], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 24/69 [05:21<10:10, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1089,  0.0491, -0.0732, -0.1348,  0.2373], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2148,  0.2656, -0.1504, -0.1030,  0.1445], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0688,  0.2217, -0.8359,  0.0080, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4512,  1.2422, -0.2285,  1.7422,  0.0073], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 25/69 [05:34<10:00, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1689, -0.0264, -0.2617, -0.1768,  0.0967], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217,  0.4512, -0.1934,  0.0070,  0.0588], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1196,  0.4004, -0.7070,  0.1826, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.3164,  1.3750, -0.8633,  2.2656, -0.3496], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 26/69 [05:48<09:49, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1196, -0.0222, -0.0097, -0.0659,  0.2148], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.2734, -0.2754, -0.0476,  0.0581], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.2500, -0.7930,  0.0079, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3654, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0104,  0.9336, -0.5352,  1.6562, -0.4746], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 27/69 [06:02<09:34, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572, -0.0172, -0.2891, -0.1846, -0.0023], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2393,  0.3457, -0.2832, -0.0508,  0.0233], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2188,  0.2754, -0.8203,  0.1055, -0.6680], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0476,  0.8398, -0.9805,  2.0156, -0.9062], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 28/69 [06:15<09:17, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1099,  0.0625, -0.0361, -0.1494,  0.1367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.1357, -0.0698, -0.1328,  0.0271], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1099,  0.1963, -0.9023, -0.1177, -0.5391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5781,  0.9062, -0.2793,  1.3281,  0.4531], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 29/69 [06:29<09:07, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494,  0.1709, -0.1245, -0.1621,  0.0898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3926,  0.3281, -0.2383, -0.1367,  0.0762], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1030,  0.3477, -0.8633, -0.0515, -0.6055], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3066,  0.8945, -1.1172,  2.0312,  0.0679], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 30/69 [06:43<08:58, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0688,  0.0242,  0.0732, -0.1177,  0.2910], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3691,  0.2578, -0.0884, -0.0645,  0.1377], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1406,  0.1504, -0.8789,  0.0703, -0.6094], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3418,  0.3750, -1.0625,  1.7109, -0.0417], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 31/69 [06:57<08:45, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3709, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0830,  0.1299,  0.0076, -0.1748,  0.2793], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3709, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3242,  0.2305, -0.1592, -0.1172,  0.1123], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3709, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.1553, -0.9297,  0.0361, -0.5117], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3709, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1699,  0.4395, -1.0234,  2.0312, -0.0256], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 32/69 [07:11<08:27, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0251,  0.0874,  0.0510, -0.0903,  0.2227], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3574,  0.3066, -0.2412, -0.0437,  0.1240], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1396,  0.1514, -0.8672,  0.0718, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1377,  0.7305, -0.9805,  1.8125, -0.0393], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 33/69 [07:24<08:13, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2051,  0.0010, -0.0820, -0.1226,  0.0396], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.1680, -0.3105, -0.0444, -0.0388], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1143,  0.2441, -0.8242,  0.0182, -0.6094], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.5156,  0.5781, -1.0938,  2.1719,  0.1367], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 34/69 [07:38<07:58, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3712, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520,  0.1953, -0.1621, -0.2051,  0.0126], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3712, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.3535, -0.1875, -0.1133,  0.0996], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3712, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1025,  0.2832, -0.7461, -0.0156, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3712, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.6367, -0.8320,  1.7969, -0.7500], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 35/69 [07:51<07:44, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1318,  0.1260, -0.2266, -0.2061,  0.2158], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3887,  0.1533, -0.2715, -0.1235,  0.1338], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1729,  0.3672, -0.7109, -0.0518, -0.5469], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6523,  0.5547,  0.0034,  1.4922,  0.1924], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 36/69 [08:05<07:29, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1865,  0.0166, -0.4277, -0.1533, -0.0008], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.4062, -0.3184, -0.0160,  0.0449], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1982,  0.3359, -0.8242,  0.0386, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1484,  0.9336, -0.7891,  2.1094, -0.7109], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 37/69 [08:19<07:17, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1904,  0.0508, -0.1523, -0.1924,  0.0143], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.3574, -0.2500, -0.0264,  0.0549], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1152,  0.3047, -0.7266,  0.0801, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0586,  0.8203, -0.9180,  1.5000, -0.7656], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 38/69 [08:33<07:06, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520,  0.2422, -0.0332, -0.1543,  0.1914], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.1592, -0.2715, -0.0952, -0.1416], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.1367, -0.8438, -0.0086, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.4395, -0.2021,  1.8438, -0.2969], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 39/69 [08:46<06:52, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4219,  0.1270,  0.0233, -0.0898,  0.2656], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2393,  0.2168, -0.3652,  0.0024,  0.0400], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1196,  0.2871, -0.6797,  0.1006, -0.6523], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.3340,  0.4023, -0.6328,  1.6797, -0.0508], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 40/69 [09:00<06:41, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1172,  0.1328, -0.0135, -0.2500,  0.1309], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2402,  0.2656, -0.0439, -0.0977,  0.0608], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1475,  0.2871, -0.7930, -0.0036, -0.6055], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0718,  0.3164, -0.6680,  2.1250, -0.6680], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 41/69 [09:14<06:26, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3707, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1904,  0.1035, -0.4082, -0.2695,  0.0645], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3707, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871, -0.0461, -0.1416, -0.2217, -0.1973], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3707, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1260, -0.0204, -0.8789, -0.1816, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3707, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.6680, -0.5781, -1.0156,  1.6406, -0.4082], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 42/69 [09:27<06:08, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3704, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0052,  0.0903,  0.0645, -0.0398,  0.1416], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3704, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3008,  0.1426, -0.0791, -0.0713,  0.1299], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3704, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0276,  0.1543, -0.8203, -0.0078, -0.5273], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3704, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3320,  1.0703, -0.1191,  1.9141, -0.2910], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 43/69 [09:41<05:54, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0134,  0.1133, -0.1079, -0.1187,  0.0942], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2178,  0.1787, -0.1875, -0.1484,  0.0732], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0352,  0.1934, -0.8164, -0.1387, -0.5547], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3242,  0.7109,  0.2500,  2.2656,  0.0378], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 44/69 [09:55<05:40, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2041,  0.1963, -0.1299, -0.0894,  0.2051], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2451,  0.1670, -0.2578, -0.1270,  0.1328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1758,  0.1201, -0.7930, -0.0476, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1475,  0.1797, -0.1855,  2.1875, -0.1523], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 45/69 [10:08<05:26, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1768,  0.1099, -0.0879, -0.1299,  0.2480], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.2480,  0.1206, -0.0933,  0.1768], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.8457e-01,  2.1191e-01, -7.3047e-01,  7.1716e-04, -5.0000e-01],\n",
      "       device='cuda:1', dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5508,  0.6055, -1.2578,  1.1953,  0.5273], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 46/69 [10:22<05:13, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494, -0.0025, -0.4375, -0.1001,  0.1157], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1758,  0.3418,  0.0026, -0.0206, -0.1084], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2285,  0.1592, -0.8320,  0.0588, -0.6562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3650, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3457,  0.3809, -1.2109,  1.7109, -0.5156], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 47/69 [10:35<04:57, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3809, -0.0337, -0.4258, -0.1348,  0.1187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1406,  0.1533, -0.3535, -0.1162, -0.2324], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.0757, -1.0078, -0.1040, -0.4727], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2676,  0.0898, -1.2578,  0.8281, -0.4512], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 48/69 [10:49<04:45, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1533,  0.0150, -0.1221,  0.0232], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.2051, -0.0349, -0.1099, -0.0220], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 3.1662e-04,  2.4414e-01, -7.1875e-01, -1.0742e-01, -6.2109e-01],\n",
      "       device='cuda:1', dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1426,  0.2383, -1.1250,  0.6133,  0.5156], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 49/69 [11:03<04:34, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1226,  0.2422, -0.1787, -0.0845,  0.3750], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.3301, -0.1338, -0.0645,  0.0088], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2178,  0.2754, -0.8008,  0.0586, -0.6055], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2344,  0.3652, -0.6602,  1.7734, -0.3125], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 50/69 [11:17<04:21, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0206,  0.1660, -0.1777, -0.1543,  0.1582], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2500,  0.2031, -0.1592, -0.0737,  0.0811], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.2031, -0.8047,  0.0075, -0.5195], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1201,  0.6641, -0.5859,  1.3516, -0.1738], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 51/69 [11:31<04:08, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0986, -0.0276, -0.3164, -0.1040,  0.1050], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.1270, -0.1064, -0.0435, -0.0322], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2109,  0.1328, -0.8438,  0.0212, -0.6445], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.3281, -0.3379, -0.4414,  0.9141,  0.1226], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 52/69 [11:45<03:55, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1445,  0.2168, -0.2598, -0.1768,  0.1611], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2432,  0.0977,  0.0132, -0.0552, -0.1133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1621,  0.0630, -0.7969, -0.0044, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8633,  0.2598, -1.1953,  0.9102, -0.2812], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 53/69 [11:59<03:44, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1147,  0.1660, -0.3613, -0.1069, -0.0140], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.3770, -0.0284, -0.0045, -0.0244], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1895,  0.3184, -0.7578,  0.0325, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4648,  0.2656, -0.9492,  1.1250, -0.5117], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 54/69 [12:13<03:29, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.1484, -0.1748, -0.1221,  0.0908], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2891,  0.1846,  0.1445, -0.0713, -0.0776], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0317,  0.2266, -0.7500, -0.0525, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4707,  0.7734, -1.0312,  0.8359,  0.0757], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 55/69 [12:27<03:14, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949, -0.2012, -0.6641, -0.4609,  0.3770], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520, -0.2676,  0.2090, -0.2070, -0.0654], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1729, -0.0908, -0.9375, -0.2256, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3656, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3574,  0.2217, -1.2344,  1.0703, -0.6445], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 56/69 [12:41<03:00, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0033,  0.0559, -0.2266, -0.1504,  0.0427], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.3320,  0.1084, -0.0698,  0.0109], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.2314, -0.6953, -0.0403, -0.5625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2188,  0.7383, -0.9062,  1.2109, -0.4961], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 57/69 [12:55<02:47, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0757,  0.0879, -0.0635, -0.0674,  0.1289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2236,  0.0923,  0.0405,  0.0102,  0.0801], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1260,  0.1465, -0.8086,  0.0386, -0.4863], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7539,  0.3945, -1.4922,  1.0000,  0.0527], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 58/69 [13:09<02:34, 14.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2441,  0.0986, -0.1055, -0.0559,  0.1118], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.1060, -0.1309,  0.0128,  0.0454], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0065,  0.1582, -0.8867,  0.0282, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3906, -0.0801, -0.5742,  1.5781, -0.2412], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 59/69 [13:23<02:20, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1504,  0.1465,  0.0238, -0.0591,  0.0757], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1768,  0.2129,  0.0728, -0.0796, -0.0957], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1455,  0.1582, -0.7148, -0.0913, -0.6484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4414,  0.4258, -0.8320,  1.4297, -0.0312], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 60/69 [13:37<02:06, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1299,  0.0349, -0.1123, -0.0640,  0.2910], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.1777, -0.2119, -0.1021,  0.0593], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1084,  0.2314, -0.9219, -0.0322, -0.6562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0703,  0.8398, -0.5898,  0.7812, -0.8008], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 61/69 [13:51<01:52, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1167,  0.1807, -0.1318, -0.0981,  0.1992], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2178,  0.1963, -0.0630, -0.1030,  0.0574], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0669,  0.2275, -0.8945, -0.1279, -0.4551], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3690, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.9844,  0.8125, -0.9023,  0.8945,  0.4766], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 62/69 [14:05<01:38, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2373,  0.0732, -0.0688, -0.1206,  0.3867], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.1143, -0.1060, -0.1162, -0.1299], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494,  0.0488, -0.8984, -0.1377, -0.6406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1855,  0.7383, -0.3535,  1.0156,  0.3086], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 63/69 [14:19<01:24, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0840,  0.2012, -0.1279, -0.1592, -0.0396], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2354,  0.1836,  0.0302, -0.1099, -0.0820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2432,  0.0840, -0.7930, -0.1309, -0.4844], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0454,  0.2871, -1.0781,  0.4980, -0.2539], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 64/69 [14:33<01:09, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0605,  0.1836, -0.3008, -0.1592,  0.0193], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.3887,  0.1270, -0.0413, -0.0679], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.2500, -0.7266, -0.0115, -0.7070], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3630, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5430,  0.4688, -1.0938,  1.0781, -0.5391], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 65/69 [14:47<00:55, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3086,  0.0728, -0.5898, -0.2559,  0.2266], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2412,  0.0344, -0.4199, -0.0996, -0.1357], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2002,  0.0337, -0.7070, -0.1729, -0.4473], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.4512, -0.4434, -0.4258,  1.5859, -0.1387], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 66/69 [15:01<00:41, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1318,  0.3125, -0.0752, -0.1104,  0.0737], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.2988, -0.1982, -0.0322, -0.0654], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1855,  0.1865, -0.6953,  0.0310, -0.6250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0388, -0.0664, -1.2109,  1.5234, -0.2559], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 67/69 [15:09<00:24, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 2627, 3584]), Averaged feature shape: torch.Size([10, 3584]), Sample: tensor([-0.2168,  0.2217, -0.0981, -0.0830,  0.2314], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 2627, 3584]), Averaged feature shape: torch.Size([10, 3584]), Sample: tensor([-0.2393,  0.2188, -0.1533, -0.0520, -0.0688], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 2627, 3584]), Averaged feature shape: torch.Size([10, 3584]), Sample: tensor([-0.2070,  0.1865, -0.6875, -0.0106, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 2627, 3584]), Averaged feature shape: torch.Size([10, 3584]), Sample: tensor([-0.5234, -0.1934, -1.4297,  1.6406, -0.0128], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 68/69 [15:11<00:13, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 777, 3584]), Averaged feature shape: torch.Size([3, 3584]), Sample: tensor([-0.1040, -0.0378,  0.0491, -0.6211, -0.3828], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 777, 3584]), Averaged feature shape: torch.Size([3, 3584]), Sample: tensor([-0.2910, -0.2812, -0.0135, -0.2100, -0.0248], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 777, 3584]), Averaged feature shape: torch.Size([3, 3584]), Sample: tensor([ 0.0203,  0.0493, -0.5195, -0.1924, -0.6250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 777, 3584]), Averaged feature shape: torch.Size([3, 3584]), Sample: tensor([ 1.2734, -0.2832, -1.6406,  0.0354, -1.5625], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e17a.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e17a.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames in the video: 21554.0\n",
      "Original Resolution: (720.0, 480.0)\n",
      "FPS: 29.968454258675077\n",
      "Duration (seconds): 719.222947368421\n",
      "Target Resolution: (224, 224)\n",
      "Read 21554 frames.\n",
      "Frames shape: torch.Size([21554, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6f7064069995>:50: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 719.23 seconds\n",
      "Number of intervals: 482\n",
      "Sample rate: 48000\n",
      "Output file: /kaggle/working/friends/s1/friends_s01e17a.h5\n",
      "Num splits: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 1/69 [00:12<14:43, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1133, -0.0520,  0.0320, -0.6055, -0.3887], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910, -0.2949,  0.0277, -0.1865, -0.0327], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0347,  0.0649, -0.5273, -0.1670, -0.6562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.3125, -0.4414, -1.5156,  0.1611, -1.5703], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/69 [00:26<14:42, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.1001, -0.0505, -0.0645,  0.2969], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3555,  0.1299,  0.0544, -0.0291,  0.0294], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1523,  0.1631, -0.8398,  0.0356, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4180,  0.8906, -0.9727,  1.6641,  0.4707], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/69 [00:39<14:23, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4219,  0.0669,  0.0388, -0.1338,  0.3828], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3066,  0.0771, -0.2910, -0.0066, -0.0845], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2012,  0.1543, -0.9727,  0.0471, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5195,  0.4219, -1.1719,  1.9531, -0.1206], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/69 [00:52<14:14, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3887,  0.0796, -0.0601, -0.1235,  0.2734], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.0952, -0.1729, -0.0535, -0.0742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2324,  0.1299, -0.9570, -0.0564, -0.5039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2207,  0.5898, -1.3516,  1.3516,  0.0339], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/69 [01:05<13:58, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1631,  0.0251,  0.0004, -0.1582,  0.1206], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2266,  0.2090, -0.3457, -0.0762, -0.0620], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1309,  0.1187, -0.8008,  0.0420, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.6445,  0.4766, -0.8281,  1.6953, -0.3340], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/69 [01:18<13:50, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3555,  0.0476,  0.1011, -0.2344,  0.0811], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.2812, -0.0669, -0.0732, -0.0742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1494,  0.2275, -0.7344, -0.0081, -0.4980], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.4121,  0.7773, -0.3496,  2.0000,  0.1611], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/69 [01:32<13:45, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4316,  0.0654,  0.0175, -0.2832,  0.0967], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.2773, -0.1191, -0.0471, -0.1426], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1934,  0.2246, -0.7734, -0.0100, -0.4746], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2852,  1.0938, -0.4746,  2.1406,  0.2578], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/69 [01:45<13:26, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1523,  0.0796, -0.1582, -0.2266,  0.0850], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3027,  0.3535, -0.3066, -0.0708,  0.0201], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2051,  0.3086, -0.8750,  0.0508, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1138,  0.6367, -1.2500,  1.7109, -0.2363], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/69 [01:58<13:14, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1089,  0.2246, -0.0659, -0.2246,  0.1089], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.1797, -0.2217, -0.1553,  0.0654], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0488,  0.2373, -0.8359, -0.0444, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1240,  0.5469, -0.7539,  1.6328,  0.0530], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10/69 [02:12<13:02, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0781,  0.2246, -0.2012, -0.2793,  0.1328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3516,  0.2119, -0.1533, -0.1934,  0.0581], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1035,  0.2930, -0.7930, -0.1963, -0.5312], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4355,  0.9336, -0.8789,  1.3672, -0.2236], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 11/69 [02:25<12:55, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1133,  0.2969, -0.1289, -0.2012,  0.2793], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520,  0.1348, -0.1602, -0.1982,  0.0325], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0496,  0.2129, -0.8828, -0.1621, -0.5195], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1953,  0.4629, -0.7500,  1.0859,  0.0767], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/69 [02:39<12:50, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3706, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1660,  0.2021, -0.1641, -0.2617,  0.1289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3706, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3203,  0.1279, -0.2334, -0.2100, -0.0391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3706, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0430,  0.1953, -0.8438, -0.1553, -0.6289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3706, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3711,  0.3027, -0.7305,  1.1953, -0.1504], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 13/69 [02:53<12:44, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1367,  0.1904, -0.0942, -0.1777,  0.2246], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.2109, -0.2461, -0.1650,  0.0476], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0405,  0.3125, -0.8047, -0.1167, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3887,  0.7461, -0.9688,  1.2188, -0.0479], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 14/69 [03:07<12:29, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.1318, -0.2734, -0.2295,  0.2197], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2432,  0.2871, -0.1963, -0.0369,  0.0757], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2266,  0.3125, -0.7930, -0.0293, -0.5039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0154,  0.5156, -1.2188,  1.5312,  0.2314], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 15/69 [03:20<12:13, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.0718, -0.2051, -0.1709,  0.1621], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.1191, -0.1475, -0.0996,  0.0474], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0669,  0.1514, -0.8477,  0.0156, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4336,  0.6562, -0.6367,  1.6406,  0.0432], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/69 [03:34<12:03, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.0557, -0.2012, -0.0864,  0.2461], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3008,  0.2314, -0.2100,  0.0166, -0.0376], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.1904, -0.8594,  0.0625, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0287,  0.4941, -1.3828,  1.4141,  0.0537], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 17/69 [03:48<11:50, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1865,  0.0005, -0.2695, -0.2217,  0.1582], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2178,  0.2930, -0.1504, -0.0187,  0.0280], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2480,  0.3789, -0.8594,  0.0520, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0366,  0.8242, -1.2969,  1.8047, -0.1846], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 18/69 [04:01<11:40, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.0403, -0.2246, -0.1934,  0.0645], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.2480, -0.2500, -0.0476, -0.0059], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2227,  0.2158, -0.7930, -0.0305, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3699, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0021,  0.5898, -1.2734,  1.5547, -0.5117], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 19/69 [04:15<11:30, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2197,  0.1465, -0.1406, -0.1299,  0.3223], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2520,  0.2236, -0.1475, -0.0654,  0.0859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1191,  0.2578, -0.8047, -0.0344, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0889,  0.5508, -1.4609,  1.4766,  0.4160], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 20/69 [04:29<11:13, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930, -0.0332, -0.2285, -0.1670,  0.1494], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.3320, -0.2637,  0.0039, -0.0371], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2119,  0.2988, -0.7461,  0.0273, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3661, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1270,  0.9570, -1.3750,  1.9375, -0.2461], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 21/69 [04:43<11:01, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2412,  0.1855, -0.0140, -0.0530,  0.1357], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.0688, -0.0806, -0.0250, -0.0310], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0933,  0.1631, -0.7578,  0.0273, -0.6719], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0786,  0.7852, -0.5430,  1.4062,  0.2578], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 22/69 [04:57<10:48, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3707, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5859,  0.2930, -0.2734, -0.0155,  0.1621], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3707, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.0109,  0.0654, -0.0020,  0.0282], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3707, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0110,  0.0815, -0.8281,  0.0923, -0.4805], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3707, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0454, -0.1377, -0.5859,  0.4805,  0.1406], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 23/69 [05:10<10:34, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6992,  0.3242, -0.0364, -0.0869,  0.2969], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.1309, -0.1602, -0.0884,  0.0400], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0894,  0.2461, -0.7969, -0.0781, -0.3887], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0718, -0.3887, -0.8320,  0.4512,  0.3730], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 24/69 [05:24<10:20, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.1318, -0.1719, -0.0859,  0.1196], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.1040, -0.1221, -0.0106, -0.0640], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1543,  0.1416, -0.8242,  0.0361, -0.6289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2344,  0.8516, -0.7578,  1.3047,  0.3789], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 25/69 [05:38<10:07, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2207, -0.0796, -0.5273, -0.3262,  0.2754], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3262,  0.0728,  0.1719, -0.2100, -0.1484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1206,  0.1738, -0.8828, -0.1787, -0.4707], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.7656, -0.1133, -1.7891,  0.5469, -0.3340], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 26/69 [05:52<09:49, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0967,  0.3164, -0.1982, -0.1777,  0.1562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.2412, -0.1758, -0.0771,  0.0625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1914,  0.2461, -0.7773, -0.0386, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3703, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4824,  0.4199, -0.7188,  1.2656, -0.1416], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 27/69 [06:05<09:32, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0952,  0.2832, -0.0762, -0.0835,  0.2119], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.3320, -0.0723, -0.0183,  0.0042], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2148,  0.2930, -0.7891,  0.0148, -0.5391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1074,  0.3008, -0.4414,  1.5156,  0.1689], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 28/69 [06:18<09:14, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1162,  0.2910, -0.0444, -0.1670,  0.2158], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3965,  0.3379, -0.0564, -0.1357,  0.0854], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2031,  0.2158, -0.7812, -0.1396, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6211,  0.6953, -0.6602,  1.0859,  0.4609], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 29/69 [06:32<08:58, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2168,  0.2227, -0.2324, -0.2578,  0.0078], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.2539,  0.0408, -0.1729,  0.0082], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2285,  0.1904, -0.6953, -0.1992, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3789,  0.5156, -0.3848,  0.5664,  0.1885], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 30/69 [06:45<08:40, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1797,  0.2715, -0.1641, -0.3008, -0.0271], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.3047,  0.1182, -0.2598, -0.0115], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.2871, -0.6797, -0.3223, -0.6055], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3301,  0.6875, -0.5469,  0.2988, -0.2852], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 31/69 [06:58<08:26, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2158,  0.1289, -0.1768, -0.3730,  0.0014], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2402,  0.1904, -0.0457, -0.1514, -0.0520], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3770,  0.0767, -0.8359, -0.1797, -0.4961], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5508,  1.0469, -0.9336,  1.3438,  0.0063], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 32/69 [07:11<08:08, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1147,  0.2090, -0.0918, -0.2178,  0.2852], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.3555, -0.1953, -0.1533,  0.0415], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1641,  0.2676, -0.7891, -0.1250, -0.6719], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.0938,  0.7539, -0.8438,  1.1094, -0.3281], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 33/69 [07:24<07:54, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.2793, -0.1885, -0.1216,  0.2773], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.1143, -0.1133, -0.0430, -0.0096], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2031,  0.0747, -0.9023, -0.0138, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.9414,  0.7305, -0.4551,  1.6875, -0.0869], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 34/69 [07:37<07:41, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3242,  0.2344, -0.0977,  0.0200,  0.2891], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2500,  0.1533, -0.2617, -0.0498, -0.1621], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0272,  0.1807, -0.8867,  0.0110, -0.6445], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3672, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0679,  0.3066, -1.0469,  1.6250, -0.4824], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 35/69 [07:50<07:28, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4375,  0.1709, -0.1982, -0.3008,  0.2236], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2305,  0.1797, -0.3574, -0.1543, -0.1748], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.0442, -0.9219, -0.1001, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3642, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8945,  0.6797, -0.0791,  1.5156, -0.0654], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 36/69 [08:04<07:20, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.1562, -0.1216, -0.1836,  0.3242], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3379,  0.2354, -0.1040, -0.0786, -0.1328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2246,  0.0635, -0.9336, -0.1309, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-1.1719,  0.5156, -0.8867,  1.7109, -0.1982], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 37/69 [08:18<07:09, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3701, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2354,  0.0757, -0.6289, -0.2314,  0.1504], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3701, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1523,  0.2139, -0.0630, -0.0688, -0.0306], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3701, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1553,  0.1758, -0.9844,  0.0178, -0.5039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3701, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.3262, -0.5820,  1.0391, -0.2793], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 38/69 [08:32<06:59, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3750,  0.0535, -0.2266, -0.1963,  0.0928], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217,  0.0496, -0.1807, -0.0547,  0.1084], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1787,  0.1592, -0.8477,  0.0188, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3685, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.6523, -0.9805,  1.3516, -0.5117], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 39/69 [08:45<06:44, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3965, -0.0104, -0.1738, -0.1689,  0.0613], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1953, -0.0576, -0.2363, -0.0255,  0.0630], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1992,  0.0786, -0.8164, -0.0107, -0.5117], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4727,  0.6406, -0.9648,  1.0703, -0.7617], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 40/69 [08:58<06:29, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637, -0.0508, -0.2617, -0.2373,  0.0913], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.1289, -0.2578, -0.1089, -0.0591], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1787,  0.2695, -0.9023, -0.0913, -0.6523], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.8711, -0.7031,  0.5547, -0.2520], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 41/69 [09:12<06:17, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3770,  0.1123, -0.1797, -0.1768,  0.0845], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2256,  0.0752, -0.1904, -0.0435,  0.0688], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1963,  0.1807, -0.8438,  0.0476, -0.4785], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3697, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2148,  0.4512, -0.9062,  1.0859, -0.6875], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 42/69 [09:26<06:06, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.0508, -0.1816, -0.1895,  0.0840], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.0042, -0.2139, -0.0767,  0.0544], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.0938, -0.8984,  0.0195, -0.5000], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2676,  0.7031, -0.7773,  1.2500, -0.8516], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 43/69 [09:39<05:51, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3066,  0.1011, -0.1924, -0.2578,  0.0488], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2188,  0.0522, -0.1387, -0.1348,  0.1025], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.1396, -0.8203, -0.0425, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.6445, -0.8906,  1.1797, -0.6406], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 44/69 [09:52<05:37, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2891, -0.0005, -0.3320, -0.1885,  0.1338], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2207,  0.1133, -0.1943, -0.0581, -0.0659], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0874,  0.2383, -0.9336, -0.0503, -0.6523], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5703,  0.7578, -0.7109,  0.5391, -0.3320], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 45/69 [10:06<05:26, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4180,  0.1069, -0.1875, -0.2754,  0.1455], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2188,  0.2578, -0.3027, -0.0630,  0.0142], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0771,  0.3203, -0.9062, -0.0193, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2275,  0.4551, -0.8320,  1.1875, -0.2676], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 46/69 [10:19<05:09, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598, -0.0046, -0.3906, -0.2354,  0.1377], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1826,  0.1089, -0.2275, -0.0618, -0.0776], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1270,  0.2812, -0.8555, -0.1182, -0.6680], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3965,  0.9414, -0.5625,  0.5469, -0.1201], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 47/69 [10:33<04:56, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3691,  0.0513, -0.2598, -0.2930,  0.0337], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1650,  0.1777, -0.1357, -0.1367, -0.0266], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1021,  0.2695, -0.9023, -0.1016, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.5195, -0.6406,  0.6523, -0.3066], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 48/69 [10:46<04:43, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715, -0.0361, -0.3535, -0.1973,  0.1069], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1787,  0.1079, -0.2314, -0.0820, -0.0938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0693,  0.2178, -0.8203, -0.1426, -0.6250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5391,  0.5117, -0.6641,  0.3145, -0.1021], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 49/69 [11:00<04:29, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1279,  0.0747, -0.2734, -0.1079,  0.0243], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.1699, -0.1982, -0.0579, -0.1138], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3008,  0.0840, -0.8086, -0.0466, -0.6758], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0038,  0.8789, -0.5625,  0.8594, -0.4902], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 50/69 [11:14<04:16, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1973,  0.2520, -0.2080, -0.1973,  0.2383], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.2461,  0.0417, -0.1465,  0.0457], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2500,  0.1953, -0.7578, -0.1128, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7188,  0.5469, -1.0938,  1.1641, -0.4023], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 51/69 [11:27<04:04, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1670,  0.2451, -0.1875, -0.2285,  0.0654], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.1182,  0.1299, -0.1338, -0.1094], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1514,  0.2676, -0.6836, -0.1270, -0.4570], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8555,  0.0815, -0.7500,  0.8516,  0.5469], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 52/69 [11:41<03:52, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1348,  0.1582, -0.0361, -0.2256,  0.1660], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.2285, -0.0903, -0.1572, -0.0684], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2109,  0.2217, -0.7891, -0.1602, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3594,  0.7695, -0.8945,  0.8906, -0.1030], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 53/69 [11:55<03:38, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3728, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0496,  0.1177, -0.0771,  0.0859,  0.1162], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3728, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2178,  0.2061, -0.1943,  0.0520, -0.0129], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3728, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3086,  0.2217, -0.6836, -0.0061, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3728, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.5000,  0.3984, -0.5039,  1.1172,  0.1602], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 54/69 [12:08<03:24, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1787,  0.2930, -0.0806, -0.1426,  0.2344], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.1494, -0.1641, -0.1260,  0.0074], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1787,  0.1348, -0.8008, -0.1699, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3696, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1445,  0.4570, -0.0065,  0.8281,  0.2832], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 55/69 [12:22<03:09, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3965,  0.1611, -0.2949, -0.1826,  0.2109], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.0923, -0.0552, -0.0908,  0.0004], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1406,  0.1406, -0.8633, -0.0806, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.9453, -0.1011, -1.3828,  0.7539, -0.0913], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 56/69 [12:35<02:54, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3398,  0.1738, -0.2852, -0.2012,  0.2832], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.1797, -0.0659, -0.1748, -0.0011], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2100,  0.2305, -0.8984, -0.1562, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.9766,  0.2051, -1.6094,  0.5273, -0.0869], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 57/69 [12:48<02:40, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1089,  0.2295, -0.0096, -0.1128,  0.0057], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2305,  0.1758, -0.2578, -0.1079, -0.0177], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2715,  0.2314, -0.8359, -0.1299, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([0.1729, 0.6289, 0.1094, 0.6172, 0.2354], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 58/69 [13:02<02:27, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1157,  0.2256, -0.2617, -0.2197,  0.1299], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.2051, -0.1836, -0.1592, -0.0510], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2891,  0.1729, -0.8008, -0.1309, -0.5391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3677, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0317,  0.2832, -0.4590,  0.7500, -0.3047], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 59/69 [13:15<02:13, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1387,  0.0815, -0.2598, -0.1562,  0.1230], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.3262, -0.0278, -0.0532,  0.0117], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812,  0.2002, -0.8242, -0.0266, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3658, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1562,  1.0078, -0.8203,  1.2656, -0.4141], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 60/69 [13:28<02:00, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0908,  0.1030, -0.2354, -0.1270,  0.1533], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.1133, -0.0060, -0.0398,  0.0200], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0649,  0.2021, -0.8047, -0.1196, -0.6250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3242,  0.3730, -1.1094,  0.8359, -0.0096], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 61/69 [13:42<01:46, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0737,  0.1455, -0.1709, -0.2617,  0.1924], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3555,  0.2393,  0.0222, -0.1230,  0.1904], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2090,  0.1836, -0.7852, -0.1436, -0.5195], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4688,  1.0234, -0.7969,  1.1797, -0.2139], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 62/69 [13:55<01:33, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1729,  0.1211, -0.2334,  0.0525,  0.2422], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.0732, -0.2471,  0.0304, -0.0608], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0403,  0.1406, -0.9180,  0.1113, -0.6719], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3659, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4258,  1.0078, -0.1914,  1.5625, -0.0608], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 63/69 [14:08<01:20, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.1650, -0.2432, -0.2734,  0.1475], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.2988,  0.1504, -0.1631, -0.0542], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2793,  0.2314, -0.7812, -0.2129, -0.6523], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3645, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5977,  0.6875, -1.2344,  0.7461, -0.6328], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 64/69 [14:22<01:07, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.2197, -0.2373, -0.1807,  0.1582], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3281,  0.2236, -0.1807, -0.0947,  0.1196], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1875,  0.2295, -0.8672, -0.1426, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3665, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2988,  0.9805, -0.8086,  1.0469,  0.1748], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 65/69 [14:35<00:53, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1904,  0.2129, -0.2080, -0.2461,  0.0413], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3086,  0.2715, -0.0275, -0.1187,  0.0918], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1504,  0.2051, -0.8242, -0.1289, -0.6250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0605,  1.0156, -0.6172,  0.9688, -0.0508], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 66/69 [14:49<00:40, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2061,  0.1846, -0.1641, -0.2285,  0.1953], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1826,  0.1992, -0.2520, -0.0918, -0.0080], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0253,  0.2275, -0.8984, -0.0713, -0.6953], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3662, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4160,  0.5586, -1.1094,  1.0078, -0.3145], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 67/69 [15:02<00:26, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1172,  0.1104, -0.1118, -0.0233,  0.1025], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3438,  0.0679, -0.0498,  0.0082,  0.0209], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0688,  0.1562, -0.8789,  0.0732, -0.6016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3644, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  1.2266, -0.1904,  1.0625, -0.3340], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 68/69 [15:15<00:13, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3378, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.1206, -0.0092, -0.3711, -0.1680,  0.0320], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3378, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.3242,  0.4180, -0.0576, -0.0776,  0.0417], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3378, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.2812,  0.2969, -0.9062, -0.0422, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3378, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.4004,  0.8555, -0.8086,  1.3906, -0.7031], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [15:19<00:00, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 1559, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.5039,  0.1963, -0.5469, -0.1455,  0.2275], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 1559, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.2393,  0.2334, -0.1064, -0.0408,  0.1084], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 1559, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.1611,  0.2715, -0.8672,  0.0598, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 1559, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.0359,  0.2539, -1.5547,  1.1172, -0.2539], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "friends/s1\n",
      "Movie:      /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s1/friends_s01e16b.mkv\n",
      "Transcript: /kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s1/friends_s01e16b.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames in the video: 21554.0\n",
      "Original Resolution: (720.0, 480.0)\n",
      "FPS: 29.968454258675077\n",
      "Duration (seconds): 719.222947368421\n",
      "Target Resolution: (224, 224)\n",
      "Read 21554 frames.\n",
      "Frames shape: torch.Size([21554, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6f7064069995>:50: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 719.23 seconds\n",
      "Number of intervals: 482\n",
      "Sample rate: 48000\n",
      "Output file: /kaggle/working/friends/s1/friends_s01e16b.h5\n",
      "Num splits: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 1/69 [00:13<14:47, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1201, -0.0488,  0.0815, -0.6055, -0.3887], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2812, -0.2910, -0.0079, -0.1895, -0.0308], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0266,  0.0527, -0.5352, -0.1689, -0.6211], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 1.3906, -0.1738, -1.5234,  0.0879, -1.5000], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/69 [00:26<14:35, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2256,  0.0669,  0.0703, -0.1069,  0.2432], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3027,  0.1406, -0.1523,  0.0029,  0.0234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1387,  0.0417, -0.9648,  0.0613, -0.4434], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0305,  0.7656, -1.0078,  2.4219,  0.0425], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/69 [00:39<14:31, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1602,  0.0908, -0.1465, -0.1050,  0.1201], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1387, -0.1641, -0.0070, -0.0366], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.1426, -0.9258,  0.0141, -0.5391], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2070,  0.2773, -0.6211,  1.8750, -0.3438], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/69 [00:52<14:19, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1875,  0.1406,  0.0698, -0.1211,  0.2832], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2773,  0.1689, -0.1177, -0.0532,  0.1187], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1650,  0.0801, -0.8906,  0.0304, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0400,  0.4609, -1.1875,  2.0938, -0.3262], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/69 [01:05<14:04, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3398, -0.0014, -0.1836, -0.1533,  0.0354], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2051, -0.0111,  0.0295, -0.1592,  0.1992], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1157,  0.2129, -0.8398, -0.1777, -0.5742], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1035, -0.6328, -1.0312,  1.2812, -0.4062], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/69 [01:19<13:51, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1602,  0.3438, -0.0256, -0.1699,  0.1797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.2393, -0.1904, -0.0728,  0.0942], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2539,  0.2246, -0.7578, -0.1138, -0.4727], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1611,  0.3594, -1.2344,  0.5586, -0.1895], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/69 [01:32<13:38, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1406,  0.1279, -0.0164, -0.2061,  0.2051], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3477,  0.2393, -0.0157, -0.1064,  0.0684], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.2676, -0.7773, -0.1279, -0.4844], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5547,  0.4258, -0.8125,  1.1172,  0.0977], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/69 [01:45<13:32, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2236,  0.3652, -0.1963, -0.1260,  0.1943], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3613,  0.2461, -0.3691, -0.0693,  0.1377], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3301,  0.2324, -0.8164, -0.0869, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0403,  0.1973, -0.7695,  1.3984,  0.1279], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/69 [01:58<13:14, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0354,  0.2295, -0.1904, -0.1328,  0.1406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3418,  0.0295, -0.1128, -0.0400,  0.0099], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2246,  0.0574, -0.8555, -0.0349, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3668, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1104,  0.0037, -1.0234,  0.9102, -0.1885], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10/69 [02:12<13:01, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1455,  0.2344,  0.0090, -0.2119,  0.0356], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-3.0273e-01,  2.6953e-01, -6.4373e-05, -1.5820e-01, -1.4746e-01],\n",
      "       device='cuda:1', dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2471,  0.1074, -0.7383, -0.2021, -0.5664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4121,  0.5586, -1.1250,  0.3867, -0.0430], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 11/69 [02:25<12:51, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3320,  0.1846,  0.2100, -0.1816,  0.1348], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1030,  0.1133, -0.0732, -0.0045], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0084,  0.1943, -0.7812,  0.0084, -0.6367], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0186,  0.6328, -0.9492,  1.1484,  0.3164], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/69 [02:38<12:35, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0684,  0.1069, -0.1138, -0.1357,  0.1494], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2871,  0.1338, -0.1123, -0.0586, -0.0454], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3711,  0.1758, -0.9102, -0.0859, -0.5859], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3684, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.3320,  0.1943, -0.8203,  0.8359, -0.2314], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 13/69 [02:52<12:27, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0150,  0.0234, -0.2754, -0.1650,  0.2598], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.2197, -0.0825, -0.0540,  0.0588], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2471,  0.2412, -0.6680, -0.0381, -0.4961], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4102,  0.9141, -1.2969,  0.9375,  0.4824], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 14/69 [03:05<12:16, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0116,  0.1787, -0.1182, -0.1133,  0.2656], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2227,  0.2422, -0.0732, -0.0469, -0.0103], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.2383, -0.7891, -0.0228, -0.5820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0894,  0.8047, -1.8438,  1.0000,  0.1807], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 15/69 [03:19<12:11, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1934,  0.1729, -0.3711, -0.1162,  0.0811], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3125,  0.2832, -0.2461, -0.0128,  0.0669], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.2324, -0.8281, -0.0242, -0.5430], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3688, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1211,  0.1108, -0.7422,  1.5625,  0.1235], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/69 [03:33<11:59, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0025,  0.2832,  0.0391, -0.2832,  0.1260], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3008,  0.1748, -0.0535, -0.2451,  0.0806], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.1152, -0.8125, -0.2227, -0.5625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.5625, -1.0234,  1.2969, -0.3262], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 17/69 [03:47<11:47, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1357,  0.1641, -0.1572, -0.1191,  0.1533], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.1001, -0.0703, -0.0437, -0.0850], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1064,  0.1367, -0.8438, -0.0192, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0126,  0.4121, -1.1094,  0.5938,  0.9297], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 18/69 [04:00<11:28, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1641,  0.2832,  0.0771, -0.2021,  0.1533], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3555,  0.3008, -0.2490, -0.1377,  0.1035], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.2344, -0.8008, -0.1709, -0.5547], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3643, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0035,  0.0664, -1.1641,  1.1953, -0.2041], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 19/69 [04:13<11:16, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1904, -0.4707, -0.2637,  0.1138], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2217,  0.1338, -0.0791, -0.2354, -0.0908], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2412,  0.0635, -0.8594, -0.2656, -0.5703], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3457,  0.7734, -0.6914,  0.3809, -0.3516], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 20/69 [04:27<11:01, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4023,  0.1206,  0.1523, -0.3262,  0.0942], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2695,  0.0669, -0.0508, -0.1621, -0.1016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0405,  0.0540, -0.7695, -0.1196, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3678, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2031,  1.0391, -0.4238,  0.4492,  0.1196], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 21/69 [04:40<10:50, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2461,  0.2305, -0.0654, -0.3164,  0.1562], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2119,  0.0425,  0.1201, -0.2754, -0.0854], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1147,  0.1157, -0.8711, -0.2383, -0.5117], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3691, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.9258, -0.0297, -0.6836,  0.1436, -0.2021], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 22/69 [04:54<10:36, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3066,  0.2021,  0.1221, -0.1914,  0.1328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2119,  0.0757, -0.1533, -0.1504, -0.0603], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1416,  0.2178, -0.8359, -0.0688, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3689, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5391,  0.8008, -1.3281,  1.1016, -0.2930], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 23/69 [05:08<10:26, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2412,  0.2061,  0.1416, -0.2412,  0.2002], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2129,  0.0830, -0.0042, -0.1650, -0.0136], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0349,  0.1855, -0.7617, -0.1270, -0.5664], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6680,  0.4277, -0.7891,  0.4199, -0.2334], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 24/69 [05:21<10:12, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.0101, -0.0503, -0.2559, -0.1094], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1089,  0.2061,  0.0845, -0.1641, -0.1826], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2285,  0.2441, -0.8359, -0.1768, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8672,  0.9609, -1.0312,  0.5938, -0.2002], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 25/69 [05:35<09:59, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2461,  0.1953,  0.1055, -0.1235,  0.0378], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1924, -0.0520,  0.0510, -0.0786,  0.0640], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1055,  0.1768, -0.7852,  0.0165, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3680, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0786,  1.2109, -0.5039,  0.6914,  0.0383], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 26/69 [05:49<09:47, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1533,  0.0659, -0.1279, -0.1465,  0.1807], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.2354, -0.2285, -0.1348,  0.0476], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1797,  0.1689, -0.9102, -0.1377, -0.6094], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3664, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0044,  0.9141, -1.4766,  1.3516, -0.3672], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 27/69 [06:03<09:34, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1099,  0.2363, -0.1289, -0.1299,  0.1245], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2236,  0.1777, -0.1895, -0.0947, -0.0762], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0532,  0.1836, -0.7617, -0.0767, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1108,  0.5820, -0.3027,  0.7930,  0.0825], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 28/69 [06:17<09:24, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2051,  0.1631, -0.2969, -0.1660,  0.2197], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.1904, -0.3242,  0.0096, -0.0947], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0903,  0.0413, -0.8125,  0.0376, -0.4746], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.5625,  0.1113, -1.5156,  1.4688, -0.1572], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 29/69 [06:30<09:10, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2354,  0.1270,  0.0549, -0.1719, -0.0713], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.1348, -0.2266, -0.1201, -0.1128], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0898,  0.2051, -0.9375, -0.1279, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.9297,  0.9141, -0.7188,  1.3047,  0.1514], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 30/69 [06:44<08:57, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2305,  0.3340, -0.0518, -0.1514,  0.1016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.2520, -0.3945, -0.0417, -0.0820], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2090,  0.2197, -0.7500,  0.0239, -0.5977], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0293,  0.7305, -1.0391,  0.8711,  0.1738], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 31/69 [06:58<08:45, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1484,  0.2520, -0.1357, -0.1582,  0.2373], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2637,  0.2793, -0.2324, -0.0547, -0.0149], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.1592, -0.7227,  0.0282, -0.5586], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3682, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.1543,  1.0938, -1.0625,  1.5625,  0.0674], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 32/69 [07:12<08:31, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0147,  0.1602,  0.0142, -0.2480,  0.1777], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2930,  0.1992, -0.0796, -0.1416, -0.0299], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.1416, -0.7070, -0.1387, -0.4023], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3683, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.2275,  0.3633, -0.9961,  0.6367,  0.6250], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 33/69 [07:25<08:14, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.1367, -0.1206, -0.0781,  0.1191], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1738,  0.2695, -0.0649,  0.0439,  0.1338], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1807,  0.1963, -0.6875,  0.0216, -0.4824], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3692, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1416,  0.6367, -1.2891,  1.3203, -0.0132], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 34/69 [07:39<08:01, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2598,  0.2266, -0.2002, -0.1328,  0.0327], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.2793, -0.1484, -0.0081,  0.1196], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2109,  0.2119, -0.7188,  0.0317, -0.4941], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4023,  0.6562, -1.2500,  1.6016,  0.0942], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 35/69 [07:53<07:49, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1777,  0.1494, -0.0138, -0.1279,  0.3320], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3477,  0.1475, -0.0791, -0.0947,  0.0625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0574,  0.0698, -0.8555, -0.0190, -0.5625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3649, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5977,  1.4609, -0.8555,  1.6953, -0.0123], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 36/69 [08:07<07:34, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2656,  0.3320, -0.0444, -0.1543,  0.1484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2500,  0.1709, -0.0674, -0.0364,  0.0898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0781,  0.2080, -0.7891,  0.0544, -0.4297], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5117,  0.2988, -0.8438,  1.8516,  0.1084], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 37/69 [08:20<07:18, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2412, -0.0182, -0.0791, -0.0193,  0.2197], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.1611, -0.0884,  0.0378,  0.0461], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0669,  0.2363, -0.9258,  0.1357, -0.6602], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3651, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4590,  0.6250, -0.9531,  2.0469, -0.2578], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 38/69 [08:34<07:02, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1641,  0.1191, -0.1030, -0.1377,  0.1001], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3477,  0.3125, -0.3438, -0.0114,  0.0559], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.2383, -0.7188,  0.0408, -0.5234], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3648, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0952,  0.3594, -1.5391,  1.9375, -0.4785], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 39/69 [08:48<06:49, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2344, -0.0182, -0.2832, -0.4180,  0.2236], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1138, -0.0791, -0.1973, -0.1680,  0.0203], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0505,  0.1289, -1.0312, -0.3730, -0.5195], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3671, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5469, -0.0271, -0.2832,  0.1299, -0.0483], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 40/69 [09:01<06:36, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2256, -0.1504, -0.1514, -0.1875, -0.0732], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2559,  0.0947,  0.2715, -0.0388, -0.1035], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1631,  0.0564, -0.7969,  0.0025, -0.4961], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5664,  0.7461, -1.0625,  0.8828, -0.3984], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 41/69 [09:15<06:20, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3164,  0.0874,  0.0796, -0.2500, -0.0381], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1992,  0.1367,  0.0172, -0.1133, -0.0144], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1191,  0.1396, -0.7188, -0.0723, -0.5469], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3670, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6797,  0.7930, -0.6914,  0.7344, -0.2793], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 42/69 [09:28<06:04, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1309,  0.1060, -0.2812, -0.2490,  0.1387], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2168,  0.0933,  0.0055, -0.1797, -0.0811], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1201,  0.1396, -0.7539, -0.1816, -0.5898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7578,  0.2734, -0.4590,  0.3613, -0.0270], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 43/69 [09:41<05:49, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3809,  0.2598,  0.0791, -0.1377,  0.1455], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2314,  0.0513, -0.1377, -0.1055, -0.0625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0312,  0.0608, -0.7734, -0.0591, -0.4590], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3667, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4434,  1.2656, -0.4414,  0.9180,  0.1836], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 44/69 [09:55<05:37, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2754,  0.0889, -0.2656, -0.1973,  0.0698], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2139,  0.0742, -0.0503, -0.0659, -0.0811], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1045,  0.0967, -0.7734, -0.0688, -0.5508], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3657, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.9688,  0.3457, -0.7109,  1.1875, -0.0693], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 45/69 [10:09<05:28, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1553,  0.0796, -0.1953, -0.1128,  0.1328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2432,  0.2266, -0.1235, -0.0635,  0.0422], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1699,  0.2129, -0.7148,  0.0874, -0.5625], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3687, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5703,  0.7266, -1.1328,  1.4141, -0.3340], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 46/69 [10:23<05:16, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1973,  0.0767, -0.1650, -0.1914,  0.1797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.1177, -0.1113, -0.0698,  0.1084], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1006,  0.0659, -0.7539,  0.0317, -0.5781], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3675, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4004,  0.3711, -0.8242,  1.0156, -0.5664], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 47/69 [10:37<05:01, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2617,  0.1953, -0.0767, -0.1318,  0.1719], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.2637, -0.2754, -0.0986,  0.0459], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2070,  0.2676, -0.8477, -0.0139, -0.6289], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4121,  0.3926, -1.3984,  1.3828, -0.5742], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 48/69 [10:51<04:50, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1118,  0.1226, -0.0942, -0.1592,  0.1484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1943,  0.1992, -0.2207, -0.1206,  0.0596], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1543,  0.1338, -0.8086, -0.1289, -0.6016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3695, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6250,  0.4648, -0.9023,  1.1484, -0.0134], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 49/69 [11:05<04:37, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0259, -0.0051, -0.0095, -0.1040,  0.2715], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3320,  0.2031, -0.0874, -0.0762,  0.0388], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1357,  0.2021, -0.9844, -0.0298, -0.6641], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3693, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-6.1768e-02,  8.2422e-01, -1.0781e+00,  1.1875e+00,  6.4468e-04],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 50/69 [11:18<04:19, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3704, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0913,  0.1113, -0.1001, -0.1738,  0.1338], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3704, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3027,  0.1035, -0.1836, -0.1514,  0.0427], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3704, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1777,  0.1826, -0.7891, -0.0757, -0.6484], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3704, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6484,  0.9648, -1.1641,  1.0547,  0.2871], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 51/69 [11:31<04:04, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0081, -0.0137,  0.1240, -0.1484,  0.2988], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.2656, -0.0535, -0.1504,  0.0771], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1089,  0.2559, -0.8906, -0.0942, -0.6797], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3694, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2461,  0.9609, -0.8477,  1.1172, -0.0291], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 52/69 [11:45<03:49, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0310,  0.0403,  0.0505, -0.2197,  0.2949], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3633,  0.2754, -0.0383, -0.1934,  0.0830], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1377,  0.3047, -0.9375, -0.1279, -0.6641], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3673, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2969,  0.6836, -0.6797,  1.2031, -0.3125], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 53/69 [11:58<03:35, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0206,  0.1318, -0.1143, -0.1943,  0.0120], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2949,  0.2598, -0.0938, -0.0747,  0.0228], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2041,  0.2207, -0.7422, -0.1055, -0.6133], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3676, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1777,  0.8164, -1.0625,  1.0703, -0.2285], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 54/69 [12:11<03:22, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0977,  0.0221,  0.1514, -0.1680,  0.2637], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3008,  0.2207, -0.1396, -0.1143,  0.0015], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1167,  0.1660, -0.9609, -0.0204, -0.6758], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3681, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.0913,  0.4316, -0.7656,  1.4531,  0.2539], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 55/69 [12:25<03:10, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1289,  0.0284, -0.1338, -0.1787,  0.1650], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3145,  0.3652, -0.1523, -0.1143, -0.0349], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2910,  0.1748, -0.7930, -0.0564, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3698, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4141,  0.4727, -0.8555,  1.6953, -0.3418], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 56/69 [12:39<02:55, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0513,  0.1885, -0.0742, -0.1475,  0.1445], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3203,  0.1914, -0.1006, -0.0356,  0.0566], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2295,  0.1895, -0.8164,  0.0112, -0.6250], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3686, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5078,  0.5430, -0.9141,  1.1016, -0.4258], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 57/69 [12:52<02:40, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0474,  0.0845,  0.0928, -0.0559,  0.2129], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.2119, -0.0835, -0.0957,  0.0444], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1572,  0.2109, -0.8203, -0.0184, -0.6406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3640, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.7148,  0.7930, -0.6602,  1.1406,  0.0483], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 58/69 [13:05<02:27, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0508,  0.2246, -0.0737, -0.1299,  0.0698], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2734,  0.2793, -0.0381, -0.0669,  0.0220], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2402,  0.2637, -0.7812, -0.0564, -0.6328], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1611,  0.8008, -0.8047,  0.8359, -0.2598], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 59/69 [13:19<02:13, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3613,  0.1631, -0.2080, -0.2373,  0.2168], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.1396,  0.0457, -0.2480, -0.0253], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1914,  0.1289, -1.0078, -0.2559, -0.5898], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3626, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1719, -0.2637, -0.6133,  0.1064, -0.0981], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 60/69 [13:32<02:01, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3184,  0.2090, -0.4141, -0.1182,  0.0918], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3633,  0.1162,  0.1406, -0.1816, -0.1016], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1553,  0.2070, -0.8633, -0.1729, -0.6406], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3655, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8828,  0.3027, -0.7773,  0.7734,  0.0615], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 61/69 [13:46<01:47, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5781, -0.1641, -0.4609, -0.2451,  0.3848], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3203,  0.1475, -0.0374, -0.2129,  0.0364], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3652,  0.0188, -0.9141, -0.2109, -0.5039], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3666, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.6250,  0.1328, -0.6367,  1.0781, -1.1641], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 62/69 [13:59<01:34, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4766,  0.1133, -0.3652, -0.2070,  0.4766], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3027,  0.1562, -0.0669, -0.1206, -0.0112], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.0752, -0.7305, -0.2021, -0.3945], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3660, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.5078,  0.1235, -0.5312,  0.6992, -1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 63/69 [14:13<01:21, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4180,  0.0146, -0.3477, -0.1553,  0.3008], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1118,  0.2334, -0.2949, -0.1650, -0.1040], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1191,  0.2715, -1.0000, -0.1221, -0.4551], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3674, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.0452,  0.2773, -0.9883,  1.3203, -0.6680], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 64/69 [14:27<01:08, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3691,  0.1074, -0.6172, -0.3965,  0.2168], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2383, -0.1826, -0.0540, -0.2217, -0.0459], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1934, -0.0043, -0.8477, -0.3535, -0.3496], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3679, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.8281, -0.1816, -0.8164, -0.0097,  0.5898], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 65/69 [14:41<00:54, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2578,  0.1396,  0.1006, -0.2539,  0.2041], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2832,  0.2969, -0.1045, -0.0933, -0.0640], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1846,  0.2021, -0.7578, -0.0549, -0.4961], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3663, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.4883,  0.8438, -0.2695,  1.8281,  0.5781], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 66/69 [14:54<00:41, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3359,  0.2295,  0.0713, -0.1650,  0.1157], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3047,  0.1680,  0.0337, -0.0110, -0.0171], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.3359,  0.1895, -0.7148,  0.1099, -0.4824], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3653, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([ 0.5156,  0.4316, -0.6133,  1.4766,  0.1553], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 67/69 [15:08<00:27, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.4004,  0.1592, -0.0854, -0.1021,  0.3340], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2891,  0.1729, -0.0312,  0.0020, -0.1064], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.1118,  0.2178, -0.9297,  0.0276, -0.5352], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3652, 3584]), Averaged feature shape: torch.Size([14, 3584]), Sample: tensor([-0.2852,  0.9648, -1.2734,  1.4062,  0.4121], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 68/69 [15:20<00:13, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 3381, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.4258,  0.0654, -0.1011, -0.1099,  0.3379], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 3381, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.2754,  0.1553, -0.2256, -0.0238, -0.0679], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 3381, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.1748,  0.1670, -0.9648,  0.0786, -0.5156], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 3381, 3584]), Averaged feature shape: torch.Size([13, 3584]), Sample: tensor([-0.0608,  1.0781, -0.8555,  2.1406,  0.2178], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [15:24<00:00, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: language_model.model.layers.10.post_attention_layernorm, Feature shape: torch.Size([1, 1559, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.4160,  0.0201,  0.0398, -0.0486,  0.3906], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.15.post_attention_layernorm, Feature shape: torch.Size([1, 1559, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.2637,  0.0986, -0.2344,  0.0043, -0.1494], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.layers.20.post_attention_layernorm, Feature shape: torch.Size([1, 1559, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.1128,  0.1514, -1.0234,  0.0508, -0.5938], device='cuda:1',\n",
      "       dtype=torch.bfloat16)\n",
      "Layer: language_model.model.norm, Feature shape: torch.Size([1, 1559, 3584]), Averaged feature shape: torch.Size([6, 3584]), Sample: tensor([-0.0981,  1.0156, -0.7891,  2.1562,  0.4043], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "parts = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 'wolf', 'life', 'bourne', 'figures'] \n",
    "movies_base = root_dir / \"algonauts_2025.competitors/stimuli/movies\"\n",
    "transcripts_base = root_dir / \"algonauts_2025.competitors/stimuli/transcripts\"\n",
    "out_dir = '/kaggle/working/'\n",
    "ignore_done = [\n",
    "    \"friends_s01e01a.h5\",\n",
    "    \"friends_s01e01b.h5\",\n",
    "    \"friends_s01e02a.h5\",\n",
    "    \"friends_s01e02b.h5\",\n",
    "    \"friends_s01e03b.h5\",\n",
    "    \"friends_s01e04a.h5\",\n",
    "    \"friends_s01e04b.h5\",\n",
    "    \"friends_s01e05a.h5\",\n",
    "    \"friends_s01e05b.h5\",\n",
    "    \"friends_s01e06a.h5\",\n",
    "    \"friends_s01e06b.h5\",\n",
    "    \"friends_s01e07a.h5\",\n",
    "    \"friends_s01e07b.h5\",\n",
    "    \"friends_s01e08b.h5\",\n",
    "    \"friends_s01e09a.h5\",\n",
    "    \"friends_s01e09b.h5\",\n",
    "    \"friends_s01e10a.h5\",\n",
    "    \"friends_s01e10b.h5\",\n",
    "    \"friends_s01e11a.h5\",\n",
    "    \"friends_s01e12a.h5\",\n",
    "    \"friends_s01e12b.h5\",\n",
    "    \"friends_s01e13a.h5\",\n",
    "    \"friends_s01e13b.h5\",\n",
    "    \"friends_s01e14a.h5\",\n",
    "    \"friends_s01e15b.h5\",\n",
    "    \"friends_s01e16a.h5\",\n",
    "    \"friends_s01e17b.h5\",\n",
    "    \"friends_s01e18a.h5\",\n",
    "    \"friends_s01e18b.h5\",\n",
    "    \"friends_s01e19a.h5\",\n",
    "    \"friends_s01e19b.h5\",\n",
    "    \"friends_s01e20a.h5\",\n",
    "    \"friends_s01e20b.h5\",\n",
    "    \"friends_s01e21a.h5\",\n",
    "    \"friends_s01e21b.h5\",\n",
    "    \"friends_s01e22a.h5\",\n",
    "    \"friends_s01e22b.h5\",\n",
    "    \"friends_s01e23a.h5\",\n",
    "    \"friends_s01e23b.h5\",\n",
    "    \"friends_s01e24a.h5\",\n",
    "    \"friends_s01e24b.h5\"\n",
    "]\n",
    "\n",
    "\n",
    "extract_features(parts = parts, movies_base = movies_base, transcripts_base = transcripts_base, output_dir = out_dir, extraction_fn = extract_fn, verbose = True, modality = 'all', past_context_in_seconds = 20, splits_overlap=0.5, ignore_done = ignore_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa0a7d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:37:35.108310Z",
     "iopub.status.busy": "2025-04-29T16:37:35.108022Z",
     "iopub.status.idle": "2025-04-29T16:37:35.111322Z",
     "shell.execute_reply": "2025-04-29T16:37:35.110692Z"
    },
    "papermill": {
     "duration": 0.067164,
     "end_time": "2025-04-29T16:37:35.112544",
     "exception": false,
     "start_time": "2025-04-29T16:37:35.045380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modality = 'video'\n",
    "# verbose = True\n",
    "# movie_file = \"/kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/movies/friends/s5/friends_s05e02a.mkv\"\n",
    "# transcript_file = \"/kaggle/input/algonauts2025nsl/algonauts_2025.competitors/stimuli/transcripts/friends/s5/friends_s05e02a.tsv\"\n",
    "# interval = 1.49\n",
    "\n",
    "# ####################33\n",
    "\n",
    "# video, audio, transcript, sample_rate, fps_video = None, None, None, None, None\n",
    "# if modality == 'all' or modality == 'video':\n",
    "#     video, fps_video = load_video(movie_file, verbose=verbose)\n",
    "# if modality == 'all' or modality == 'audio' or modality == 'video' or modality == 'transcript':\n",
    "#     audio, sample_rate = load_audio(movie_file)\n",
    "# if modality == 'all' or modality == 'transcript':\n",
    "#     transcript = load_transcript(transcript_file)\n",
    "\n",
    "# # round fps video\n",
    "\n",
    "# if transcript is not None:\n",
    "#     transcript = resample_transcript(transcript, interval)\n",
    "    \n",
    "# total_duration = audio.shape[1] / sample_rate\n",
    "# num_intervals = int(total_duration // interval)\n",
    "\n",
    "# if verbose:\n",
    "#     print(f\"Total duration: {total_duration:.2f} seconds\")\n",
    "#     print(f\"Number of intervals: {num_intervals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6dd4e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:37:35.232918Z",
     "iopub.status.busy": "2025-04-29T16:37:35.232638Z",
     "iopub.status.idle": "2025-04-29T16:37:35.235446Z",
     "shell.execute_reply": "2025-04-29T16:37:35.234860Z"
    },
    "papermill": {
     "duration": 0.064089,
     "end_time": "2025-04-29T16:37:35.236632",
     "exception": false,
     "start_time": "2025-04-29T16:37:35.172543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range num_intervals:\n",
    "#     video_section, audio_section, transcript_section = extract_section(\n",
    "#         video, audio, transcript, interval, i, sample_rate, modality, fps_video\n",
    "#     )\n",
    "#     print(video_section.shape)\n",
    "    \n",
    "\n",
    "# # output_features = extract_fn(video_section, audio_section, transcript_section, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7d93ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:37:35.355615Z",
     "iopub.status.busy": "2025-04-29T16:37:35.355358Z",
     "iopub.status.idle": "2025-04-29T16:37:35.358579Z",
     "shell.execute_reply": "2025-04-29T16:37:35.357791Z"
    },
    "papermill": {
     "duration": 0.064024,
     "end_time": "2025-04-29T16:37:35.359957",
     "exception": false,
     "start_time": "2025-04-29T16:37:35.295933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# # Path to the generated h5 file.\n",
    "# file_path = '/kaggle/working/friends/s5/friends_s05e10b.h5'\n",
    "\n",
    "# with h5py.File(file_path, 'r') as f:\n",
    "#     print(\"Datasets in the file:\")\n",
    "#     for layer_name in f.keys():\n",
    "#         dataset = f[layer_name]\n",
    "#         print(f\"Layer: {layer_name}\")\n",
    "#         print(f\" - Shape: {dataset.shape}\")\n",
    "#         print(f\" - Data type: {dataset.dtype}\")\n",
    "        \n",
    "#         num_intervals_to_print = min(50, dataset.shape[0])\n",
    "#         for i in range(num_intervals_to_print):\n",
    "#             # Get the i-th interval data, flatten it, and print the first 5 elements.\n",
    "#             interval_data = dataset[i]\n",
    "#             flat_data = interval_data.flatten()\n",
    "#             first_five = flat_data[:5] if flat_data.size >= 5 else flat_data\n",
    "#             print(f\" Interval {i}: first 5 elements: {first_five}\")\n",
    "#         print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6674202,
     "sourceId": 10759729,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7212.940745,
   "end_time": "2025-04-29T16:37:38.420029",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-29T14:37:25.479284",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0084d093c08b427a887c6b364b9a8d03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "014c3a4b19584933aeb7ca126ee4c4c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ec6cc47595da442085d6c564f5e4ec3a",
       "placeholder": "​",
       "style": "IPY_MODEL_ded1ead4fec44a8ab2de6e64e49c93a3",
       "tabbable": null,
       "tooltip": null,
       "value": "merges.txt: 100%"
      }
     },
     "015251e5633840fe9445d6adb7c9756f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "019d83f68efc445197fcc0827d1eaf46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "02db384147814a7397bc0a9eb73ed03d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a1b62ac171bc4d69977548d0649e44f6",
       "placeholder": "​",
       "style": "IPY_MODEL_ed7393a4fd9d465fbd15daed921b85fb",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "0300ca7e02c342ef8f9085d3a4a58506": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5a4168dce9f141f78155ef48cb69370f",
        "IPY_MODEL_9de73d244ece4159b25e6fbb2f2b2726",
        "IPY_MODEL_a6ca3a3bd0e249c2b4b72fa3c99556cd"
       ],
       "layout": "IPY_MODEL_6dc27a6bab2b48ab845702843c897d8f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "03e9d67dca834bda98e7447f5a4c6962": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fd4aa43421154e35ae940c50b91dbc09",
        "IPY_MODEL_af3568b6ccce4f06a9cd10013280342a",
        "IPY_MODEL_66fccc531424429382d4b5b6d1a056dc"
       ],
       "layout": "IPY_MODEL_09a7a2dd91fd49e289faadb0084d9ff0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "04bb38e296ce4d22990c3a832fddb0f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "053b7c5c14da491eacd565bbd6beec3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0555e74c88624d23954bfc7c30b5f6b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "056c888eb82b4c4ea0090e92586d82b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "059395ad411d43c9ab5f9c418bedf184": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "07fb176f2e6a414aa5ec609dea51a71d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_95e371b01ab74bb7add9103101c46f41",
       "placeholder": "​",
       "style": "IPY_MODEL_db7082c0c5f649c1b9e42466fb4146d8",
       "tabbable": null,
       "tooltip": null,
       "value": "model-00003-of-00004.safetensors: 100%"
      }
     },
     "09a7a2dd91fd49e289faadb0084d9ff0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a80acbcf2424927a03b3ca9bc6183fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0bc5bffa7561413b98c014261ba6b649": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_314764f9bbba4b9488fe3727a1dfda80",
        "IPY_MODEL_4b5d32c7b59540009d4c17d6177c8dcc",
        "IPY_MODEL_1dd2594c1c2040479ca5a4db0d44d3dc"
       ],
       "layout": "IPY_MODEL_10f7185ac703470c9f32cbdb2d0c0f7c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0c179b9ae16b48ffaf76a6196bf97bb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0d9305238b4849939d8aba1f02314126": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "10f7185ac703470c9f32cbdb2d0c0f7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1163d7909b57437ea53b1ecfc221281a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "131934edceaf440bb9ce6c9c8e53c09f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1373e8e6bdb44ec1846886807127df9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7f76be1b2f054f12859928a378b8a8ec",
       "placeholder": "​",
       "style": "IPY_MODEL_fdf9bc4398284a1bb6e596927ac48ab1",
       "tabbable": null,
       "tooltip": null,
       "value": "conversation.py: 100%"
      }
     },
     "1618736e2a9f40fe9676e78aa4eea9e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18b4fdeec6604ba3a9e430d6183ff2b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1914b0ba045b4adcbd0a70021f821ec8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_79bfbc69b9814f95b55f59d058a64082",
       "max": 1142280864,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2bdf499828a54aed9ab8420d02675697",
       "tabbable": null,
       "tooltip": null,
       "value": 1142280864
      }
     },
     "19a2a86b7f334aaf9c5e7fed9d8eea7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dc611e9e3100460586352b43b5c50eca",
       "placeholder": "​",
       "style": "IPY_MODEL_1dbf5c38dbe64d73b850713f7a4cb66e",
       "tabbable": null,
       "tooltip": null,
       "value": "generation_config.json: 100%"
      }
     },
     "1a1a5a16c2dc4d7887d2d128133a581c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a2291acb005474195acb378ec22d2d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1b670d2fb4194d9f93d036181d74798e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1caf92f4c66146098421048a53a89136": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_308fa5c760a14460b065d15a5d75e361",
       "placeholder": "​",
       "style": "IPY_MODEL_18b4fdeec6604ba3a9e430d6183ff2b9",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d183fc6b83f49b6a573e8b67601894e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1db0ff5e2fb640759797b6f81feac95a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9bfd1485e0ee4a0da1d1e098e8ce7b11",
       "max": 18099,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1a2291acb005474195acb378ec22d2d5",
       "tabbable": null,
       "tooltip": null,
       "value": 18099
      }
     },
     "1dbf5c38dbe64d73b850713f7a4cb66e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1dd2594c1c2040479ca5a4db0d44d3dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_83d2d1f2defe498eac2aaa23e2c46202",
       "placeholder": "​",
       "style": "IPY_MODEL_7e924eeb5e3f4e43b3f7af511680c39b",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [01:17&lt;00:00, 16.56s/it]"
      }
     },
     "20a9e55e0cd44d5ab1c7927740fcfa56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2349ab1106d54e538bd9137de973d203": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "24b58b7daa914edb82d7f6485dc793e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81971e8263d54e68973a705e780508bd",
       "placeholder": "​",
       "style": "IPY_MODEL_95d0cd8608a5461aaa5af4fb5031ff16",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "25b8ad6f8325470b8d2796bbdc3ab229": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_29ca7f0b2b194bdeaf9742db64c0738b",
       "placeholder": "​",
       "style": "IPY_MODEL_6e820190332f42e19a8948a223b22786",
       "tabbable": null,
       "tooltip": null,
       "value": "model-00004-of-00004.safetensors: 100%"
      }
     },
     "25ca333da0aa4138944e07501771069d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_805e5158257d4d59bf25697b4420c717",
       "max": 3383407,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_89df51d033d14fcc925e5283c9197060",
       "tabbable": null,
       "tooltip": null,
       "value": 3383407
      }
     },
     "25fcf94a493f4e5faeb789649ad8a907": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2629269eda014c84b51aff1e0f614248": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_02db384147814a7397bc0a9eb73ed03d",
        "IPY_MODEL_283d0614609d4ae2b7531f6cf0af19e6",
        "IPY_MODEL_38cb3dc6172848a1b9aecc9eefd23369"
       ],
       "layout": "IPY_MODEL_9feb69489b744ae2ab80d99aa89513f5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2637347f255e43c6a8ce9f2246c9db5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_850775ea144d401f840db4dd32790d51",
       "placeholder": "​",
       "style": "IPY_MODEL_f57ec0ed732048a2bf552dc1a7bb80ff",
       "tabbable": null,
       "tooltip": null,
       "value": " 4.99G/4.99G [00:23&lt;00:00, 244MB/s]"
      }
     },
     "27de74025530497e877b2d2939492e79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "283d0614609d4ae2b7531f6cf0af19e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1d183fc6b83f49b6a573e8b67601894e",
       "max": 613,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8768b4b8266f4b8e8bc39ce733db1b35",
       "tabbable": null,
       "tooltip": null,
       "value": 613
      }
     },
     "28f1f31cb8fd4b82834e9e8dd6450337": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_015251e5633840fe9445d6adb7c9756f",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_72b68b9f1043483aaef68a4d73cfad4c",
       "tabbable": null,
       "tooltip": null,
       "value": 4
      }
     },
     "2956e670f13541109aa1970b798649d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "29abd7ec70764feebe0ada8a24902185": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "29ca7f0b2b194bdeaf9742db64c0738b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a4684476dc44a84850b11a2b01b4bda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a5ea30c999d4c1180fefc5b3e9cb1bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b835aaa994a4169b74137a69ced13f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_be2625744d3a44fe8976ed138f11be05",
       "placeholder": "​",
       "style": "IPY_MODEL_0d9305238b4849939d8aba1f02314126",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.38M/3.38M [00:00&lt;00:00, 16.5MB/s]"
      }
     },
     "2b85fd277f28404591d7ce2a110272a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1163d7909b57437ea53b1ecfc221281a",
       "placeholder": "​",
       "style": "IPY_MODEL_7242ff81e5e946bc9138df0a2a472890",
       "tabbable": null,
       "tooltip": null,
       "value": " 15.3k/15.3k [00:00&lt;00:00, 1.56MB/s]"
      }
     },
     "2bdf499828a54aed9ab8420d02675697": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2f97e11f9a56492aa2b81c77c6511a34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_decf81f2c3ad42c7915496833cff07f1",
       "max": 4991123960,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e7b7d7162ea543e6a0487a5a327dd64f",
       "tabbable": null,
       "tooltip": null,
       "value": 4991123960
      }
     },
     "308fa5c760a14460b065d15a5d75e361": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "314764f9bbba4b9488fe3727a1dfda80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_79e0f1ed92ee4e61ab58c882b67dc1cc",
       "placeholder": "​",
       "style": "IPY_MODEL_41f6578d1a7f4b0eb9146492bbfc4996",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading shards: 100%"
      }
     },
     "3269d8d5ca514dbfbf39859cbeb405d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3339b4a5cd334959b515b9b202449db0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4f9345b25e8a44beaa039e42e85576d2",
       "max": 790,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_343da4e4e2334803a5db1856b3089ff8",
       "tabbable": null,
       "tooltip": null,
       "value": 790
      }
     },
     "33e95e55f024493986ee913a2fb26422": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_caa16423acc24cc1923dc9405b3902be",
        "IPY_MODEL_1db0ff5e2fb640759797b6f81feac95a",
        "IPY_MODEL_e7e540465f4244babf34273cc13531a5"
       ],
       "layout": "IPY_MODEL_053b7c5c14da491eacd565bbd6beec3e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "343da4e4e2334803a5db1856b3089ff8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "34e3628871854406a89a1f4b74b9ee7b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35391d53487042f5af1313f560cb2a5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "35a498db08654390a8153d16d1850cfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "36d65f546cc2495793f8a7073ebb58aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "38cb3dc6172848a1b9aecc9eefd23369": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3ff5a6fb962f42218f4782fad256b697",
       "placeholder": "​",
       "style": "IPY_MODEL_1a1a5a16c2dc4d7887d2d128133a581c",
       "tabbable": null,
       "tooltip": null,
       "value": " 613/613 [00:00&lt;00:00, 62.6kB/s]"
      }
     },
     "3ec088062cd245dd907b2001dc4b367b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f3d9f860a66415c8f18c318b88d2950": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ecf557353f5742a99d2325cfedc85469",
       "max": 8923,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2349ab1106d54e538bd9137de973d203",
       "tabbable": null,
       "tooltip": null,
       "value": 8923
      }
     },
     "3ff5a6fb962f42218f4782fad256b697": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40e398d4590344b5bf95716a055e0ed3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "41f6578d1a7f4b0eb9146492bbfc4996": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "42ff34fb40dd4380988934c465bc5a1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_014c3a4b19584933aeb7ca126ee4c4c6",
        "IPY_MODEL_ace911b1934a48b48311b907cd55153b",
        "IPY_MODEL_b7c3392763d34d60882228c520f8e0ab"
       ],
       "layout": "IPY_MODEL_bd3c5e931919423ca8a295e0ff4d44ad",
       "tabbable": null,
       "tooltip": null
      }
     },
     "46fa4afb8e504c82a230d5a1663a5c6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_98c2ee8b7417496c8321f1926581f1c8",
       "max": 69,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5aee90e3c99541bba0e1c5b8eed421c3",
       "tabbable": null,
       "tooltip": null,
       "value": 69
      }
     },
     "4985098b10b645aab3ddc2ed3b5729ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4b5d32c7b59540009d4c17d6177c8dcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_74a4677c63d645a4b396f08e05a2c8eb",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_53bd493e0aac4cda834084acdeb49f8b",
       "tabbable": null,
       "tooltip": null,
       "value": 4
      }
     },
     "4c9b92d3a4144b3686e4d2caf9fa9c16": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d34028d15f54d719e4d7831ba252ac3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_24b58b7daa914edb82d7f6485dc793e7",
        "IPY_MODEL_3f3d9f860a66415c8f18c318b88d2950",
        "IPY_MODEL_cbc90bba32534702a1258b8db2a990ff"
       ],
       "layout": "IPY_MODEL_b369651df20d4c44b0516b81fbfaa4a4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4d777793c39044c6b6b5c312c7ec654c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4d7957cb4ea64ea7b5bec47657fffad2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0ace2ebeeb747f1a9a93d6286eb951b",
       "placeholder": "​",
       "style": "IPY_MODEL_50466b43501c46b28542ee0146f32f9d",
       "tabbable": null,
       "tooltip": null,
       "value": " 790/790 [00:00&lt;00:00, 79.5kB/s]"
      }
     },
     "4f1374ef23ca423fb02352479f27bd61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4f9345b25e8a44beaa039e42e85576d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50466b43501c46b28542ee0146f32f9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "514f031e262f40dfb27fea0f963c5ad7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "51e6339a56ba4245903651bd965182c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53bd493e0aac4cda834084acdeb49f8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5a4168dce9f141f78155ef48cb69370f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0555e74c88624d23954bfc7c30b5f6b3",
       "placeholder": "​",
       "style": "IPY_MODEL_059395ad411d43c9ab5f9c418bedf184",
       "tabbable": null,
       "tooltip": null,
       "value": "configuration_intern_vit.py: 100%"
      }
     },
     "5aee90e3c99541bba0e1c5b8eed421c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5b41354b256945b0adca22fd86695719": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_514f031e262f40dfb27fea0f963c5ad7",
       "max": 4036,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_35a498db08654390a8153d16d1850cfa",
       "tabbable": null,
       "tooltip": null,
       "value": 4036
      }
     },
     "5bc3d775865b4dd3a7a452fa6daa6f2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e019dea9302405ab026c379c1f745e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c76fcbe437864397856edfb979700e88",
       "placeholder": "​",
       "style": "IPY_MODEL_b7d09052c8234aff90a8363328707fb6",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.json: 100%"
      }
     },
     "610c8092281a49f595523b21dbb3134c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "61d334b4f6d5480d9392ef3945905198": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "61f3f23d7fa74824a53571fb5e9ca0a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "640acc172db3430a8b86c70c37996315": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "658f4176aed64b73ad7bad46238980b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "66fccc531424429382d4b5b6d1a056dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_695a83a699814ab28fed70113f549c43",
       "placeholder": "​",
       "style": "IPY_MODEL_019d83f68efc445197fcc0827d1eaf46",
       "tabbable": null,
       "tooltip": null,
       "value": " 7.03M/7.03M [00:00&lt;00:00, 51.7MB/s]"
      }
     },
     "670ef8896b99404b96836767bd584d2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c676a938eb1648728b81b5bffd5da571",
       "placeholder": "​",
       "style": "IPY_MODEL_d3c1d868d5ce4ae89700641e97d2441a",
       "tabbable": null,
       "tooltip": null,
       "value": "configuration_internvl_chat.py: 100%"
      }
     },
     "68aacd9625d04fe3ba19de9e24af4222": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_07fb176f2e6a414aa5ec609dea51a71d",
        "IPY_MODEL_c2d7d42f020942b4ae1e00366535320b",
        "IPY_MODEL_fd57fbbc197746eda36c8a6586822922"
       ],
       "layout": "IPY_MODEL_c5f0636e53e24734b60dd0c2a578cd58",
       "tabbable": null,
       "tooltip": null
      }
     },
     "695a83a699814ab28fed70113f549c43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ba3d719b34e45d39c7f66285a06aa3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2a4684476dc44a84850b11a2b01b4bda",
       "max": 4958443072,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_de5612f03b9e476d90a95a3737312140",
       "tabbable": null,
       "tooltip": null,
       "value": 4958443072
      }
     },
     "6d1042d165544015ae87f5a261e7bf7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dc27a6bab2b48ab845702843c897d8f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e820190332f42e19a8948a223b22786": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "717b3daa9d4c40a085017f8429b7830c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7242ff81e5e946bc9138df0a2a472890": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "726cab6f4d134b31ab483cba5d1fda70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5bc3d775865b4dd3a7a452fa6daa6f2c",
       "placeholder": "​",
       "style": "IPY_MODEL_61d334b4f6d5480d9392ef3945905198",
       "tabbable": null,
       "tooltip": null,
       "value": " 16.0k/16.0k [00:00&lt;00:00, 1.68MB/s]"
      }
     },
     "72b68b9f1043483aaef68a4d73cfad4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "73b0bbd0934b43619be0f6c623d871fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73ca0283af6b4152bdbbe6ebe823349e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "74a4677c63d645a4b396f08e05a2c8eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7531fa2a9eab42fe81192fcdc22d32bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75ac0db568b94e91a64dc4cc8c66f163": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77f030a48815477498dbf29e490fdc0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "79bfbc69b9814f95b55f59d058a64082": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79e0f1ed92ee4e61ab58c882b67dc1cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7c5776e9a6d14c95a01766a5fcdf1247": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_670ef8896b99404b96836767bd584d2f",
        "IPY_MODEL_5b41354b256945b0adca22fd86695719",
        "IPY_MODEL_d9363c608e89417e8fb379eda93d62c4"
       ],
       "layout": "IPY_MODEL_131934edceaf440bb9ce6c9c8e53c09f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7e924eeb5e3f4e43b3f7af511680c39b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7f76be1b2f054f12859928a378b8a8ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8017ea2ed95f48c6ae662fe091ca2408": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_89e845bfa68144f3bb11c5b799d060e6",
       "placeholder": "​",
       "style": "IPY_MODEL_27de74025530497e877b2d2939492e79",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "805e5158257d4d59bf25697b4420c717": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81971e8263d54e68973a705e780508bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81aeba21df514e968376a406d3fc20f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "81dd81c53d3d4239aed58c1ba876f04d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "831eda78986d4b0cb5f4d67c5816a03f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_99c67872feec4ebd95632729bc1a6fbf",
       "max": 15309,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_20a9e55e0cd44d5ab1c7927740fcfa56",
       "tabbable": null,
       "tooltip": null,
       "value": 15309
      }
     },
     "83d2d1f2defe498eac2aaa23e2c46202": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "850775ea144d401f840db4dd32790d51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86a0d172dee14a989cb35d29e3ce0980": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_987b5956dd244b1bab6da6bc8cfbeb77",
        "IPY_MODEL_6ba3d719b34e45d39c7f66285a06aa3d",
        "IPY_MODEL_cfaba4ffa92c4136a9d0d0fe691cdbe1"
       ],
       "layout": "IPY_MODEL_34e3628871854406a89a1f4b74b9ee7b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8768b4b8266f4b8e8bc39ce733db1b35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "89df51d033d14fcc925e5283c9197060": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "89e845bfa68144f3bb11c5b799d060e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a84c10d2abe4df2877276f8148a6d23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8cb034a71d044eee8e6fa46242741f75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_75ac0db568b94e91a64dc4cc8c66f163",
       "max": 15971,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_efe82f6397494be9887f80bf586e4457",
       "tabbable": null,
       "tooltip": null,
       "value": 15971
      }
     },
     "8f34192fb1fd48e1ac9e7935fb2b5fff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_25b8ad6f8325470b8d2796bbdc3ab229",
        "IPY_MODEL_1914b0ba045b4adcbd0a70021f821ec8",
        "IPY_MODEL_e75ec54e659140d0bd42c0daf66d699b"
       ],
       "layout": "IPY_MODEL_d174ff1fbba641d1bd783aa3edba9bfd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "95d0cd8608a5461aaa5af4fb5031ff16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "95e371b01ab74bb7add9103101c46f41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "97077784731a41b3bcba8d0930d2bb19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "987b5956dd244b1bab6da6bc8cfbeb77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aa5fd7dbfbac41048bfb79f847f77f22",
       "placeholder": "​",
       "style": "IPY_MODEL_f506d83e8ed94e42b8ec74f7ce3812da",
       "tabbable": null,
       "tooltip": null,
       "value": "model-00002-of-00004.safetensors: 100%"
      }
     },
     "98c2ee8b7417496c8321f1926581f1c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99c67872feec4ebd95632729bc1a6fbf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9bfd1485e0ee4a0da1d1e098e8ce7b11": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9de73d244ece4159b25e6fbb2f2b2726": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3269d8d5ca514dbfbf39859cbeb405d9",
       "max": 5548,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a6ee8691822543de93e71f45cc7a2d06",
       "tabbable": null,
       "tooltip": null,
       "value": 5548
      }
     },
     "9e61f840313940859ab341aa4f65af03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9feb69489b744ae2ab80d99aa89513f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a08725f9077d42c187baa755c36ee446": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1373e8e6bdb44ec1846886807127df9d",
        "IPY_MODEL_831eda78986d4b0cb5f4d67c5816a03f",
        "IPY_MODEL_2b85fd277f28404591d7ce2a110272a7"
       ],
       "layout": "IPY_MODEL_25fcf94a493f4e5faeb789649ad8a907",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a0d4a3bcbee94952b617c8699734eec1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2956e670f13541109aa1970b798649d0",
       "placeholder": "​",
       "style": "IPY_MODEL_0084d093c08b427a887c6b364b9a8d03",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [00:58&lt;00:00, 11.03s/it]"
      }
     },
     "a1b62ac171bc4d69977548d0649e44f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1c582d039e54a0293b0943055374aa7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_04bb38e296ce4d22990c3a832fddb0f2",
       "placeholder": "​",
       "style": "IPY_MODEL_4d777793c39044c6b6b5c312c7ec654c",
       "tabbable": null,
       "tooltip": null,
       "value": "added_tokens.json: 100%"
      }
     },
     "a53e54beb78140938ead0dff7cf356a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6ca3a3bd0e249c2b4b72fa3c99556cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b670d2fb4194d9f93d036181d74798e",
       "placeholder": "​",
       "style": "IPY_MODEL_f69b79fb56d04e6d9f7d4a3ce4a08cd4",
       "tabbable": null,
       "tooltip": null,
       "value": " 5.55k/5.55k [00:00&lt;00:00, 542kB/s]"
      }
     },
     "a6ee8691822543de93e71f45cc7a2d06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a7edf79e229541d4935deea30d02f160": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5e019dea9302405ab026c379c1f745e1",
        "IPY_MODEL_25ca333da0aa4138944e07501771069d",
        "IPY_MODEL_2b835aaa994a4169b74137a69ced13f9"
       ],
       "layout": "IPY_MODEL_d1eadd60c70d4e5a95e7c40587626ffe",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a8232558702841ffa49cf020bd77c5fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a8ec4cefb69d40afa663576e36e23e6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a97be185cf6848bc91bd8a059d7b3b7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa5fd7dbfbac41048bfb79f847f77f22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab16db040b9d42c781fe9b8ea7d31395": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c4c87d8eb78442cc859ee9b0e3a0cb21",
        "IPY_MODEL_c686122fdb77433e8e2146873c612f30",
        "IPY_MODEL_b302849348c244b8818cdbf4fde3a9a8"
       ],
       "layout": "IPY_MODEL_eddc2927312d4adca751d5b7554d49b3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ace911b1934a48b48311b907cd55153b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_658f4176aed64b73ad7bad46238980b8",
       "max": 1671853,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0a80acbcf2424927a03b3ca9bc6183fe",
       "tabbable": null,
       "tooltip": null,
       "value": 1671853
      }
     },
     "ad7dc7978812436eb627f35fa425b2f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b431316ebf10400b9779cd6591a38abc",
        "IPY_MODEL_8cb034a71d044eee8e6fa46242741f75",
        "IPY_MODEL_726cab6f4d134b31ab483cba5d1fda70"
       ],
       "layout": "IPY_MODEL_4c9b92d3a4144b3686e4d2caf9fa9c16",
       "tabbable": null,
       "tooltip": null
      }
     },
     "adba5bb9cb754a298a21099be07aa3c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af3568b6ccce4f06a9cd10013280342a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1618736e2a9f40fe9676e78aa4eea9e8",
       "max": 7033297,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a8ec4cefb69d40afa663576e36e23e6c",
       "tabbable": null,
       "tooltip": null,
       "value": 7033297
      }
     },
     "af4ca20bacf84ead815ba753d11a3cee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_19a2a86b7f334aaf9c5e7fed9d8eea7f",
        "IPY_MODEL_46fa4afb8e504c82a230d5a1663a5c6e",
        "IPY_MODEL_f0d4f8eb439c4320affa9cbd087e9eb3"
       ],
       "layout": "IPY_MODEL_8a84c10d2abe4df2877276f8148a6d23",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b05ec61a33074f6693e31ae4325f6dfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a1c582d039e54a0293b0943055374aa7",
        "IPY_MODEL_3339b4a5cd334959b515b9b202449db0",
        "IPY_MODEL_4d7957cb4ea64ea7b5bec47657fffad2"
       ],
       "layout": "IPY_MODEL_a53e54beb78140938ead0dff7cf356a8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b302849348c244b8818cdbf4fde3a9a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2a5ea30c999d4c1180fefc5b3e9cb1bc",
       "placeholder": "​",
       "style": "IPY_MODEL_dde0b16578c1440a80ebb13c4ea24473",
       "tabbable": null,
       "tooltip": null,
       "value": " 62.4k/62.4k [00:00&lt;00:00, 5.44MB/s]"
      }
     },
     "b369651df20d4c44b0516b81fbfaa4a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b431316ebf10400b9779cd6591a38abc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_610c8092281a49f595523b21dbb3134c",
       "placeholder": "​",
       "style": "IPY_MODEL_0c179b9ae16b48ffaf76a6196bf97bb8",
       "tabbable": null,
       "tooltip": null,
       "value": "modeling_internvl_chat.py: 100%"
      }
     },
     "b7c3392763d34d60882228c520f8e0ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_29abd7ec70764feebe0ada8a24902185",
       "placeholder": "​",
       "style": "IPY_MODEL_cce7a5a3cb174ea88562d9114c1410a7",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.67M/1.67M [00:00&lt;00:00, 19.6MB/s]"
      }
     },
     "b7d09052c8234aff90a8363328707fb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bd3c5e931919423ca8a295e0ff4d44ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be2625744d3a44fe8976ed138f11be05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2d7d42f020942b4ae1e00366535320b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3ec088062cd245dd907b2001dc4b367b",
       "max": 4796984024,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_81dd81c53d3d4239aed58c1ba876f04d",
       "tabbable": null,
       "tooltip": null,
       "value": 4796984024
      }
     },
     "c4c87d8eb78442cc859ee9b0e3a0cb21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e5960dbf3c1445e0acd24966d3fbc386",
       "placeholder": "​",
       "style": "IPY_MODEL_81aeba21df514e968376a406d3fc20f7",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors.index.json: 100%"
      }
     },
     "c5f0636e53e24734b60dd0c2a578cd58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c676a938eb1648728b81b5bffd5da571": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c686122fdb77433e8e2146873c612f30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fbceb96950744a06930563f11557d47d",
       "max": 62420,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f6f65c4881e1493eabfe14fbf5f31567",
       "tabbable": null,
       "tooltip": null,
       "value": 62420
      }
     },
     "c76fcbe437864397856edfb979700e88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c7c8942586274105a361162a66b90004": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_61f3f23d7fa74824a53571fb5e9ca0a2",
       "max": 6329,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d6693bfd3bbe47c6a437e0b8ef5e28f9",
       "tabbable": null,
       "tooltip": null,
       "value": 6329
      }
     },
     "caa16423acc24cc1923dc9405b3902be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_73b0bbd0934b43619be0f6c623d871fc",
       "placeholder": "​",
       "style": "IPY_MODEL_ea8b8020fc6144cb9209d380eec9df9e",
       "tabbable": null,
       "tooltip": null,
       "value": "modeling_intern_vit.py: 100%"
      }
     },
     "cbc90bba32534702a1258b8db2a990ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6d1042d165544015ae87f5a261e7bf7d",
       "placeholder": "​",
       "style": "IPY_MODEL_4f1374ef23ca423fb02352479f27bd61",
       "tabbable": null,
       "tooltip": null,
       "value": " 8.92k/8.92k [00:00&lt;00:00, 936kB/s]"
      }
     },
     "cce7a5a3cb174ea88562d9114c1410a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cd3f829e383a4608b635942bb6fbd5b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1caf92f4c66146098421048a53a89136",
        "IPY_MODEL_28f1f31cb8fd4b82834e9e8dd6450337",
        "IPY_MODEL_a0d4a3bcbee94952b617c8699734eec1"
       ],
       "layout": "IPY_MODEL_056c888eb82b4c4ea0090e92586d82b3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cf36fa014b264d678fa792f6e6ce61fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cfaba4ffa92c4136a9d0d0fe691cdbe1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_717b3daa9d4c40a085017f8429b7830c",
       "placeholder": "​",
       "style": "IPY_MODEL_adba5bb9cb754a298a21099be07aa3c5",
       "tabbable": null,
       "tooltip": null,
       "value": " 4.96G/4.96G [00:23&lt;00:00, 263MB/s]"
      }
     },
     "d0ace2ebeeb747f1a9a93d6286eb951b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d139c6fbcf2d45e8919eb9bbb2be5a65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_40e398d4590344b5bf95716a055e0ed3",
       "placeholder": "​",
       "style": "IPY_MODEL_77f030a48815477498dbf29e490fdc0d",
       "tabbable": null,
       "tooltip": null,
       "value": "model-00001-of-00004.safetensors: 100%"
      }
     },
     "d174ff1fbba641d1bd783aa3edba9bfd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1eadd60c70d4e5a95e7c40587626ffe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3259d383fff489cb60d50d08a1b74af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d139c6fbcf2d45e8919eb9bbb2be5a65",
        "IPY_MODEL_2f97e11f9a56492aa2b81c77c6511a34",
        "IPY_MODEL_2637347f255e43c6a8ce9f2246c9db5d"
       ],
       "layout": "IPY_MODEL_a97be185cf6848bc91bd8a059d7b3b7d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d3c1d868d5ce4ae89700641e97d2441a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d5fff5b70b2a42ffb6d1e806de52082f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d6693bfd3bbe47c6a437e0b8ef5e28f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d695b0c72b0148138974aeb734864718": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d5fff5b70b2a42ffb6d1e806de52082f",
       "placeholder": "​",
       "style": "IPY_MODEL_e67904efd4ad47a8bd0e503d7ea966a7",
       "tabbable": null,
       "tooltip": null,
       "value": " 6.33k/6.33k [00:00&lt;00:00, 634kB/s]"
      }
     },
     "d9363c608e89417e8fb379eda93d62c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cf36fa014b264d678fa792f6e6ce61fe",
       "placeholder": "​",
       "style": "IPY_MODEL_73ca0283af6b4152bdbbe6ebe823349e",
       "tabbable": null,
       "tooltip": null,
       "value": " 4.04k/4.04k [00:00&lt;00:00, 433kB/s]"
      }
     },
     "db7082c0c5f649c1b9e42466fb4146d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dc611e9e3100460586352b43b5c50eca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dde0b16578c1440a80ebb13c4ea24473": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "de5612f03b9e476d90a95a3737312140": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "decf81f2c3ad42c7915496833cff07f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ded1ead4fec44a8ab2de6e64e49c93a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dfcd546a98ed401daf1e4c03b1a09f21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8017ea2ed95f48c6ae662fe091ca2408",
        "IPY_MODEL_c7c8942586274105a361162a66b90004",
        "IPY_MODEL_d695b0c72b0148138974aeb734864718"
       ],
       "layout": "IPY_MODEL_640acc172db3430a8b86c70c37996315",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e145d6daee58402d99ab49bc3db5ae1e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5960dbf3c1445e0acd24966d3fbc386": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e67904efd4ad47a8bd0e503d7ea966a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e75ec54e659140d0bd42c0daf66d699b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7531fa2a9eab42fe81192fcdc22d32bc",
       "placeholder": "​",
       "style": "IPY_MODEL_36d65f546cc2495793f8a7073ebb58aa",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.14G/1.14G [00:04&lt;00:00, 260MB/s]"
      }
     },
     "e7ab06f094864e508b35a9e2b760718b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7b7d7162ea543e6a0487a5a327dd64f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e7e540465f4244babf34273cc13531a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9e61f840313940859ab341aa4f65af03",
       "placeholder": "​",
       "style": "IPY_MODEL_35391d53487042f5af1313f560cb2a5b",
       "tabbable": null,
       "tooltip": null,
       "value": " 18.1k/18.1k [00:00&lt;00:00, 1.85MB/s]"
      }
     },
     "ea8b8020fc6144cb9209d380eec9df9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ec6cc47595da442085d6c564f5e4ec3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ecf557353f5742a99d2325cfedc85469": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed7393a4fd9d465fbd15daed921b85fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eddc2927312d4adca751d5b7554d49b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efe82f6397494be9887f80bf586e4457": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f0d4f8eb439c4320affa9cbd087e9eb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_51e6339a56ba4245903651bd965182c5",
       "placeholder": "​",
       "style": "IPY_MODEL_4985098b10b645aab3ddc2ed3b5729ed",
       "tabbable": null,
       "tooltip": null,
       "value": " 69.0/69.0 [00:00&lt;00:00, 7.93kB/s]"
      }
     },
     "f506d83e8ed94e42b8ec74f7ce3812da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f57ec0ed732048a2bf552dc1a7bb80ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f69b79fb56d04e6d9f7d4a3ce4a08cd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f6f65c4881e1493eabfe14fbf5f31567": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fbceb96950744a06930563f11557d47d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd4aa43421154e35ae940c50b91dbc09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e7ab06f094864e508b35a9e2b760718b",
       "placeholder": "​",
       "style": "IPY_MODEL_97077784731a41b3bcba8d0930d2bb19",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.json: 100%"
      }
     },
     "fd57fbbc197746eda36c8a6586822922": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e145d6daee58402d99ab49bc3db5ae1e",
       "placeholder": "​",
       "style": "IPY_MODEL_a8232558702841ffa49cf020bd77c5fb",
       "tabbable": null,
       "tooltip": null,
       "value": " 4.80G/4.80G [00:24&lt;00:00, 209MB/s]"
      }
     },
     "fdf9bc4398284a1bb6e596927ac48ab1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
