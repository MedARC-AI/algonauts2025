{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:56:23.194604Z",
     "iopub.status.busy": "2025-05-06T14:56:23.192961Z",
     "iopub.status.idle": "2025-05-06T14:56:27.765717Z",
     "shell.execute_reply": "2025-05-06T14:56:27.760123Z",
     "shell.execute_reply.started": "2025-05-06T14:56:23.194563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install nilearn wandb orbax-checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:56:27.76713Z",
     "iopub.status.busy": "2025-05-06T14:56:27.766894Z",
     "iopub.status.idle": "2025-05-06T14:56:31.888611Z",
     "shell.execute_reply": "2025-05-06T14:56:31.882394Z",
     "shell.execute_reply.started": "2025-05-06T14:56:27.767108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade \"orbax-checkpoint\" \"jax[tpu,cuda]\" \"flax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:56:31.890764Z",
     "iopub.status.busy": "2025-05-06T14:56:31.890495Z",
     "iopub.status.idle": "2025-05-06T14:56:37.930698Z",
     "shell.execute_reply": "2025-05-06T14:56:37.924965Z",
     "shell.execute_reply.started": "2025-05-06T14:56:31.890736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, List, Sequence\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import orbax.checkpoint as ocp\n",
    "import pickle\n",
    "import einops\n",
    "import wandb\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:56:37.932953Z",
     "iopub.status.busy": "2025-05-06T14:56:37.932725Z",
     "iopub.status.idle": "2025-05-06T14:56:37.943734Z",
     "shell.execute_reply": "2025-05-06T14:56:37.939495Z",
     "shell.execute_reply.started": "2025-05-06T14:56:37.932932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retrain = True # This indicates if we want to retrain the model using the eval detaset, or just use the already saved best checkpoint\n",
    "path_last_training = \"/kaggle/input/t-att-2\"\n",
    "root_data_dir = \"/kaggle/input/algonauts2025nsl\"\n",
    "\n",
    "config_model_path = os.path.join(path_last_training, \"model_config.json\")\n",
    "best_config_path = os.path.join(path_last_training, \"best_config.json\")\n",
    "training_config_path = os.path.join(path_last_training, \"training_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:56:37.94702Z",
     "iopub.status.busy": "2025-05-06T14:56:37.946788Z",
     "iopub.status.idle": "2025-05-06T15:00:51.437262Z",
     "shell.execute_reply": "2025-05-06T15:00:51.430719Z",
     "shell.execute_reply.started": "2025-05-06T14:56:37.946998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# creating the dataset\n",
    "\n",
    "def load_fmri(root_data_dir, subject):\n",
    "    \"\"\"\n",
    "    Load the fMRI responses for the selected subject.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_data_dir : str\n",
    "        Root data directory.\n",
    "    subject : int\n",
    "        Subject used to train and validate the encoding model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fmri : dict\n",
    "        Dictionary containing the  fMRI responses.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fmri = {}\n",
    "\n",
    "    ### Load the fMRI responses for Friends ###\n",
    "    # Data directory\n",
    "    fmri_file = f'sub-0{subject}_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5'\n",
    "    fmri_dir = os.path.join(root_data_dir, 'algonauts_2025.competitors',\n",
    "        'fmri', f'sub-0{subject}', 'func', fmri_file)\n",
    "    # Load the the fMRI responses\n",
    "    fmri_friends = h5py.File(fmri_dir, 'r')\n",
    "    for key, val in fmri_friends.items():\n",
    "        fmri[str(key[13:])] = val[:].astype(np.float32)\n",
    "    del fmri_friends\n",
    "\n",
    "    ### Load the fMRI responses for Movie10 ###\n",
    "    # Data directory\n",
    "    fmri_file = f'sub-0{subject}_task-movie10_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_bold.h5'\n",
    "    fmri_dir = os.path.join(root_data_dir, 'algonauts_2025.competitors',\n",
    "        'fmri', f'sub-0{subject}', 'func', fmri_file)\n",
    "    # Load the the fMRI responses\n",
    "    fmri_movie10 = h5py.File(fmri_dir, 'r')\n",
    "    for key, val in fmri_movie10.items():\n",
    "        fmri[key[13:]] = val[:].astype(np.float32)\n",
    "    del fmri_movie10\n",
    "    # Average the fMRI responses across the two repeats for 'figures'\n",
    "    keys_all = fmri.keys()\n",
    "    figures_splits = 12\n",
    "    for s in range(figures_splits):\n",
    "        movie = 'figures' + format(s+1, '02')\n",
    "        keys_movie = [rep for rep in keys_all if movie in rep]\n",
    "        fmri[movie] = ((fmri[keys_movie[0]] + fmri[keys_movie[1]]) / 2).astype(np.float32)\n",
    "        del fmri[keys_movie[0]]\n",
    "        del fmri[keys_movie[1]]\n",
    "    # Average the fMRI responses across the two repeats for 'life'\n",
    "    keys_all = fmri.keys()\n",
    "    life_splits = 5\n",
    "    for s in range(life_splits):\n",
    "        movie = 'life' + format(s+1, '02')\n",
    "        keys_movie = [rep for rep in keys_all if movie in rep]\n",
    "        fmri[movie] = ((fmri[keys_movie[0]] + fmri[keys_movie[1]]) / 2).astype(np.float32)\n",
    "        del fmri[keys_movie[0]]\n",
    "        del fmri[keys_movie[1]]\n",
    "\n",
    "    ### Output ###\n",
    "    return fmri\n",
    "\n",
    "def align_features_and_fmri_samples(features, fmri, excluded_samples_start,\n",
    "                                    excluded_samples_end, hrf_delay, stimulus_window, movies):\n",
    "    \"\"\"\n",
    "    Align the stimulus features with the fMRI response samples for the selected movies,\n",
    "    preallocating the output arrays to avoid duplicating memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : dict\n",
    "        Dictionary containing the stimulus features.\n",
    "    fmri : dict\n",
    "        Dictionary containing the fMRI responses.\n",
    "    excluded_samples_start : int\n",
    "        Number of initial fMRI samples to exclude.\n",
    "    excluded_samples_end : int\n",
    "        Number of trailing fMRI samples to exclude.\n",
    "    hrf_delay : int\n",
    "        The delay (in TRs) to account for the hemodynamic response.\n",
    "    stimulus_window : int\n",
    "        Number of consecutive stimulus chunks to use per fMRI sample.\n",
    "    movies : list\n",
    "        List of movie identifiers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aligned_features : np.ndarray, shape (total_samples, feature_dim)\n",
    "        The aligned stimulus feature vectors.\n",
    "    aligned_fmri : np.ndarray, shape (total_samples, fmri_dim)\n",
    "        The aligned fMRI response samples.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # First pass: compute the total number of fMRI samples and feature vector dimensionality.\n",
    "    total_samples = 0\n",
    "    feature_dim = None\n",
    "    fmri_dim = None  # assume all fmri splits have same number of features per sample\n",
    "\n",
    "    for movie in movies:\n",
    "        # Determine movie identifier\n",
    "        if movie.startswith('friends'):\n",
    "            movie_id = movie[8:]\n",
    "        elif movie.startswith('movie10'):\n",
    "            movie_id = movie[8:]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Get keys for all splits corresponding to this movie\n",
    "        movie_splits = [key for key in fmri if movie_id in key[:len(movie_id)]]\n",
    "        for split in movie_splits:\n",
    "            # Slice the fMRI data to exclude initial and final samples\n",
    "            fmri_split = fmri[split][excluded_samples_start:-excluded_samples_end]\n",
    "            n_samples = fmri_split.shape[0] if hasattr(fmri_split, \"shape\") else len(fmri_split)\n",
    "            total_samples += n_samples\n",
    "\n",
    "            # For the first split we encounter, determine the feature dimension.\n",
    "            if feature_dim is None:\n",
    "                sample_feature_dim = 0\n",
    "                for mod in features:\n",
    "                    if mod in ['visual', 'audio']:\n",
    "                        # Expecting features[mod][split] to be 2D: (n, d)\n",
    "                        d = features[mod][split].shape[1]\n",
    "                        sample_feature_dim += stimulus_window * d\n",
    "                    elif mod == 'language':\n",
    "                        # For language, each sample contributes its own feature vector (assumed to be 1D or 2D)\n",
    "                        if len(features[mod][split].shape) > 1:\n",
    "                            d = features[mod][split].shape[1]\n",
    "                        else:\n",
    "                            d = 1\n",
    "                        sample_feature_dim += d\n",
    "                feature_dim = sample_feature_dim\n",
    "\n",
    "            # Also determine the fMRI dimension (assume all fMRI splits have same dimensionality)\n",
    "            if fmri_dim is None:\n",
    "                fmri_dim = fmri_split.shape[1]\n",
    "\n",
    "    # Preallocate the final arrays.\n",
    "    aligned_features = np.empty((total_samples, feature_dim), dtype=np.float32)\n",
    "    aligned_fmri = np.empty((total_samples, fmri_dim), dtype=np.float32)\n",
    "\n",
    "    # Second pass: fill in the preallocated arrays.\n",
    "    current_index = 0\n",
    "    for movie in movies:\n",
    "        if movie.startswith('friends'):\n",
    "            movie_id = movie[8:]\n",
    "        elif movie.startswith('movie10'):\n",
    "            movie_id = movie[8:]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        movie_splits = [key for key in fmri if movie_id in key[:len(movie_id)]]\n",
    "        for split in tqdm(movie_splits, desc=f\"Processing {movie} splits\"):\n",
    "            # print(split)\n",
    "            fmri_split = fmri[split][excluded_samples_start:-excluded_samples_end]\n",
    "            n_samples = fmri_split.shape[0]\n",
    "            # Directly write the fMRI data into the preallocated array.\n",
    "            aligned_fmri[current_index:current_index+n_samples, :] = fmri_split\n",
    "\n",
    "            # For each fMRI sample, build its corresponding feature vector.\n",
    "            for s in range(n_samples):\n",
    "                parts = []  # list to collect modality-specific features\n",
    "                for mod in features:\n",
    "                    if mod in ['visual', 'audio']:\n",
    "                        if s < (stimulus_window + hrf_delay):\n",
    "                            idx_start = excluded_samples_start\n",
    "                            idx_end = idx_start + stimulus_window\n",
    "                        else:\n",
    "                            idx_start = s + excluded_samples_start - hrf_delay - stimulus_window + 1\n",
    "                            idx_end = idx_start + stimulus_window\n",
    "                        # Adjust indices if they exceed available samples.\n",
    "                        if idx_end > len(features[mod][split]):\n",
    "                            idx_end = len(features[mod][split])\n",
    "                            idx_start = idx_end - stimulus_window\n",
    "                        f = features[mod][split][idx_start:idx_end]\n",
    "                        parts.append(f.flatten())\n",
    "                    elif mod == 'language':\n",
    "                        if s < hrf_delay:\n",
    "                            idx = excluded_samples_start\n",
    "                        else:\n",
    "                            idx = s + excluded_samples_start - hrf_delay\n",
    "                        if idx >= (len(features[mod][split]) - hrf_delay):\n",
    "                            f = features[mod][split][-1, :]\n",
    "                        else:\n",
    "                            f = features[mod][split][idx]\n",
    "                        parts.append(f.flatten())\n",
    "                # Concatenate the modality parts into one feature vector.\n",
    "                aligned_features[current_index + s, :] = np.concatenate(parts)\n",
    "            current_index += n_samples\n",
    "\n",
    "    return aligned_features, aligned_fmri\n",
    "\n",
    "def load_stimulus_features(root_data_dirs: Union[str, List[str]], modality: str, layers: List[str] = None) -> dict:\n",
    "    features = {modality: {}}\n",
    "\n",
    "    # Ensure root_data_dirs is a list\n",
    "    if isinstance(root_data_dirs, str):\n",
    "        root_data_dirs = [root_data_dirs]\n",
    "\n",
    "    # Iterate over each root directory provided.\n",
    "    for root_data_dir in root_data_dirs:\n",
    "        root_path = Path(root_data_dir)\n",
    "        # Traverse all .h5 files under the current root directory.\n",
    "        for h5_file in tqdm(list(root_path.rglob(\"*.h5\")), desc=f\"Processing {root_data_dir}\"):\n",
    "            movie_name = h5_file.stem\n",
    "            # Remove the \"friends_\" prefix if it exists.\n",
    "            if movie_name.startswith(\"friends_\"):\n",
    "                movie_name = movie_name[len(\"friends_\"):]\n",
    "            \n",
    "            datasets = []\n",
    "            with h5py.File(h5_file, 'r') as f:\n",
    "                # Iterate over all datasets (layers) in the file.\n",
    "                for layer in f.keys():\n",
    "                    data = f[layer][:]\n",
    "                    if layers:\n",
    "                        if layer in layers:\n",
    "                            datasets.append(data)\n",
    "                    else:\n",
    "                        datasets.append(data)\n",
    "            \n",
    "            # If multiple layers exist, concatenate along axis=1.\n",
    "            if len(datasets) > 1:\n",
    "                datasets = [np.reshape(item, (item.shape[0], -1)) for item in datasets]\n",
    "                # print(datasets[0].shape, datasets[1].shape, datasets[2].shape)\n",
    "                concatenated_features = np.concatenate(datasets, axis=1)\n",
    "            elif datasets:\n",
    "                concatenated_features = datasets[0]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # If the same movie_name is encountered from multiple directories,\n",
    "            # you might choose to either overwrite or merge; here we overwrite.\n",
    "            features[modality][movie_name] = concatenated_features.reshape(concatenated_features.shape[0], -1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def print_keys_for_features(root_data_dirs: Union[str, List[str]]) -> List['str']:\n",
    "    layers = []\n",
    "    for root_data_dir in root_data_dirs:\n",
    "        root_path = Path(root_data_dir)\n",
    "        # Traverse all .h5 files under the current root directory.\n",
    "        for h5_file in list(root_path.rglob(\"*.h5\")):\n",
    "            movie_name = h5_file.stem\n",
    "            # Remove the \"friends_\" prefix if it exists.\n",
    "            if movie_name.startswith(\"friends_\"):\n",
    "                movie_name = movie_name[len(\"friends_\"):]\n",
    "\n",
    "            with h5py.File(h5_file, 'r') as f:\n",
    "                # Iterate over all datasets (layers) in the file.\n",
    "                for layer in f.keys():\n",
    "                    layers.append(layer)\n",
    "                print(layers)\n",
    "                return layers\n",
    "    \n",
    "# Load the model configuration\n",
    "with open(config_model_path, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "with open(best_config_path, 'r') as f:\n",
    "    best_config = json.load(f)\n",
    "\n",
    "# Load the training configuration\n",
    "with open(training_config_path, 'r') as f:\n",
    "    training_config = json.load(f)\n",
    "\n",
    "# Export variables as global\n",
    "for key, value in training_config.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "\n",
    "for key, value in best_config.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_features_dir = \"/kaggle/input/\"\n",
    "\n",
    "if retrain:    \n",
    "    # # These paths are exported from the training config\n",
    "    # video_features_name = 'internvl3-8b-8bit-features'\n",
    "    # audio_features_name = 'whisper-features'\n",
    "    # transcript_features_name = 'modernbert-features'\n",
    "\n",
    "\n",
    "    # --- Example usage ---\n",
    "\n",
    "    # Suppose the root_data_dir points to the directory containing all the .h5 files.\n",
    "    root_data_dirs_audio = [os.path.join(base_features_dir, audio_features_name)]  # adjust this path as needed\n",
    "    modality_audio = \"audio\"  # For example, we're loading audio features\n",
    "    print_keys_for_features(root_data_dirs_audio)\n",
    "    # layers_audio defined from the training config\n",
    "    features_audio = load_stimulus_features(root_data_dirs_audio, modality_audio, layers = layers_audio)\n",
    "\n",
    "    root_data_dirs_visual = [os.path.join(base_features_dir, video_features_name)]  # adjust this path as needed\n",
    "    modality_visual = \"visual\" \n",
    "    print_keys_for_features(root_data_dirs_visual)\n",
    "    # layers_visual defined from the training config\n",
    "    features_visual = load_stimulus_features(root_data_dirs_visual, modality_visual, layers = layers_visual)\n",
    "\n",
    "    root_data_dirs_transcript = [os.path.join(base_features_dir, transcript_features_name)] \n",
    "    modality_transcript = \"language\"  \n",
    "    print_keys_for_features(root_data_dirs_transcript)\n",
    "    # layers_transcript defined from the training config\n",
    "    features_transcript = load_stimulus_features(root_data_dirs_transcript, modality_transcript, layers = layers_transcript)\n",
    "\n",
    "\n",
    "    # Combine all the feature on a single variable\n",
    "    features = {modality_visual: features_visual[modality_visual], modality_audio: features_audio[modality_audio], modality_transcript: features_transcript[modality_transcript]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:00:51.43924Z",
     "iopub.status.busy": "2025-05-06T15:00:51.438989Z",
     "iopub.status.idle": "2025-05-06T15:00:51.456821Z",
     "shell.execute_reply": "2025-05-06T15:00:51.451785Z",
     "shell.execute_reply.started": "2025-05-06T15:00:51.439216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "class DatasetFeaturesFmri(Dataset):\n",
    "    def __init__(self, features: dict, modality: str, excluded_samples_start: int, excluded_samples_end: int, hrf_delay: int, stimulus_window: int, movies: list, root_data_dir: str, subject: int, return_concatenated_features: bool = False):\n",
    "        self.features = features\n",
    "        self.modality = modality\n",
    "        self.excluded_samples_start = excluded_samples_start\n",
    "        self.excluded_samples_end = excluded_samples_end\n",
    "        self.hrf_delay = hrf_delay\n",
    "        self.stimulus_window = stimulus_window\n",
    "        self.movies = movies\n",
    "        self.root_data_dir = root_data_dir\n",
    "        self.subject = subject\n",
    "        self.return_concatenated_features = return_concatenated_features\n",
    "\n",
    "        self.fmri = load_fmri(self.root_data_dir, self.subject)\n",
    "\n",
    "        self.visual_features_shape_length = features['visual']['s01e01a'][0].flatten().shape[0]\n",
    "        self.audio_features_shape_length = features['audio']['s01e01a'][0].flatten().shape[0]\n",
    "        self.language_features_shape_length = features['language']['s01e01a'][0].flatten().shape[0]\n",
    "\n",
    "        self.visual_features_shape_indexes = (0, self.visual_features_shape_length * self.stimulus_window)\n",
    "        self.audio_features_shape_indexes = (self.visual_features_shape_indexes[1], self.visual_features_shape_indexes[1] + self.audio_features_shape_length * self.stimulus_window)\n",
    "        self.language_features_shape_indexes = (self.audio_features_shape_indexes[1], self.audio_features_shape_indexes[1] + self.language_features_shape_length) # We're not using stimulus_window for language features\n",
    "\n",
    "        self.features, self.fmri = align_features_and_fmri_samples(self.features, self.fmri, self.excluded_samples_start, self.excluded_samples_end, self.hrf_delay, self.stimulus_window, self.movies)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.return_concatenated_features:\n",
    "            return self.features[index], self.fmri[index]\n",
    "        else:\n",
    "            video_features = self.features[index][self.visual_features_shape_indexes[0]:self.visual_features_shape_indexes[1]]\n",
    "            audio_features = self.features[index][self.audio_features_shape_indexes[0]:self.audio_features_shape_indexes[1]]\n",
    "            language_features = self.features[index][self.language_features_shape_indexes[0]:self.language_features_shape_indexes[1]]\n",
    "\n",
    "            video_features = einops.rearrange(video_features, '(w a) -> w a', w = self.stimulus_window)\n",
    "            audio_features = einops.rearrange(audio_features, '(w a) -> w a', w = self.stimulus_window)\n",
    "            \n",
    "\n",
    "            return video_features, audio_features, language_features, self.fmri[index]\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "    \n",
    "    def get_fmri(self):\n",
    "        return self.fmri\n",
    "    \n",
    "    def get_modality(self):\n",
    "        return self.modality\n",
    "    \n",
    "    def get_excluded_samples_start(self):\n",
    "        return self.excluded_samples_start\n",
    "    \n",
    "    def get_excluded_samples_end(self):\n",
    "        return self.excluded_samples_end\n",
    "    \n",
    "    def get_hrf_delay(self):\n",
    "        return self.hrf_delay\n",
    "    \n",
    "    def get_stimulus_window(self):\n",
    "        return self.stimulus_window\n",
    "    \n",
    "    def get_movies(self):\n",
    "        return self.movies\n",
    "    \n",
    "    def get_root_data_dir(self):\n",
    "        return self.root_data_dir\n",
    "    \n",
    "    def get_subject(self):\n",
    "        return self.subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:00:51.459791Z",
     "iopub.status.busy": "2025-05-06T15:00:51.459544Z",
     "iopub.status.idle": "2025-05-06T15:00:51.484361Z",
     "shell.execute_reply": "2025-05-06T15:00:51.479156Z",
     "shell.execute_reply.started": "2025-05-06T15:00:51.459761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetFeaturesFmriMultiSubject(Dataset):\n",
    "    def __init__(self, features: dict, modality: str, # Modality seems unused here?\n",
    "                 excluded_samples_start: int, excluded_samples_end: int,\n",
    "                 hrf_delay: int, stimulus_window: int, movies: list,\n",
    "                 root_data_dir: str, subjects: list[int],\n",
    "                 return_concatenated_features: bool = False,\n",
    "                 device: torch.device = torch.device('cpu')): # Add device option\n",
    "\n",
    "        # Store configuration\n",
    "        self.features_input_dict = features # Keep original dict if needed later\n",
    "        self.modality = modality\n",
    "        self.excluded_samples_start = excluded_samples_start\n",
    "        self.excluded_samples_end = excluded_samples_end\n",
    "        self.hrf_delay = hrf_delay\n",
    "        self.stimulus_window = stimulus_window\n",
    "        self.movies = movies\n",
    "        self.root_data_dir = root_data_dir\n",
    "        self.subjects = sorted(list(set(subjects))) # Store unique sorted subjects\n",
    "        self.return_concatenated_features = return_concatenated_features\n",
    "        self.device = device\n",
    "\n",
    "        # Calculate feature shapes based on provided example feature\n",
    "        # Assuming 's01e01a' exists and features are consistent across subjects\n",
    "        try:\n",
    "            self.visual_features_shape_length = features['visual']['s01e01a'][0].flatten().shape[0]\n",
    "            self.audio_features_shape_length = features['audio']['s01e01a'][0].flatten().shape[0]\n",
    "            self.language_features_shape_length = features['language']['s01e01a'][0].flatten().shape[0]\n",
    "        except KeyError as e:\n",
    "             raise ValueError(f\"Could not find example feature key 's01e01a' in provided features dict: {e}\")\n",
    "        except IndexError as e:\n",
    "             raise ValueError(f\"Example feature 's01e01a' seems empty or has wrong structure: {e}\")\n",
    "\n",
    "\n",
    "        self.visual_features_shape_indexes = (0, self.visual_features_shape_length * self.stimulus_window)\n",
    "        self.audio_features_shape_indexes = (self.visual_features_shape_indexes[1], self.visual_features_shape_indexes[1] + self.audio_features_shape_length * self.stimulus_window)\n",
    "        self.language_features_shape_indexes = (self.audio_features_shape_indexes[1], self.audio_features_shape_indexes[1] + self.language_features_shape_length) # No window for language\n",
    "\n",
    "        # --- Data Loading and Alignment per Subject ---\n",
    "        all_aligned_features = []\n",
    "        all_aligned_fmri = []\n",
    "        all_subject_ids = []\n",
    "        self._subject_indices_map = {} # Store indices range for each subject\n",
    "\n",
    "        print(f\"Initializing Dataset for subjects: {self.subjects}\")\n",
    "        current_index = 0\n",
    "        for subj_id in self.subjects:\n",
    "            print(f\"\\nProcessing Subject {subj_id}...\")\n",
    "            fmri_subj = load_fmri(self.root_data_dir, subj_id)\n",
    "\n",
    "            # Align features and fMRI for this specific subject\n",
    "            features_aligned_subj, fmri_aligned_subj = align_features_and_fmri_samples(\n",
    "                self.features_input_dict, # Pass the original feature dict\n",
    "                fmri_subj,\n",
    "                self.excluded_samples_start,\n",
    "                self.excluded_samples_end,\n",
    "                self.hrf_delay,\n",
    "                self.stimulus_window,\n",
    "                self.movies\n",
    "            )\n",
    "\n",
    "            num_samples_subj = len(fmri_aligned_subj)\n",
    "            if num_samples_subj > 0:\n",
    "                all_aligned_features.append(features_aligned_subj)\n",
    "                all_aligned_fmri.append(fmri_aligned_subj)\n",
    "                subject_ids_subj = np.full(num_samples_subj, subj_id, dtype=np.int32)\n",
    "                all_subject_ids.append(subject_ids_subj)\n",
    "\n",
    "                # Store the index range for this subject\n",
    "                self._subject_indices_map[subj_id] = (current_index, current_index + num_samples_subj)\n",
    "                current_index += num_samples_subj\n",
    "            else:\n",
    "                 print(f\"Warning: No aligned samples found for subject {subj_id}. Skipping.\")\n",
    "                 self._subject_indices_map[subj_id] = (current_index, current_index) # Empty range\n",
    "\n",
    "\n",
    "        if not all_aligned_features:\n",
    "             raise ValueError(\"No data loaded for any subject. Check alignment parameters or data paths.\")\n",
    "\n",
    "        # Concatenate data from all subjects\n",
    "        # Using torch.from_numpy().to(self.device) for direct tensor creation\n",
    "        self.features = torch.from_numpy(np.concatenate(all_aligned_features, axis=0)).float().to(self.device)\n",
    "        self.fmri = torch.from_numpy(np.concatenate(all_aligned_fmri, axis=0)).float().to(self.device)\n",
    "        self.subject_ids = torch.from_numpy(np.concatenate(all_subject_ids, axis=0)).long().to(self.device) # Use long for IDs\n",
    "\n",
    "        print(f\"\\nDataset Initialized. Total samples: {len(self.fmri)}\")\n",
    "        print(f\"Feature tensor shape: {self.features.shape}\")\n",
    "        print(f\"fMRI tensor shape: {self.fmri.shape}\")\n",
    "        print(f\"Subject ID tensor shape: {self.subject_ids.shape}\")\n",
    "        print(f\"Subject index map: {self._subject_indices_map}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features_sample = self.features[index]\n",
    "        fmri_sample = self.fmri[index]\n",
    "        subject_id = self.subject_ids[index] # Get subject ID for this sample\n",
    "\n",
    "        if self.return_concatenated_features:\n",
    "            # Return concatenated features, fmri, and subject_id\n",
    "            return features_sample, fmri_sample, subject_id\n",
    "        else:\n",
    "            # Split features into modalities\n",
    "            video_features = features_sample[self.visual_features_shape_indexes[0]:self.visual_features_shape_indexes[1]]\n",
    "            audio_features = features_sample[self.audio_features_shape_indexes[0]:self.audio_features_shape_indexes[1]]\n",
    "            language_features = features_sample[self.language_features_shape_indexes[0]:self.language_features_shape_indexes[1]]\n",
    "\n",
    "            # Reshape features with windows (if dimensions allow)\n",
    "            # Use try-except for robustness if a feature type might be missing/empty\n",
    "            try:\n",
    "                video_features = einops.rearrange(video_features, '(w d) -> w d', w=self.stimulus_window)\n",
    "            except Exception as e:\n",
    "                 # Handle cases where reshape might fail (e.g., length 0 or not divisible)\n",
    "                 # print(f\"Warning: Could not reshape video features at index {index}. Error: {e}\")\n",
    "                 # Decide how to handle: return as is, return zeros, raise error?\n",
    "                 # Returning as flat tensor for now if reshape fails\n",
    "                 pass # video_features remains flat\n",
    "\n",
    "            try:\n",
    "                audio_features = einops.rearrange(audio_features, '(w d) -> w d', w=self.stimulus_window)\n",
    "            except Exception as e:\n",
    "                # print(f\"Warning: Could not reshape audio features at index {index}. Error: {e}\")\n",
    "                 pass # audio_features remains flat\n",
    "\n",
    "\n",
    "            # language_features are returned as is (no window assumed)\n",
    "\n",
    "            # Return split features, fmri, and subject_id\n",
    "            return video_features, audio_features, language_features, fmri_sample, subject_id\n",
    "\n",
    "    def get_subject_indices(self, subject_id: int) -> list[int]:\n",
    "        \"\"\"Returns a list of dataset indices corresponding to the given subject_id.\"\"\"\n",
    "        if subject_id not in self._subject_indices_map:\n",
    "            print(f\"Warning: Subject ID {subject_id} not found in this dataset.\")\n",
    "            return []\n",
    "        start, end = self._subject_indices_map[subject_id]\n",
    "        return list(range(start, end))\n",
    "\n",
    "    # --- Keep other getter methods as needed ---\n",
    "    def get_features_shape_lengths(self):\n",
    "        return {\n",
    "            'visual': self.visual_features_shape_length,\n",
    "            'audio': self.audio_features_shape_length,\n",
    "            'language': self.language_features_shape_length\n",
    "            }\n",
    "\n",
    "    def get_subjects(self) -> list[int]:\n",
    "        \"\"\"Returns the list of subjects included in this dataset.\"\"\"\n",
    "        return self.subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:00:51.486832Z",
     "iopub.status.busy": "2025-05-06T15:00:51.486493Z",
     "iopub.status.idle": "2025-05-06T15:00:51.512662Z",
     "shell.execute_reply": "2025-05-06T15:00:51.507977Z",
     "shell.execute_reply.started": "2025-05-06T15:00:51.486807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FmriDatasetSingleSubject(Dataset):\n",
    "    def __init__(self,\n",
    "                 features: dict,\n",
    "                 modality: str,\n",
    "                 excluded_samples_start: int,\n",
    "                 excluded_samples_end: int,\n",
    "                 hrf_delay: int,\n",
    "                 stimulus_window: int,\n",
    "                 movies: list,\n",
    "                 root_data_dir: str,\n",
    "                 subject: int,\n",
    "                 return_concatenated_features: bool = False):\n",
    "        super().__init__()\n",
    "        # ------------- your code exactly as before -----------------------------\n",
    "        self.features = features\n",
    "        self.modality = modality\n",
    "        self.excluded_samples_start = excluded_samples_start\n",
    "        self.excluded_samples_end = excluded_samples_end\n",
    "        self.hrf_delay = hrf_delay\n",
    "        self.stimulus_window = stimulus_window\n",
    "        self.movies = movies\n",
    "        self.root_data_dir = root_data_dir\n",
    "        self.subject = subject\n",
    "        self.return_concatenated_features = return_concatenated_features\n",
    "\n",
    "        self.fmri = load_fmri(self.root_data_dir, self.subject)\n",
    "\n",
    "        self.visual_features_shape_length  = (\n",
    "            features['visual']['s01e01a'][0].flatten().shape[0]\n",
    "        )\n",
    "        self.audio_features_shape_length   = (\n",
    "            features['audio']['s01e01a'][0].flatten().shape[0]\n",
    "        )\n",
    "        self.language_features_shape_length = (\n",
    "            features['language']['s01e01a'][0].flatten().shape[0]\n",
    "        )\n",
    "\n",
    "        self.visual_features_shape_indexes = (\n",
    "            0,\n",
    "            self.visual_features_shape_length * self.stimulus_window,\n",
    "        )\n",
    "        self.audio_features_shape_indexes  = (\n",
    "            self.visual_features_shape_indexes[1],\n",
    "            self.visual_features_shape_indexes[1]\n",
    "            + self.audio_features_shape_length * self.stimulus_window,\n",
    "        )\n",
    "        self.language_features_shape_indexes = (\n",
    "            self.audio_features_shape_indexes[1],\n",
    "            self.audio_features_shape_indexes[1]\n",
    "            + self.language_features_shape_length,  # no window for language\n",
    "        )\n",
    "\n",
    "        self.features, self.fmri = align_features_and_fmri_samples(\n",
    "            self.features,\n",
    "            self.fmri,\n",
    "            self.excluded_samples_start,\n",
    "            self.excluded_samples_end,\n",
    "            self.hrf_delay,\n",
    "            self.stimulus_window,\n",
    "            self.movies,\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if self.return_concatenated_features:\n",
    "            sample = (self.features[idx], self.fmri[idx])\n",
    "        else:\n",
    "            video_features = self.features[idx][\n",
    "                self.visual_features_shape_indexes[0]:\n",
    "                self.visual_features_shape_indexes[1]\n",
    "            ]\n",
    "            audio_features = self.features[idx][\n",
    "                self.audio_features_shape_indexes[0]:\n",
    "                self.audio_features_shape_indexes[1]\n",
    "            ]\n",
    "            language_features = self.features[idx][\n",
    "                self.language_features_shape_indexes[0]:\n",
    "                self.language_features_shape_indexes[1]\n",
    "            ]\n",
    "\n",
    "            # reshape so that time window dimension is explicit\n",
    "            video_features   = einops.rearrange(\n",
    "                video_features, '(w a) -> w a', w=self.stimulus_window\n",
    "            )\n",
    "            audio_features   = einops.rearrange(\n",
    "                audio_features, '(w a) -> w a', w=self.stimulus_window\n",
    "            )\n",
    "            sample = (video_features, audio_features, language_features,\n",
    "                      self.fmri[idx])\n",
    "\n",
    "        # we **add** the subject id so the outer dataset can expose it\n",
    "        return (*sample, self.subject)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  Multi-subject dataset  (thin wrapper around N single-subject datasets)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from itertools import accumulate\n",
    "from bisect import bisect_right\n",
    "\n",
    "class FmriDatasetMultiSubject(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:  (*your_original_sample, subject_id)\n",
    "    \"\"\"\n",
    "    def __init__(self, *, subjects: list[int] = (1, 2, 3, 5), **single_ds_kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        subjects : list[int]\n",
    "            Which subject IDs to include.\n",
    "        single_ds_kwargs :\n",
    "            All the keyword arguments that a single-subject dataset expects,\n",
    "            **except** 'subject'. They are forwarded internally.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.datasets = [\n",
    "            FmriDatasetSingleSubject(subject=sid, **single_ds_kwargs)\n",
    "            for sid in subjects\n",
    "        ]\n",
    "        # pre-compute lengths so we can map a global index to (dataset, local_idx)\n",
    "        self.cum_sizes = list(accumulate(len(ds) for ds in self.datasets))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def __len__(self) -> int:\n",
    "        return self.cum_sizes[-1]\n",
    "\n",
    "    def __getitem__(self, global_idx: int):\n",
    "        # find which underlying dataset this index falls into\n",
    "        ds_idx = bisect_right(self.cum_sizes, global_idx)\n",
    "        prev_cum = 0 if ds_idx == 0 else self.cum_sizes[ds_idx - 1]\n",
    "        local_idx = global_idx - prev_cum\n",
    "        return self.datasets[ds_idx][local_idx]  # sample already carries subject\n",
    "\n",
    "    # convenience so you can pull out a single-subject dataset later\n",
    "    def get_subject_dataset(self, subject_id: int) -> Dataset:\n",
    "        for ds in self.datasets:\n",
    "            if ds.subject == subject_id:\n",
    "                return ds\n",
    "        raise KeyError(f\"Subject {subject_id} not in this dataset.\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  Collate function (stacks tensors, leaves scalars/ints alone)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def fmri_collate(batch):\n",
    "    \"\"\"\n",
    "    batch = list[tuple(\n",
    "        video_features, audio_features, language_features,\n",
    "        fmri, subject_id\n",
    "    )]\n",
    "    \"\"\"\n",
    "    # everything except 'subject_id' is a tensor you want stacked\n",
    "    *data, subjects = zip(*batch)\n",
    "    data = default_collate(list(zip(*data)))  # stack each field\n",
    "    subjects = torch.tensor(subjects)         # shape [batch]\n",
    "    return (*data, subjects)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  Flexible DataLoader factory\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def make_dataloader(dataset: FmriDatasetMultiSubject,\n",
    "                    *,\n",
    "                    policy: str = \"mixed_shuffle\",\n",
    "                    subject_id: int | None = None,\n",
    "                    batch_size: int = 32,\n",
    "                    num_workers: int = 0,\n",
    "                    pin_memory: bool = True) -> DataLoader:\n",
    "    \"\"\"\n",
    "    policy ∈ {\"mixed_shuffle\", \"mixed_sorted\",\n",
    "              \"subject_shuffle\", \"subject_sorted\"}\n",
    "    subject_id must be given for the two \"subject_*\" policies.\n",
    "    \"\"\"\n",
    "    if policy.startswith(\"subject\") and subject_id is None:\n",
    "        raise ValueError(\"You must supply subject_id for a subject-only loader.\")\n",
    "\n",
    "    if policy == \"mixed_shuffle\":\n",
    "        sampler = RandomSampler(dataset)\n",
    "    elif policy == \"mixed_sorted\":\n",
    "        sampler = SequentialSampler(dataset)\n",
    "    elif policy == \"subject_shuffle\":\n",
    "        sub_ds  = dataset.get_subject_dataset(subject_id)\n",
    "        sampler = RandomSampler(sub_ds)\n",
    "        dataset = sub_ds\n",
    "    elif policy == \"subject_sorted\":\n",
    "        sub_ds  = dataset.get_subject_dataset(subject_id)\n",
    "        sampler = SequentialSampler(sub_ds)\n",
    "        dataset = sub_ds\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown policy '{policy}'\")\n",
    "\n",
    "    return DataLoader(dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      sampler=sampler,\n",
    "                      collate_fn=fmri_collate,\n",
    "                      num_workers=num_workers,\n",
    "                      pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:00:51.51371Z",
     "iopub.status.busy": "2025-05-06T15:00:51.513494Z",
     "iopub.status.idle": "2025-05-06T15:00:51.524586Z",
     "shell.execute_reply": "2025-05-06T15:00:51.51998Z",
     "shell.execute_reply.started": "2025-05-06T15:00:51.51369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    # # These variables should be exported from the training_config.pkl file\n",
    "    # subject = 1\n",
    "    # multi_subject = True\n",
    "    # subjects = [1,2,3,5]\n",
    "    # modality = 'all'\n",
    "    # excluded_samples_start = 5\n",
    "    # excluded_samples_end = 5\n",
    "    # hrf_delay = 3\n",
    "    # stimulus_window = 6\n",
    "\n",
    "    movies_train = [\"friends-s01\", \"friends-s02\", \"friends-s03\", \"friends-s04\", \"friends-s05\", \"movie10-bourne\", \"movie10-figures\", \"movie10-wolf\", \"movie10-life\", \"friends-s06\"]\n",
    "    movies_val = [\"friends-s07\"]\n",
    "\n",
    "    root_data_dir = '/kaggle/input/algonauts2025nsl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:00:51.527225Z",
     "iopub.status.busy": "2025-05-06T15:00:51.526981Z",
     "iopub.status.idle": "2025-05-06T15:05:53.991381Z",
     "shell.execute_reply": "2025-05-06T15:05:53.981399Z",
     "shell.execute_reply.started": "2025-05-06T15:00:51.527201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    multi_ds = FmriDatasetMultiSubject(\n",
    "        features=features,\n",
    "        modality='all',\n",
    "        excluded_samples_start=excluded_samples_start,\n",
    "        excluded_samples_end=excluded_samples_end,\n",
    "        hrf_delay=hrf_delay,\n",
    "        stimulus_window=stimulus_window,\n",
    "        movies=movies_train,\n",
    "        root_data_dir=root_data_dir,\n",
    "        subjects=subjects,\n",
    "        return_concatenated_features=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:05:53.993138Z",
     "iopub.status.busy": "2025-05-06T15:05:53.992878Z",
     "iopub.status.idle": "2025-05-06T15:05:56.483986Z",
     "shell.execute_reply": "2025-05-06T15:05:56.476812Z",
     "shell.execute_reply.started": "2025-05-06T15:05:53.993114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    batch_size = 256\n",
    "    # All subjects, fully intermixed\n",
    "    train_dataloader = make_dataloader(multi_ds,\n",
    "                                policy=\"mixed_shuffle\",\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        video_features, audio_features, language_features, fmri, subject_ids = batch\n",
    "        print(video_features.shape, audio_features.shape, language_features.shape, fmri.shape, subject_ids.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:05:56.486284Z",
     "iopub.status.busy": "2025-05-06T15:05:56.485601Z",
     "iopub.status.idle": "2025-05-06T15:05:59.933933Z",
     "shell.execute_reply": "2025-05-06T15:05:59.928073Z",
     "shell.execute_reply.started": "2025-05-06T15:05:56.486255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nnx.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_size: int,\n",
    "        mlp_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout_rate: float = 0.0,\n",
    "        *,\n",
    "        rngs: nnx.Rngs = nnx.Rngs(0),\n",
    "    ) -> None:\n",
    "        self.norm1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n",
    "        self.attn = nnx.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            in_features=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            broadcast_dropout=False,\n",
    "            decode=False,\n",
    "            deterministic=False,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n",
    "\n",
    "        self.mlp = nnx.Sequential(\n",
    "            nnx.Linear(hidden_size, mlp_dim, rngs=rngs),\n",
    "            jax.nn.gelu,\n",
    "            nnx.Dropout(rate=dropout_rate, rngs=rngs),\n",
    "            nnx.Linear(mlp_dim, hidden_size, rngs=rngs),\n",
    "            nnx.Dropout(rate=dropout_rate, rngs=rngs),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:05:59.937056Z",
     "iopub.status.busy": "2025-05-06T15:05:59.936767Z",
     "iopub.status.idle": "2025-05-06T15:05:59.962381Z",
     "shell.execute_reply": "2025-05-06T15:05:59.957588Z",
     "shell.execute_reply.started": "2025-05-06T15:05:59.937028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiModalTransformer(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        subjects: Sequence[int],\n",
    "        stimuli_window: int = 5,\n",
    "        visual_dim: int = 1000,\n",
    "        audio_dim: int = 1000,\n",
    "        text_dim: int = 1000,\n",
    "        hidden_size: int = 768,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_dim: int = 3072,\n",
    "        dropout_rate: float = 0.1,\n",
    "        *,\n",
    "        rngs: nnx.Rngs = nnx.Rngs(0),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- shared backbone ----------\n",
    "        self.stimuli_window = stimuli_window\n",
    "        self.num_modalities = 3\n",
    "        self.total_tokens   = self.stimuli_window * self.num_modalities\n",
    "\n",
    "        self.visual_proj = nnx.Linear(visual_dim, hidden_size, rngs=rngs)\n",
    "        self.audio_proj  = nnx.Linear(audio_dim,  hidden_size, rngs=rngs)\n",
    "        self.text_proj   = nnx.Linear(text_dim,   hidden_size, rngs=rngs)\n",
    "\n",
    "        t_init = jax.nn.initializers.truncated_normal(stddev=0.02)\n",
    "        self.positional_embeddings = nnx.Param(\n",
    "            t_init(rngs.params(), (1, self.total_tokens + 1, hidden_size), jnp.float32)\n",
    "        )\n",
    "        self.dropout   = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
    "        self.cls_token = nnx.Param(jnp.zeros((1, 1, hidden_size), jnp.float32))\n",
    "\n",
    "        self.encoder = nnx.Sequential(*[\n",
    "            TransformerEncoder(\n",
    "                hidden_size  = hidden_size,\n",
    "                mlp_dim      = mlp_dim,\n",
    "                num_heads    = num_heads,\n",
    "                dropout_rate = dropout_rate,\n",
    "                rngs         = rngs,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = nnx.LayerNorm(hidden_size, rngs=rngs)\n",
    "\n",
    "        # ---------- subject-specific classifier bank ----------\n",
    "        self.subject_ids = tuple(int(s) for s in subjects)\n",
    "        S = len(subjects)\n",
    "\n",
    "        k_init = jax.nn.initializers.lecun_normal()\n",
    "        self.W = nnx.Param(k_init(rngs.params(), (S, hidden_size, num_classes),\n",
    "                                  jnp.float32))                   # [S,H,C]\n",
    "        self.b = nnx.Param(jnp.zeros((S, num_classes), jnp.float32))  # [S,C]\n",
    "\n",
    "    # NEW signature: subject_ids is mandatory\n",
    "    def __call__(\n",
    "        self,\n",
    "        x_visual: jax.Array,\n",
    "        x_audio:  jax.Array,\n",
    "        x_text:   jax.Array,\n",
    "        subject_ids: jax.Array,         # shape [B]\n",
    "        *,\n",
    "        return_last_hidden_state: bool = False,\n",
    "    ) -> jax.Array:\n",
    "\n",
    "        # 1. backbone forward\n",
    "        v   = self.visual_proj(x_visual)\n",
    "        a   = self.audio_proj(x_audio)\n",
    "        t   = self.text_proj(x_text)\n",
    "        tok = jnp.concatenate([v, a, t], axis=1)                  # [B,T*3,H]\n",
    "\n",
    "        B = tok.shape[0]\n",
    "        cls = jnp.tile(self.cls_token, [B, 1, 1])                 # [B,1,H]\n",
    "        tok = jnp.concatenate([cls, tok], axis=1)                 # prepend\n",
    "        tok = self.dropout(tok + self.positional_embeddings)\n",
    "\n",
    "        enc = self.encoder(tok)\n",
    "        cls_out = self.final_norm(enc)[:, 0]                      # [B,H]\n",
    "\n",
    "        # 2. vectorised subject-specific head\n",
    "        subj_bank = jnp.asarray(self.subject_ids, dtype=jnp.int32)  # [S]\n",
    "        idx = jnp.argmax((subject_ids[:, None] == subj_bank[None, :]),\n",
    "                         axis=1)\n",
    "        W_sel = self.W[idx]                                        # [B,H,C]\n",
    "        b_sel = self.b[idx]                                        # [B,C]\n",
    "        logits = jnp.einsum('bh,bhc->bc', cls_out, W_sel) + b_sel  # [B,C]\n",
    "\n",
    "        if return_last_hidden_state:\n",
    "            return cls_out, logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:05:59.964902Z",
     "iopub.status.busy": "2025-05-06T15:05:59.964621Z",
     "iopub.status.idle": "2025-05-06T15:06:43.356962Z",
     "shell.execute_reply": "2025-05-06T15:06:43.349508Z",
     "shell.execute_reply.started": "2025-05-06T15:05:59.964877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "# model = VisionTransformer(num_classes=1000, in_channels=3, img_size=224, patch_size=16, num_layers=12, num_heads=12, mlp_dim=3072, hidden_size=768, dropout_rate=0.1)\n",
    "# x = jnp.ones((1, 224, 224, 3))\n",
    "# logits = model(x)\n",
    "\n",
    "# model_config = {\n",
    "#     'num_classes': 1000,\n",
    "#     'subjects': subjects,\n",
    "#     'stimuli_window': stimulus_window,\n",
    "#     'visual_dim': multi_ds.datasets[0].visual_features_shape_length,\n",
    "#     'audio_dim': multi_ds.datasets[0].audio_features_shape_length,\n",
    "#     'text_dim': multi_ds.datasets[0].language_features_shape_length,\n",
    "#     'num_layers': 8,\n",
    "#     'num_heads': 16,\n",
    "#     'mlp_dim': 3072,\n",
    "#     'hidden_size': 2048,\n",
    "#     'dropout_rate': 0.10,\n",
    "# }\n",
    "\n",
    "\n",
    "save_ckpt = True\n",
    "save_ckpt_path = './checkpoints'\n",
    "##########\n",
    "\n",
    "\n",
    "model = MultiModalTransformer(**model_config)\n",
    "\n",
    "def save_ckpt(tag, model, path='./checkpoints', display = False):\n",
    "    # if path exists/tag exists, remove it\n",
    "    if os.path.exists(os.path.join(path, tag)):\n",
    "        shutil.rmtree(os.path.join(path, tag))\n",
    "\n",
    "    _, state = nnx.split(model)\n",
    "    pure_dict_state = nnx.to_pure_dict(state)\n",
    "    if display:\n",
    "        nnx.display(pure_dict_state)\n",
    "    checkpointer.save(ckpt_dir / tag, pure_dict_state)\n",
    "    \n",
    "if retrain:\n",
    "    model_config_json = json.dumps(model_config)\n",
    "    with open('model_config.json', 'w') as f:\n",
    "        f.write(model_config_json)\n",
    "        \n",
    "    if save_ckpt:\n",
    "        ckpt_dir = ocp.test_utils.erase_and_create_empty(os.path.abspath(save_ckpt_path))\n",
    "        checkpointer = ocp.StandardCheckpointer()\n",
    "        save_ckpt('last', model, display = True)\n",
    "\n",
    "\n",
    "    x_visual = video_features[0:5]\n",
    "    x_audio = audio_features[0:5]\n",
    "    x_text = language_features[0:5].unsqueeze(1).repeat(1, stimulus_window, 1)\n",
    "    x_ids = subject_ids[0:5]\n",
    "\n",
    "    logits = model(jnp.array(x_visual), jnp.array(x_audio), jnp.array(x_text), jnp.array(x_ids))\n",
    "    print(\"Output shape: \", logits.shape)\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        # Delete the model to free up memory\n",
    "        del model\n",
    "\n",
    "        # Load the checkpoint\n",
    "        checkpointer = ocp.StandardCheckpointer()\n",
    "        restored_pure_dict = checkpointer.restore(os.path.join(path_last_training, 'checkpoints', 'last')) # /kaggle/input/t-att-2/checkpoints/best.orbax-checkpoint-tmp-40/_CHECKPOINT_METADATA\n",
    "        abstract_model = nnx.eval_shape(lambda: MultiModalTransformer(**model_config, rngs=nnx.Rngs(0)))\n",
    "        graphdef, abstract_state = nnx.split(abstract_model)\n",
    "        nnx.replace_by_pure_dict(abstract_state, restored_pure_dict)\n",
    "        model = nnx.merge(graphdef, abstract_state)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:43.359751Z",
     "iopub.status.busy": "2025-05-06T15:06:43.359493Z",
     "iopub.status.idle": "2025-05-06T15:06:43.37751Z",
     "shell.execute_reply": "2025-05-06T15:06:43.373714Z",
     "shell.execute_reply.started": "2025-05-06T15:06:43.359726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_model_parameters(model: nnx.Module):\n",
    "    params = nnx.state(model, nnx.Param)\n",
    "    total_params = sum((np.prod(x.shape) for x in jax.tree.leaves(params)), 0)\n",
    "    print(\"Parameters on the model: \", total_params)\n",
    "    return total_params\n",
    "total_model_parameters = count_model_parameters(model) # Parameters on the model:  415478760 # Parameters on the model:  415491048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:43.382202Z",
     "iopub.status.busy": "2025-05-06T15:06:43.381934Z",
     "iopub.status.idle": "2025-05-06T15:06:46.343192Z",
     "shell.execute_reply": "2025-05-06T15:06:46.338572Z",
     "shell.execute_reply.started": "2025-05-06T15:06:43.38218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = nnx.state(model, nnx.Param)\n",
    "# sum_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:46.347062Z",
     "iopub.status.busy": "2025-05-06T15:06:46.346772Z",
     "iopub.status.idle": "2025-05-06T15:06:46.894873Z",
     "shell.execute_reply": "2025-05-06T15:06:46.888658Z",
     "shell.execute_reply.started": "2025-05-06T15:06:46.347022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    import optax\n",
    "\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-5\n",
    "    regularization_strength = 1e-4\n",
    "    total_steps = len(train_dataloader.dataset) // train_dataloader.batch_size\n",
    "    lr_schedule_type = 'polynomial'\n",
    "    optimizer_type = 'adam'\n",
    "\n",
    "    if lr_schedule_type == 'linear':\n",
    "        lr_schedule = optax.linear_schedule(\n",
    "            learning_rate,\n",
    "            0.0,\n",
    "            num_epochs * total_steps\n",
    "        )\n",
    "    else:\n",
    "        lr_schedule = optax.polynomial_schedule(init_value=learning_rate, end_value=learning_rate/10, power=2, transition_steps=num_epochs*total_steps)\n",
    "\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = nnx.Optimizer(model, optax.adam(lr_schedule, regularization_strength))\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = nnx.Optimizer(model, optax.sgd(lr_schedule, regularization_strength))\n",
    "    elif optimizer_type == 'adan':\n",
    "        optimizer = nnx.Optimizer(model, optax.adan(learning_rate = learning_rate, weight_decay = regularization_strength))\n",
    "    elif optimizer_type == 'lion':\n",
    "        optimizer = nnx.Optimizer(model, optax.lion(learning_rate = lr_schedule, weight_decay = regularization_strength))\n",
    "\n",
    "    def compute_loss_and_logits(model: nnx.Module, x_visual: jax.Array, x_audio: jax.Array, x_text: jax.Array, y: jax.Array, sub_ids: jax.Array):\n",
    "        logits = model(x_visual, x_audio, x_text, sub_ids)\n",
    "        mse_loss = jnp.mean(jnp.square(logits - y))\n",
    "        \n",
    "        # Retrieve model parameters using nnx.state with nnx.Param\n",
    "        params = nnx.state(model, nnx.Param)\n",
    "        # Compute L2 regularization over all parameter leaves\n",
    "        l2_reg = sum(jnp.sum(jnp.square(p)) for p in jax.tree.leaves(params))\n",
    "        \n",
    "        # Combine MSE loss and L2 regularization term\n",
    "        total_loss = mse_loss + regularization_strength * l2_reg\n",
    "        return total_loss, logits\n",
    "\n",
    "    @nnx.jit\n",
    "    def train_step(model: nnx.Module, optimizer: nnx.Optimizer, batch: tuple[jax.Array, jax.Array, jax.Array, jax.Array], train_metrics: nnx.MultiMetric):\n",
    "        x_visual, x_audio, x_text, fmri, sub_ids = batch\n",
    "        y = fmri\n",
    "\n",
    "        grad_fn = nnx.value_and_grad(compute_loss_and_logits, has_aux=True)\n",
    "        (loss, logits), grads = grad_fn(model, x_visual, x_audio, x_text, y, sub_ids)\n",
    "\n",
    "        optimizer.update(grads)\n",
    "\n",
    "        train_metrics.update(\n",
    "            loss=loss,\n",
    "            person_correlation=compute_person_correlation(logits, y)\n",
    "        )\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "    # def compute_person_correlation(logits: jax.Array, y: jax.Array):\n",
    "    #     return jnp.corrcoef(logits, y)[0, 1]\n",
    "\n",
    "    def compute_person_correlation(logits: jax.Array, y: jax.Array):\n",
    "        # Compute correlation for each feature separately\n",
    "        # logits and y shape: [batch_size, num_features]\n",
    "        \n",
    "        # Center the data\n",
    "        logits_centered = logits - jnp.mean(logits, axis=0, keepdims=True)\n",
    "        y_centered = y - jnp.mean(y, axis=0, keepdims=True)\n",
    "        \n",
    "        # Compute correlation for each feature\n",
    "        numerator = jnp.sum(logits_centered * y_centered, axis=0)\n",
    "        denominator = jnp.sqrt(jnp.sum(logits_centered**2, axis=0) * jnp.sum(y_centered**2, axis=0))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        correlation = jnp.where(denominator != 0, numerator / denominator, 0.0)\n",
    "        \n",
    "        # Return mean correlation across all features\n",
    "        return jnp.mean(correlation)\n",
    "\n",
    "    @nnx.jit \n",
    "    def eval_step(model: nnx.Module, batch: tuple[jax.Array, jax.Array, jax.Array, jax.Array], eval_metrics: nnx.MultiMetric, subject_id: int):\n",
    "        x_visual, x_audio, x_text, fmri, sub_ids = batch\n",
    "        y = fmri\n",
    "        loss, logits = compute_loss_and_logits(model, x_visual, x_audio, x_text, y, sub_ids)\n",
    "        person_correlation = compute_person_correlation(logits, y)\n",
    "        eval_metrics.update(\n",
    "            loss = loss,\n",
    "            person_correlation = person_correlation\n",
    "        )\n",
    "        \n",
    "    def convert_batch_to_jax(batch):\n",
    "        x_visual = jnp.array(batch[0], dtype=jnp.float32)\n",
    "        x_audio = jnp.array(batch[1], dtype=jnp.float32)\n",
    "        x_text = jnp.array(batch[2].unsqueeze(1).repeat(1, stimulus_window, 1), dtype=jnp.float32)\n",
    "        y = jnp.array(batch[3], dtype=jnp.float32)\n",
    "        sub_id = jnp.array(batch[4], dtype=jnp.float32)\n",
    "        return x_visual, x_audio, x_text, y, sub_id\n",
    "        \n",
    "    def convert_tensor_to_jax(tensor):\n",
    "        return jnp.array(tensor, dtype=jnp.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:46.898453Z",
     "iopub.status.busy": "2025-05-06T15:06:46.898176Z",
     "iopub.status.idle": "2025-05-06T15:06:46.920115Z",
     "shell.execute_reply": "2025-05-06T15:06:46.916224Z",
     "shell.execute_reply.started": "2025-05-06T15:06:46.898426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    individual_subject_loss_metrics = {\n",
    "        f\"loss_subject_{subject}\": nnx.metrics.Average(f\"loss_subject_{subject}\") for subject in subjects\n",
    "    }\n",
    "    individual_subject_correlation_metrics = {\n",
    "        f\"person_correlation_{subject}\": nnx.metrics.Average(f\"person_correlation_{subject}\") for subject in subjects\n",
    "    }\n",
    "\n",
    "    eval_metrics = nnx.MultiMetric(\n",
    "        **individual_subject_loss_metrics,\n",
    "        **individual_subject_correlation_metrics\n",
    "    )\n",
    "\n",
    "    all_eval_metrics = {sub: nnx.MultiMetric(loss = nnx.metrics.Average(f\"loss\"), \n",
    "                                                person_correlation = nnx.metrics.Average(f\"person_correlation\")) for sub in subjects}\n",
    "\n",
    "    train_metrics = nnx.MultiMetric(\n",
    "        loss=nnx.metrics.Average('loss'),\n",
    "        person_correlation=nnx.metrics.Average('person_correlation')\n",
    "    )\n",
    "\n",
    "    train_metrics_history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_person_correlation\": [],\n",
    "    }\n",
    "\n",
    "    eval_metrics_history = {\n",
    "        \"eval_loss\": [],\n",
    "        \"eval_person_correlation\": [],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:46.924394Z",
     "iopub.status.busy": "2025-05-06T15:06:46.924129Z",
     "iopub.status.idle": "2025-05-06T15:06:47.08009Z",
     "shell.execute_reply": "2025-05-06T15:06:47.075705Z",
     "shell.execute_reply.started": "2025-05-06T15:06:46.92437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "\n",
    "    bar_format = \"{desc}[{n_fmt}/{total_fmt}] {postfix} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_one_epoch(epoch):\n",
    "        model.train()\n",
    "        with tqdm(\n",
    "            desc=f\"[train] epoch {epoch}/{num_epochs}\",\n",
    "            total=total_steps,\n",
    "            bar_format=bar_format\n",
    "        ) as pbar:\n",
    "            for batch in train_dataloader:\n",
    "                loss, logits = train_step(model, optimizer, convert_batch_to_jax(batch), train_metrics)\n",
    "                pbar.set_postfix(**{\n",
    "                    \"loss\": loss,\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "    def evaluate_model(epoch, subject_id, eval_metrics):\n",
    "        model.eval()\n",
    "\n",
    "        eval_metrics.reset()\n",
    "        for val_batch in val_dataloaders[subject_id]:\n",
    "            eval_step(model, convert_batch_to_jax(val_batch), eval_metrics, subject_id)\n",
    "\n",
    "        for metric, value in eval_metrics.compute().items():\n",
    "            eval_metrics_history[f\"eval_{metric}\"].append(value)\n",
    "\n",
    "        print(f\"Epoch {epoch} for subject {subject_id} - {eval_metrics.compute()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:47.084328Z",
     "iopub.status.busy": "2025-05-06T15:06:47.084105Z",
     "iopub.status.idle": "2025-05-06T15:06:51.869213Z",
     "shell.execute_reply": "2025-05-06T15:06:51.86195Z",
     "shell.execute_reply.started": "2025-05-06T15:06:47.084306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    from scipy.stats import pearsonr\n",
    "    from nilearn import plotting\n",
    "    import os\n",
    "    from nilearn.maskers import NiftiLabelsMasker\n",
    "    from io import BytesIO\n",
    "    import io\n",
    "    from PIL import Image\n",
    "\n",
    "    def compute_correlation_plot(model, dataloader, run_name = 'vit_jax', dataset_split = 1):\n",
    "        \"Running this function for training set can take more than 2 minutes, so changing the random_percentage you can just sample a small part of the dataset\"\n",
    "        \n",
    "        fmri_val = []\n",
    "        fmri_val_pred = []\n",
    "        for i, batch_val in enumerate(dataloader):\n",
    "            x_visual, x_audio, x_text, fmri, sub_id = convert_batch_to_jax(batch_val)\n",
    "            logits = model(x_visual, x_audio, x_text, sub_id)\n",
    "            fmri_val_pred.append(logits)\n",
    "            fmri_val.append(fmri)\n",
    "            \n",
    "            if i > ((len(dataloader.dataset) / train_dataloader.batch_size) * dataset_split):\n",
    "                break\n",
    "                \n",
    "        fmri_val = np.concatenate(fmri_val, axis=0)\n",
    "        fmri_val_pred = np.concatenate(fmri_val_pred, axis=0)\n",
    "\n",
    "        output_dir = '.'\n",
    "        alpha = regularization_strength\n",
    "\n",
    "        encoding_accuracy = np.zeros(fmri_val.shape[1], dtype=np.float32)\n",
    "        for p in range(fmri_val.shape[1]):\n",
    "            encoding_accuracy[p] = pearsonr(fmri_val[:, p], fmri_val_pred[:, p])[0]\n",
    "        mean_accuracy = np.round(np.mean(encoding_accuracy), 3)\n",
    "        print(f\"Stimulus Window: {stimulus_window} | Alpha: {alpha} -> Mean Accuracy: {mean_accuracy}\")\n",
    "\n",
    "        atlas_file = f'sub-0{subject}_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-dseg_parcellation.nii'\n",
    "        atlas_path = os.path.join(root_data_dir, 'algonauts_2025.competitors', 'fmri',\n",
    "                                f'sub-0{subject}', 'atlas', atlas_file)\n",
    "        atlas_masker = NiftiLabelsMasker(labels_img=atlas_path)\n",
    "        atlas_masker.fit()\n",
    "        encoding_accuracy_nii = atlas_masker.inverse_transform(encoding_accuracy)\n",
    "\n",
    "        title = (f\"Encoding accuracy, sub-0{subject}, modality-{modality}, \"\n",
    "                f\"sw-{stimulus_window}, alpha-{alpha:.2g}, mean accuracy: {mean_accuracy}\")\n",
    "\n",
    "        display = None # Initialize display to None\n",
    "        fig = None     # Initialize fig to None\n",
    "        pil_image = None # Initialize result to None\n",
    "\n",
    "        try:\n",
    "            display = plotting.plot_glass_brain(\n",
    "                encoding_accuracy_nii,\n",
    "                display_mode=\"lyrz\",\n",
    "                cmap='hot_r',\n",
    "                colorbar=True,\n",
    "                plot_abs=False,\n",
    "                symmetric_cbar=False,\n",
    "                title=title,\n",
    "                # threshold='auto' # Consider adding a threshold\n",
    "            )\n",
    "\n",
    "            # --- Convert display object to PIL Image ---\n",
    "            # 1. Create an in-memory buffer\n",
    "            buf = io.BytesIO()\n",
    "\n",
    "            # 2. Save the figure to the buffer\n",
    "            # Access the underlying matplotlib figure via frame_axes\n",
    "            fig = display.frame_axes.figure\n",
    "            fig.savefig(buf, format='png', dpi=150, bbox_inches='tight') # Adjust dpi as needed\n",
    "\n",
    "            # 3. Rewind the buffer to the beginning\n",
    "            buf.seek(0)\n",
    "\n",
    "            # 4. Load the image from the buffer using PIL\n",
    "            pil_image = Image.open(buf)\n",
    "\n",
    "            # 5. Copy the image data and close the buffer to release memory\n",
    "            # (PIL can sometimes keep the buffer open lazily)\n",
    "            pil_image = pil_image.copy()\n",
    "            buf.close()\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error accessing figure from nilearn display object: {e}\")\n",
    "            print(\"Nilearn's internal structure might have changed or plotting failed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during plotting or PIL conversion: {e}\")\n",
    "        finally:\n",
    "            # 6. Close the matplotlib figure explicitly to free memory\n",
    "            if display is not None:\n",
    "                try:\n",
    "                    # Use the display object's close method if available\n",
    "                    display.close()\n",
    "                except AttributeError:\n",
    "                    # Fallback to closing the figure directly if display.close() doesn't exist\n",
    "                    if fig is not None:\n",
    "                        plt.close(fig)\n",
    "            elif fig is not None:\n",
    "                # If display failed but fig was somehow created\n",
    "                plt.close(fig)\n",
    "        return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:51.872634Z",
     "iopub.status.busy": "2025-05-06T15:06:51.872093Z",
     "iopub.status.idle": "2025-05-06T15:06:57.707757Z",
     "shell.execute_reply": "2025-05-06T15:06:57.703518Z",
     "shell.execute_reply.started": "2025-05-06T15:06:51.872606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n",
    "\n",
    "    # set up wandb\n",
    "    import wandb\n",
    "    wandb.login(key=secret_value_0)\n",
    "\n",
    "\n",
    "    wandb_project = 'algonauts2025'\n",
    "    run_name = 'vit_jax_internvl31b_whisper_modernbert_sw6_full_features_multi_submission'\n",
    "\n",
    "    print(f\"Wandb project: {wandb_project}, run name: {run_name}\")\n",
    "\n",
    "    wandb_config = {\n",
    "        'run_name': run_name,\n",
    "        'stimulus_window': stimulus_window,\n",
    "        'regularization_strength': regularization_strength,\n",
    "        'model_config': model_config,\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'total_steps': total_steps,\n",
    "        'lr_schedule': lr_schedule_type,\n",
    "        'optimizer': optimizer_type,\n",
    "        \"total_model_parameters\": total_model_parameters,\n",
    "        \"subjects\": subjects\n",
    "    }\n",
    "\n",
    "    print(f\"Wandb config: {wandb_config}\")\n",
    "    print(f\"Wandb id: {wandb.util.generate_id()}\")\n",
    "\n",
    "    wandb.init(\n",
    "        id=wandb.util.generate_id(),\n",
    "        project=wandb_project,\n",
    "        name=run_name,\n",
    "        config=wandb_config,\n",
    "        resume='allow',\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:57.712035Z",
     "iopub.status.busy": "2025-05-06T15:06:57.711802Z",
     "iopub.status.idle": "2025-05-06T15:06:57.724842Z",
     "shell.execute_reply": "2025-05-06T15:06:57.719141Z",
     "shell.execute_reply.started": "2025-05-06T15:06:57.712013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def log_wandb(\n",
    "    epoch: int,\n",
    "    train_metrics: nnx.MultiMetric,\n",
    "    train_correlation_plot: wandb.Plotly,\n",
    "):\n",
    "    # build a single dict of everything\n",
    "    metrics = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_metrics.loss.compute(),                  \n",
    "        \"train_correlation\": train_metrics.person_correlation.compute(),                    \n",
    "        \"train_correlation_plot\": wandb.Image(train_correlation_plot),       \n",
    "    }\n",
    "    \n",
    "    wandb.log(metrics, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T15:06:57.725947Z",
     "iopub.status.busy": "2025-05-06T15:06:57.725697Z",
     "iopub.status.idle": "2025-05-06T16:28:32.901254Z",
     "shell.execute_reply": "2025-05-06T16:28:32.896041Z",
     "shell.execute_reply.started": "2025-05-06T15:06:57.725922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    import time\n",
    "\n",
    "\n",
    "    best_epoch = 8 # num_epoch # loaded from best_config.json\n",
    "    for epoch in range(best_epoch + 1):\n",
    "        print(f\"\\n--- Epoch {epoch} ---\")\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_one_epoch(epoch)\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Training took {duration:.2f} seconds\")\n",
    "\n",
    "  \n",
    "\n",
    "        start_time = time.time()\n",
    "        plot_correlation_train = compute_correlation_plot(model, train_dataloader, dataset_split = 0.1)\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Train correlation plot took {duration:.2f} seconds\")\n",
    "\n",
    "\n",
    "        save_ckpt('last', model, display = False)\n",
    "\n",
    "        \n",
    "        start_time = time.time()\n",
    "        log_wandb(\n",
    "            epoch,\n",
    "            train_metrics,\n",
    "            plot_correlation_train\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"W&B logging took {duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare inference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:31:04.84972Z",
     "iopub.status.busy": "2025-05-06T16:31:04.849312Z",
     "iopub.status.idle": "2025-05-06T16:31:04.883257Z",
     "shell.execute_reply": "2025-05-06T16:31:04.875555Z",
     "shell.execute_reply.started": "2025-05-06T16:31:04.849688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_stimulus_features_s7(root_data_dirs: Union[str, List[str]], modality: str, layers: List[str] = None) -> dict:\n",
    "    features = {modality: {}}\n",
    "\n",
    "    # Ensure root_data_dirs is a list\n",
    "    if isinstance(root_data_dirs, str):\n",
    "        root_data_dirs = [root_data_dirs]\n",
    "\n",
    "    # Iterate over each root directory provided.\n",
    "    for root_data_dir in root_data_dirs:\n",
    "        root_path = Path(root_data_dir)\n",
    "        # Traverse all .h5 files under the current root directory.\n",
    "        all_files = list(root_path.rglob(\"*.h5\"))\n",
    "        s7_files = [f for f in all_files if \"friends_s07\" in f.stem]\n",
    "\n",
    "        print(f\"Found {len(s7_files)} S7 files in {root_data_dir}\")\n",
    "\n",
    "        for h5_file in tqdm(s7_files, desc=f\"Processing {root_data_dir}\"):\n",
    "            movie_name = h5_file.stem\n",
    "            # Remove the \"friends_\" prefix if it exists.\n",
    "            if movie_name.startswith(\"friends_\"):\n",
    "                movie_name = movie_name[len(\"friends_\"):]\n",
    "                \n",
    "            datasets = []\n",
    "            with h5py.File(h5_file, 'r') as f:\n",
    "                # Iterate over all datasets (layers) in the file.\n",
    "                for layer in f.keys():\n",
    "                    data = f[layer][:]\n",
    "                    if layers:\n",
    "                        if layer in layers:\n",
    "                            datasets.append(data)\n",
    "                    else:\n",
    "                        datasets.append(data)\n",
    "            \n",
    "            # If multiple layers exist, concatenate along axis=1.\n",
    "            if len(datasets) > 1:\n",
    "                datasets = [np.reshape(item, (item.shape[0], -1)) for item in datasets]\n",
    "                # print(datasets[0].shape, datasets[1].shape, datasets[2].shape)\n",
    "                concatenated_features = np.concatenate(datasets, axis=1)\n",
    "            elif datasets:\n",
    "                concatenated_features = datasets[0]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # If the same movie_name is encountered from multiple directories,\n",
    "            # you might choose to either overwrite or merge; here we overwrite.\n",
    "            features[modality][movie_name] = concatenated_features.reshape(concatenated_features.shape[0], -1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def align_features_and_fmri_samples_friends_s7(features_friends_s7,\n",
    "    root_data_dir):\n",
    "    \"\"\"\n",
    "    Align the stimulus feature with the fMRI response samples for Friends season\n",
    "    7 episodes, later used to predict the fMRI responses for challenge\n",
    "    submission.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_friends_s7 : dict\n",
    "        Dictionary containing the stimulus features for Friends season 7.\n",
    "    root_data_dir : str\n",
    "        Root data directory.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aligned_features_friends_s7 : dict\n",
    "        Aligned stimulus features for each subject and Friends season 7 episode.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### Empty results dictionary ###\n",
    "    aligned_features_friends_s7 = {}\n",
    "\n",
    "    ### HRF delay ###\n",
    "    # fMRI detects the BOLD (Blood Oxygen Level Dependent) response, a signal\n",
    "    # that reflects changes in blood oxygenation levels in response to activity\n",
    "    # in the brain. Blood flow increases to a given brain region in response to\n",
    "    # its activity. This vascular response, which follows the hemodynamic\n",
    "    # response function (HRF), takes time. Typically, the HRF peaks around 5–6\n",
    "    # seconds after a neural event: this delay reflects the time needed for\n",
    "    # blood oxygenation changes to propagate and for the fMRI signal to capture\n",
    "    # them. Therefore, this parameter introduces a delay between stimulus chunks\n",
    "    # and fMRI samples for a better correspondence between input stimuli and the\n",
    "    # brain response. For example, with a hrf_delay of 3, if the stimulus chunk\n",
    "    # of interest is 17, the corresponding fMRI sample will be 20.\n",
    "\n",
    "    ### Stimulus window ###\n",
    "    # stimulus_window indicates how many stimulus feature samples are used to\n",
    "    # model each fMRI sample, starting from the stimulus sample corresponding to\n",
    "    # the fMRI sample of interest, minus the hrf_delay, and going back in time.\n",
    "    # For example, with a stimulus_window of 5, and a hrf_delay of 3, if the\n",
    "    # fMRI sample of interest is 20, it will be modeled with stimulus samples\n",
    "    # [13, 14, 15, 16, 17]. Note that this only applies to visual and audio\n",
    "    # features, since the language features were already extracted using\n",
    "    # transcript words spanning several movie samples (thus, each fMRI sample\n",
    "    # will only be modeled using the corresponding language feature sample,\n",
    "    # minus the hrf_delay). Also note that a larger stimulus window will\n",
    "    # increase compute time, since it increases the amount of stimulus features\n",
    "    # used to train and validate the fMRI encoding models. Here you will use a\n",
    "    # value of 5, since this is how the challenge baseline encoding models were\n",
    "    # trained.\n",
    "\n",
    "    ### Loop over subjects ###\n",
    "    subjects = [1, 2, 3, 5]\n",
    "    desc = \"Aligning stimulus and fMRI features of the four subjects\"\n",
    "    for sub in tqdm(subjects, desc=desc):\n",
    "        aligned_features_friends_s7[f'sub-0{sub}'] = {}\n",
    "\n",
    "        ### Load the Friends season 7 fMRI samples ###\n",
    "        samples_dir = os.path.join(root_data_dir, 'algonauts_2025.competitors',\n",
    "            'fmri', f'sub-0{sub}', 'target_sample_number',\n",
    "            f'sub-0{sub}_friends-s7_fmri_samples.npy')\n",
    "        fmri_samples = np.load(samples_dir, allow_pickle=True).item()\n",
    "\n",
    "        ### Loop over Friends season 7 episodes ###\n",
    "        for epi, samples in fmri_samples.items():\n",
    "            features_epi = []\n",
    "\n",
    "            ### Loop over fMRI samples ###\n",
    "            for s in range(samples):\n",
    "                # Empty variable containing the stimulus features of all\n",
    "                # modalities for each sample\n",
    "                f_all = np.empty(0)\n",
    "\n",
    "                ### Loop across modalities ###\n",
    "                for mod in features_friends_s7.keys():\n",
    "\n",
    "                    ### Visual and audio features ###\n",
    "                    # If visual or audio modality, model each fMRI sample using\n",
    "                    # the N stimulus feature samples up to the fMRI sample of\n",
    "                    # interest minus the hrf_delay (where N is defined by the\n",
    "                    # 'stimulus_window' variable)\n",
    "                    if mod == 'visual' or mod == 'audio':\n",
    "                        # In case there are not N stimulus feature samples up to\n",
    "                        # the fMRI sample of interest minus the hrf_delay (where\n",
    "                        # N is defined by the 'stimulus_window' variable), model\n",
    "                        # the fMRI sample using the first N stimulus feature\n",
    "                        # samples\n",
    "                        if s < (stimulus_window + hrf_delay):\n",
    "                            idx_start = 0\n",
    "                            idx_end = idx_start + stimulus_window\n",
    "                        else:\n",
    "                            idx_start = s - hrf_delay - stimulus_window + 1\n",
    "                            idx_end = idx_start + stimulus_window\n",
    "                        # In case there are less visual/audio feature samples\n",
    "                        # than fMRI samples minus the hrf_delay, use the last N\n",
    "                        # visual/audio feature samples available (where N is\n",
    "                        # defined by the 'stimulus_window' variable)\n",
    "                        if idx_end > len(features_friends_s7[mod][epi]):\n",
    "                            idx_end = len(features_friends_s7[mod][epi])\n",
    "                            idx_start = idx_end - stimulus_window\n",
    "                        f = features_friends_s7[mod][epi][idx_start:idx_end]\n",
    "                        f_all = np.append(f_all, f.flatten())\n",
    "\n",
    "                    ### Language features ###\n",
    "                    # Since language features already consist of embeddings\n",
    "                    # spanning several samples, only model each fMRI sample\n",
    "                    # using the corresponding stimulus feature sample minus the\n",
    "                    # hrf_delay\n",
    "                    elif mod == 'language':\n",
    "                        # In case there are no language features for the fMRI\n",
    "                        # sample of interest minus the hrf_delay, model the fMRI\n",
    "                        # sample using the first language feature sample\n",
    "                        if s < hrf_delay:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            idx = s - hrf_delay\n",
    "                        # In case there are fewer language feature samples than\n",
    "                        # fMRI samples minus the hrf_delay, use the last\n",
    "                        # language feature sample available\n",
    "                        if idx >= (len(features_friends_s7[mod][epi]) - hrf_delay):\n",
    "                            f = features_friends_s7[mod][epi][-1,:]\n",
    "                        else:\n",
    "                            f = features_friends_s7[mod][epi][idx]\n",
    "                        f_all = np.append(f_all, f.flatten())\n",
    "\n",
    "                ### Append the stimulus features of all modalities for this sample ###\n",
    "                features_epi.append(f_all)\n",
    "\n",
    "            ### Add the episode stimulus features to the features dictionary ###\n",
    "            aligned_features_friends_s7[f'sub-0{sub}'][epi] = np.asarray(\n",
    "                features_epi, dtype=np.float32)\n",
    "\n",
    "    return aligned_features_friends_s7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:31:17.756785Z",
     "iopub.status.busy": "2025-05-06T16:31:17.756417Z",
     "iopub.status.idle": "2025-05-06T16:33:51.287075Z",
     "shell.execute_reply": "2025-05-06T16:33:51.278446Z",
     "shell.execute_reply.started": "2025-05-06T16:31:17.756756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "features_friends_s7 = {}\n",
    "\n",
    "features_friends_s7['visual'] = load_stimulus_features_s7(\n",
    "    [os.path.join(base_features_dir, video_features_name)],\n",
    "    modality='visual',\n",
    "    layers=layers_visual)['visual']\n",
    "\n",
    "features_friends_s7['audio'] = load_stimulus_features_s7(\n",
    "    [os.path.join(base_features_dir, audio_features_name)],\n",
    "    modality='audio',\n",
    "    layers=layers_audio)['audio']\n",
    "\n",
    "features_friends_s7['language'] = load_stimulus_features_s7(\n",
    "    [os.path.join(base_features_dir, transcript_features_name)],\n",
    "    modality='language',\n",
    "    layers=layers_transcript)['language']\n",
    "\n",
    "\n",
    "aligned_features_friends_s7 = align_features_and_fmri_samples_friends_s7(features_friends_s7, root_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:33:51.289777Z",
     "iopub.status.busy": "2025-05-06T16:33:51.289512Z",
     "iopub.status.idle": "2025-05-06T16:50:27.505766Z",
     "shell.execute_reply": "2025-05-06T16:50:27.496737Z",
     "shell.execute_reply.started": "2025-05-06T16:33:51.289751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Empty submission predictions dictionary\n",
    "submission_predictions = {}\n",
    "\n",
    "# Loop through each subject \n",
    "desc = \"Predicting fMRI responses of each subject\"\n",
    "for sub, features in tqdm(aligned_features_friends_s7.items(), desc=desc):\n",
    "\n",
    "    visual_features_shape_length = features_friends_s7['visual']['s07e01a'].shape[-1]\n",
    "    audio_features_shape_length = features_friends_s7['audio']['s07e01a'].shape[-1]\n",
    "    language_features_shape_length = features_friends_s7['language']['s07e01a'].shape[-1]\n",
    "\n",
    "    visual_features_shape_indexes = (0, visual_features_shape_length * stimulus_window)\n",
    "    audio_features_shape_indexes = (visual_features_shape_indexes[1], visual_features_shape_indexes[1] + audio_features_shape_length * stimulus_window)\n",
    "    language_features_shape_indexes = (audio_features_shape_indexes[1], audio_features_shape_indexes[1] + language_features_shape_length) # No window for language\n",
    "\n",
    "    # Initialize the nested dictionary for each subject's predictions\n",
    "    submission_predictions[sub] = {}\n",
    "\n",
    "    # Loop through each Friends season 7 episode\n",
    "    for epi, feat_epi in tqdm(features.items()):\n",
    "\n",
    "        \n",
    "        # Do the inference all at once\n",
    "        subject_ids = np.array([int(sub.split('-')[1]) for _ in range(feat_epi.shape[0])])\n",
    "        subject_ids = jnp.array(subject_ids, dtype=jnp.int32)\n",
    "\n",
    "        x_visual = jnp.array(feat_epi[:, visual_features_shape_indexes[0]:visual_features_shape_indexes[1]], dtype=jnp.float32)\n",
    "        x_audio = jnp.array(feat_epi[:, audio_features_shape_indexes[0]:audio_features_shape_indexes[1]], dtype=jnp.float32)\n",
    "        x_text = torch.Tensor(feat_epi[:, language_features_shape_indexes[0]:language_features_shape_indexes[1]])\n",
    "\n",
    "        x_visual = einops.rearrange(x_visual, 'b (w a) -> b w a', w = stimulus_window)\n",
    "        x_audio = einops.rearrange(x_audio, 'b (w a) -> b w a', w = stimulus_window)\n",
    "        x_text = jnp.array(x_text.unsqueeze(1).repeat(1, stimulus_window, 1), dtype=jnp.float32)\n",
    "        \n",
    "        # print(x_visual.shape)\n",
    "        # Make predictions\n",
    "        fmri_pred = model(x_visual, x_audio, x_text, subject_ids)\n",
    "        fmri_pred = np.array(fmri_pred, dtype=np.float32)\n",
    "        \n",
    "\n",
    "        # Store formatted predictions in the nested dictionary\n",
    "        submission_predictions[sub][epi] = fmri_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:50:27.508659Z",
     "iopub.status.busy": "2025-05-06T16:50:27.508377Z",
     "iopub.status.idle": "2025-05-06T16:50:27.525414Z",
     "shell.execute_reply": "2025-05-06T16:50:27.514719Z",
     "shell.execute_reply.started": "2025-05-06T16:50:27.508631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visual_features_shape_indexes, audio_features_shape_indexes, language_features_shape_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:50:27.528047Z",
     "iopub.status.busy": "2025-05-06T16:50:27.52778Z",
     "iopub.status.idle": "2025-05-06T16:50:27.54299Z",
     "shell.execute_reply": "2025-05-06T16:50:27.533429Z",
     "shell.execute_reply.started": "2025-05-06T16:50:27.528021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# features_friends_s7['visual']['s07e01a'].shape[-1] / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:50:27.54547Z",
     "iopub.status.busy": "2025-05-06T16:50:27.545236Z",
     "iopub.status.idle": "2025-05-06T16:50:27.559015Z",
     "shell.execute_reply": "2025-05-06T16:50:27.551284Z",
     "shell.execute_reply.started": "2025-05-06T16:50:27.545447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# aligned_features_friends_s7['sub-01']['s07e01a'].shape[-1] / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:50:27.561229Z",
     "iopub.status.busy": "2025-05-06T16:50:27.560999Z",
     "iopub.status.idle": "2025-05-06T16:50:27.933494Z",
     "shell.execute_reply": "2025-05-06T16:50:27.928926Z",
     "shell.execute_reply.started": "2025-05-06T16:50:27.561207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display the structure and shapes of the predicted fMRI responses dictionary\n",
    "for subject, episodes_dict in submission_predictions.items():\n",
    "    # Print the subject and episode number for Friends season 7\n",
    "    print(f\"Subject: {subject}\")\n",
    "    print(f\"  Number of Episodes: {len(episodes_dict)}\")\n",
    "    # Print the predicted fMRI response shape for each episode\n",
    "    for episode, predictions in episodes_dict.items():\n",
    "        print(f\"    - Episode: {episode}, Predicted fMRI shape: {predictions.shape}\")\n",
    "    print(\"-\" * 40)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T16:50:27.938575Z",
     "iopub.status.busy": "2025-05-06T16:50:27.938259Z",
     "iopub.status.idle": "2025-05-06T16:50:30.341352Z",
     "shell.execute_reply": "2025-05-06T16:50:30.332443Z",
     "shell.execute_reply.started": "2025-05-06T16:50:27.938546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Select the saving directory\n",
    "save_dir = './' #@param {type:\"string\"}\n",
    "\n",
    "# Save the predicted fMRI dictionary as a .npy file\n",
    "output_file = save_dir + \"fmri_predictions_friends_s7.npy\"\n",
    "np.save(output_file, submission_predictions)\n",
    "print(f\"Formatted predictions saved to: {output_file}\")\n",
    "\n",
    "# Zip the saved file for submission\n",
    "zip_file = save_dir + \"fmri_predictions_friends_s7.zip\"\n",
    "with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "    zipf.write(output_file, os.path.basename(output_file))\n",
    "print(f\"Submission file successfully zipped as: {zip_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6674202,
     "sourceId": 10759729,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6734778,
     "sourceId": 10844276,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6864493,
     "sourceId": 11023369,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6872303,
     "sourceId": 11034440,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7233037,
     "sourceId": 11532053,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7288241,
     "sourceId": 11663482,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 237416082,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31013,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
