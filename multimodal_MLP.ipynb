{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_files(features_dir, movie_list):\n",
    "    total_files = 0\n",
    "    for movie in movie_list:\n",
    "        if 'friends' in movie:\n",
    "            season = movie.split('-')[1]\n",
    "            pattern = f\"{season}e\"\n",
    "        else:\n",
    "            movie_name = movie.replace('movie10-', '')\n",
    "            pattern = movie_name\n",
    "            \n",
    "        files = [f for f in os.listdir(features_dir + 'audio') \n",
    "                if '_features_' in f and pattern in f]\n",
    "        total_files += len(files)\n",
    "            \n",
    "    return total_files\n",
    "\n",
    "def get_fmri_files(subject, task_type, fmri_dir):\n",
    "    subject_dir = os.path.join(fmri_dir, f'sub-{subject}/func/')\n",
    "    files = [f for f in os.listdir(subject_dir) if f'sub-{subject}_task-{task_type}' in f]\n",
    "    if len(files) != 1:\n",
    "        raise ValueError(f\"Expected 1 fMRI file for subject {subject} and task {task_type}, found {len(files)}\")\n",
    "    return os.path.join(subject_dir, files[0])\n",
    "\n",
    "def load_fmri(root_data_dir, subject):\n",
    "    fmri = {}\n",
    "\n",
    "    fmri_file = f'sub-0{subject}_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5'\n",
    "    fmri_dir = os.path.join(root_data_dir, f'sub-0{subject}', 'func', fmri_file)\n",
    "    fmri_friends = h5py.File(fmri_dir, 'r')\n",
    "    for key, val in fmri_friends.items():\n",
    "        fmri[str(key[13:])] = val[:].astype(np.float32)\n",
    "    del fmri_friends\n",
    "\n",
    "    fmri_file = f'sub-0{subject}_task-movie10_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_bold.h5'\n",
    "    fmri_dir = os.path.join(root_data_dir, f'sub-0{subject}', 'func', fmri_file)\n",
    "    fmri_movie10 = h5py.File(fmri_dir, 'r')\n",
    "    for key, val in fmri_movie10.items():\n",
    "        fmri[key[13:]] = val[:].astype(np.float32)\n",
    "    del fmri_movie10\n",
    "\n",
    "    keys_all = fmri.keys()\n",
    "    figures_splits = 12\n",
    "    for s in range(figures_splits):\n",
    "        movie = 'figures' + format(s+1, '02')\n",
    "        keys_movie = [rep for rep in keys_all if movie in rep]\n",
    "        fmri[movie] = ((fmri[keys_movie[0]] + fmri[keys_movie[1]]) / 2).astype(np.float32)\n",
    "        del fmri[keys_movie[0]]\n",
    "        del fmri[keys_movie[1]]\n",
    "\n",
    "    keys_all = fmri.keys()\n",
    "    life_splits = 5\n",
    "    for s in range(life_splits):\n",
    "        movie = 'life' + format(s+1, '02')\n",
    "        keys_movie = [rep for rep in keys_all if movie in rep]\n",
    "        fmri[movie] = ((fmri[keys_movie[0]] + fmri[keys_movie[1]]) / 2).astype(np.float32)\n",
    "        del fmri[keys_movie[0]]\n",
    "        del fmri[keys_movie[1]]\n",
    "\n",
    "    return fmri\n",
    "\n",
    "class AlgonautsDataset(Dataset):\n",
    "    def __init__(self, features_dir, fmri_dir, movies, subjects, excluded_samples_start=5, excluded_samples_end=5, hrf_delay=3, stimulus_window=5):\n",
    "        self.features_dir = features_dir\n",
    "        self.fmri_dir = fmri_dir\n",
    "        self.movies = movies\n",
    "        self.subjects = subjects\n",
    "        self.excluded_samples_start = excluded_samples_start\n",
    "        self.excluded_samples_end = excluded_samples_end\n",
    "        self.hrf_delay = hrf_delay\n",
    "        self.stimulus_window = stimulus_window\n",
    "        self.partition_indices = defaultdict(list)\n",
    "        \n",
    "        audio_data, video_data, language_data, fmri_data = [], [], [], []\n",
    "        current_idx = 0\n",
    "        \n",
    "        # Load fMRI data for current subject\n",
    "        for subject in self.subjects:\n",
    "            fmri_dict = load_fmri(self.fmri_dir, subject)\n",
    "        \n",
    "        total_files = count_total_files(self.features_dir, self.movies)\n",
    "        pbar = tqdm(desc='Loading dataset', total=total_files)\n",
    "        \n",
    "        for movie in self.movies:\n",
    "            start_idx = current_idx\n",
    "            if 'friends' in movie:\n",
    "                season = movie.split('-')[1]\n",
    "                dir_list = sorted(os.listdir(self.features_dir + 'audio'))\n",
    "                for episode in dir_list:\n",
    "                    if f\"{season}e\" in episode and '_features_' in episode:\n",
    "                        episode_base = episode.split('_features_')[0]\n",
    "                        \n",
    "                        features = {'audio': None, 'visual': None, 'language': None}\n",
    "                        \n",
    "                        # Load all modalities\n",
    "                        for modality in ['audio', 'visual', 'language']:\n",
    "                            with h5py.File(os.path.join(self.features_dir, modality, f\"{episode_base}_features_{modality}.h5\"), 'r') as f:\n",
    "                                try:\n",
    "                                    key = 'language_pooler_output' if modality == 'language' else modality\n",
    "                                    features[modality] = f[episode_base.split('_')[1]][key][:]\n",
    "                                except:\n",
    "                                    f.visit(lambda x: print(x))\n",
    "                                    \n",
    "                        # Get fMRI data\n",
    "                        try:\n",
    "                            fmri = fmri_dict[episode_base.split(\"_\")[1]]\n",
    "                        except:\n",
    "                            print(fmri_dict.keys())\n",
    "                            print(\"Current key:\", episode_base.split(\"_\")[1])\n",
    "                            \n",
    "                        # Align features with fMRI using sliding window\n",
    "                        if all(v is not None for v in features.values()):\n",
    "                            valid_fmri = fmri[excluded_samples_start:-excluded_samples_end]\n",
    "                            for s in range(len(valid_fmri)):\n",
    "                                aligned_features = []\n",
    "                                \n",
    "                                # Handle audio and video with sliding window\n",
    "                                for mod in ['audio', 'visual']:\n",
    "                                    if s < (stimulus_window + hrf_delay):\n",
    "                                        idx_start = excluded_samples_start\n",
    "                                        idx_end = idx_start + stimulus_window\n",
    "                                    else:\n",
    "                                        idx_start = s + excluded_samples_start - hrf_delay - stimulus_window + 1\n",
    "                                        idx_end = idx_start + stimulus_window\n",
    "                                        \n",
    "                                    if idx_end > len(features[mod]):\n",
    "                                        idx_end = len(features[mod])\n",
    "                                        idx_start = idx_end - stimulus_window\n",
    "                                        \n",
    "                                    feat = features[mod][idx_start:idx_end]\n",
    "                                    aligned_features.append(feat)\n",
    "                                \n",
    "                                # Handle language features\n",
    "                                idx = s + excluded_samples_start - hrf_delay\n",
    "                                if idx >= len(features['language']):\n",
    "                                    lang_feat = features['language'][-1]\n",
    "                                else:\n",
    "                                    lang_feat = features['language'][idx]\n",
    "                                aligned_features.append(lang_feat)\n",
    "                                \n",
    "                                # Store aligned features\n",
    "                                audio_data.append(aligned_features[0])\n",
    "                                video_data.append(aligned_features[1])\n",
    "                                language_data.append(aligned_features[2])\n",
    "                                fmri_data.append(fmri[s + excluded_samples_start])\n",
    "                                current_idx += 1\n",
    "                                \n",
    "                        pbar.update(1)\n",
    "                        \n",
    "            else:\n",
    "                # Handle movies similarly...\n",
    "                movie_name = movie.replace('movie10-', '')\n",
    "                partitions = sorted([f for f in os.listdir(self.features_dir + 'audio') if movie_name in f and '_features_' in f])\n",
    "                \n",
    "                for partition in partitions:\n",
    "                    partition_base = partition.split('_features_')[0]\n",
    "                    \n",
    "                    features = {'audio': None, 'visual': None, 'language': None}\n",
    "                    \n",
    "                    # Load features\n",
    "                    for modality in ['audio', 'visual', 'language']:\n",
    "                        with h5py.File(os.path.join(self.features_dir, modality, f\"{partition_base}_features_{modality}.h5\"), 'r') as f:\n",
    "                            try:\n",
    "                                key = 'language_pooler_output' if modality == 'language' else modality\n",
    "                                features[modality] = f[partition_base][key][:]\n",
    "                            except:\n",
    "                                f.visit(lambda x: print(x))\n",
    "                                \n",
    "                    # Get fMRI data\n",
    "                    fmri = fmri_dict[partition_base]\n",
    "                    \n",
    "                    # Align features with fMRI\n",
    "                    if all(v is not None for v in features.values()):\n",
    "                        valid_fmri = fmri[excluded_samples_start:-excluded_samples_end]\n",
    "                        for s in range(len(valid_fmri)):\n",
    "                            aligned_features = []\n",
    "                            \n",
    "                            # Handle audio and video\n",
    "                            for mod in ['audio', 'visual']:\n",
    "                                if s < (stimulus_window + hrf_delay):\n",
    "                                    idx_start = excluded_samples_start\n",
    "                                    idx_end = idx_start + stimulus_window\n",
    "                                else:\n",
    "                                    idx_start = s + excluded_samples_start - hrf_delay - stimulus_window + 1\n",
    "                                    idx_end = idx_start + stimulus_window\n",
    "                                    \n",
    "                                if idx_end > len(features[mod]):\n",
    "                                    idx_end = len(features[mod])\n",
    "                                    idx_start = idx_end - stimulus_window\n",
    "                                    \n",
    "                                feat = features[mod][idx_start:idx_end]\n",
    "                                aligned_features.append(feat)\n",
    "                            \n",
    "                            # Handle language\n",
    "                            idx = s + excluded_samples_start - hrf_delay\n",
    "                            if idx >= len(features['language']):\n",
    "                                lang_feat = features['language'][-1]\n",
    "                            else:\n",
    "                                lang_feat = features['language'][idx]\n",
    "                            aligned_features.append(lang_feat)\n",
    "                            \n",
    "                            # Store aligned features\n",
    "                            audio_data.append(aligned_features[0])\n",
    "                            video_data.append(aligned_features[1])\n",
    "                            language_data.append(aligned_features[2])\n",
    "                            fmri_data.append(fmri[s + excluded_samples_start])\n",
    "                            current_idx += 1\n",
    "                            \n",
    "                    pbar.update(1)\n",
    "            \n",
    "            self.partition_indices[movie] = (start_idx, current_idx)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.audio = torch.from_numpy(np.stack(audio_data))\n",
    "        self.video = torch.from_numpy(np.stack(video_data))\n",
    "        self.language = torch.from_numpy(np.stack(language_data))\n",
    "        self.fmri = torch.from_numpy(np.stack(fmri_data))\n",
    "        \n",
    "        # Verify loading order\n",
    "        for movie in self.movies:\n",
    "            start, end = self.partition_indices[movie]\n",
    "            assert start < end, f\"Invalid index range for {movie}\"\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.audio)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'audio': self.audio[idx],\n",
    "            'video': self.video[idx],\n",
    "            'language': self.language[idx],\n",
    "            'fmri': self.fmri[idx]\n",
    "        }\n",
    "        \n",
    "    def get_partition_indices(self):\n",
    "        return self.partition_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 286/286 [00:01<00:00, 215.99it/s]\n",
      "Loading dataset: 100%|██████████| 50/50 [00:00<00:00, 326.16it/s]\n"
     ]
    }
   ],
   "source": [
    "features_dir = '/home/pranav/mihir/algonauts_challenge/AlgonautsDS-features/developer_kit/stimulus_features/raw/'\n",
    "fmri_dir = '/home/pranav/mihir/algonauts_challenge/algonauts_2025.competitors/fmri/'\n",
    "movies_train = [\"friends-s01\", \"friends-s02\", \"friends-s03\", \"friends-s04\", \"friends-s05\", \"movie10-bourne\", \"movie10-figures\", \"movie10-life\", \"movie10-wolf\"]\n",
    "movies_val = [\"friends-s06\"]\n",
    "modality = \"all\"  #@param [\"visual\", \"audio\", \"language\", \"all\"]\n",
    "\n",
    "excluded_samples_start = 5  #@param {type:\"slider\", min:0, max:20, step:1}\n",
    "excluded_samples_end = 5  #@param {type:\"slider\", min:0, max:20, step:1}\n",
    "hrf_delay = 3  #@param {type:\"slider\", min:0, max:10, step:1}\n",
    "stimulus_window = 1\n",
    "\n",
    "train_subjects = ['1'] #@param [\"1\", \"2\", \"3\", \"5\"] {type:\"raw\", allow-input: true}\n",
    "test_subjects = ['2']\n",
    "\n",
    "train_ds = AlgonautsDataset(features_dir, fmri_dir, movies=movies_train, subjects=train_subjects, excluded_samples_start=excluded_samples_start, excluded_samples_end=excluded_samples_end, hrf_delay=hrf_delay, stimulus_window=stimulus_window)\n",
    "val_ds = AlgonautsDataset(features_dir, fmri_dir, movies=movies_val, subjects=test_subjects, excluded_samples_start=excluded_samples_start, excluded_samples_end=excluded_samples_end, hrf_delay=hrf_delay, stimulus_window=stimulus_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "from torchmetrics.functional import pearson_corrcoef\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "   def __init__(self, in_dim, out_dim):\n",
    "       super().__init__()\n",
    "       self.downsample = in_dim != out_dim\n",
    "       self.net = nn.Sequential(\n",
    "           nn.Linear(in_dim, out_dim),\n",
    "           nn.LayerNorm(out_dim),\n",
    "           nn.GELU(),\n",
    "           nn.Linear(out_dim, out_dim),\n",
    "           nn.LayerNorm(out_dim),\n",
    "           nn.GELU()\n",
    "       )\n",
    "       if self.downsample:\n",
    "           self.proj = nn.Linear(in_dim, out_dim)\n",
    "   \n",
    "   def forward(self, x):\n",
    "       if self.downsample:\n",
    "           return self.proj(x) + self.net(x)\n",
    "       return x + self.net(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "   def __init__(self, input_dim=1000, hidden_dims=[512, 384, 256], num_tokens=32, codebook_dim=64):\n",
    "       super().__init__()\n",
    "       \n",
    "       # Initial projection with one residual block\n",
    "       self.input_proj = ResidualBlock(input_dim, hidden_dims[0])\n",
    "       \n",
    "       # Main network with one residual block per layer\n",
    "       layers = []\n",
    "       for i in range(len(hidden_dims)-1):\n",
    "           layers.append(ResidualBlock(hidden_dims[i], hidden_dims[i+1]))\n",
    "       self.layers = nn.Sequential(*layers)\n",
    "       \n",
    "       # Project to token space with one residual block\n",
    "       self.token_proj = ResidualBlock(hidden_dims[-1], num_tokens * codebook_dim)\n",
    "       \n",
    "       self.num_tokens = num_tokens\n",
    "       self.codebook_dim = codebook_dim\n",
    "       \n",
    "   def forward(self, x):\n",
    "       x = self.input_proj(x)\n",
    "       x = self.layers(x)\n",
    "       x = self.token_proj(x)\n",
    "       return x.view(x.shape[0], self.num_tokens, self.codebook_dim)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "   def __init__(self, output_dim=1000, hidden_dims=[256, 384, 512], num_tokens=32, codebook_dim=64):\n",
    "       super().__init__()\n",
    "       \n",
    "       # Process tokens with one residual block\n",
    "       self.token_proj = ResidualBlock(num_tokens * codebook_dim, hidden_dims[0])\n",
    "       \n",
    "       # Main network with one residual block per layer\n",
    "       layers = []\n",
    "       for i in range(len(hidden_dims)-1):\n",
    "           layers.append(ResidualBlock(hidden_dims[i], hidden_dims[i+1]))\n",
    "       self.layers = nn.Sequential(*layers)\n",
    "       \n",
    "       # Final projection with one residual block\n",
    "       self.output_proj = ResidualBlock(hidden_dims[-1], output_dim)\n",
    "       \n",
    "   def forward(self, x):\n",
    "       # x shape: [batch_size, num_tokens, codebook_dim] \n",
    "       x = x.reshape(x.shape[0], -1)  # Flatten tokens\n",
    "       x = self.token_proj(x)\n",
    "       x = self.layers(x)\n",
    "       return self.output_proj(x)\n",
    "\n",
    "class VQVAE(L.LightningModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim=1000, \n",
    "            hidden_dims=[512, 384, 256], \n",
    "            num_tokens=32, \n",
    "            codebook_size=1024, \n",
    "            codebook_dim=8,\n",
    "            commitment_weight=0.25,\n",
    "            quantizer_decay=0.99,\n",
    "            learning_rate=3e-4,\n",
    "            weight_decay=0.01\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_dim, hidden_dims, num_tokens, codebook_dim)\n",
    "        self.decoder = Decoder(input_dim, hidden_dims[::-1], num_tokens, codebook_dim)\n",
    "        self.quantizer = VectorQuantize(\n",
    "                dim=codebook_dim,\n",
    "                codebook_size=codebook_size,\n",
    "                decay=quantizer_decay,\n",
    "                commitment_weight=commitment_weight\n",
    "                )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, indices, commitment_loss = self.quantizer(z)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, commitment_loss, indices\n",
    "    \n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        _, indices, _ = self.quantizer(z)\n",
    "        return indices\n",
    "        \n",
    "    def decode(self, indices):\n",
    "        z_q = self.quantizer.get_codes_from_indices(indices)\n",
    "        return self.decoder(z_q)\n",
    "    \n",
    "class MultiModalMLP(L.LightningModule):\n",
    "    def __init__(self, video_dim, audio_dim, text_dim, hidden_dim, num_tokens, fmri_tok_dir, learning_rate, weight_decay):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dimensions for each modality\n",
    "        self.video_dim = video_dim \n",
    "        self.audio_dim = audio_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.fmri_tokenizer = VQVAE.load_from_checkpoint(fmri_tok_dir)\n",
    "        \n",
    "        for param in self.fmri_tokenizer.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.fmri_tokenizer.eval()\n",
    "        \n",
    "        # Trainable token for missing text\n",
    "        self.missing_text_token = nn.Parameter(torch.randn(1, text_dim))\n",
    "        \n",
    "        # MLP layers\n",
    "        total_dim = video_dim + audio_dim + text_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(total_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_tokens)\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, video, audio, text):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video: [batch_size, video_dim]\n",
    "            audio: [batch_size, audio_dim] \n",
    "            text: [batch_size, text_dim]\n",
    "        \"\"\"\n",
    "        batch_size = video.shape[0]\n",
    "        # Generate text mask based on NaN values\n",
    "        text_mask = ~torch.isnan(text).any(dim=1, keepdim=True)\n",
    "        \n",
    "        # Replace missing text with learned token\n",
    "        text = torch.where(\n",
    "            text_mask,\n",
    "            text,\n",
    "            self.missing_text_token.expand(batch_size, -1)\n",
    "        )\n",
    "        \n",
    "        # Concatenate all features\n",
    "        text = text.unsqueeze(1)\n",
    "        x = torch.cat([video, audio, text], dim=-1).squeeze(1)\n",
    "        # Predict tokens\n",
    "        return self.mlp(x)\n",
    "\n",
    "    def calculate_metrics(self, x, x_recon):\n",
    "        # Flatten the tensors for correlation calculation\n",
    "        x_flat = x.reshape(x.shape[0], -1)\n",
    "        x_recon_flat = x_recon.reshape(x_recon.shape[0], -1)\n",
    "        \n",
    "        # Calculate Pearson R for each sample in batch\n",
    "        correlations = torch.stack([\n",
    "            pearson_corrcoef(x_flat[i], x_recon_flat[i])\n",
    "            for i in range(x_flat.shape[0])\n",
    "        ])\n",
    "        avg_pearson_r = correlations.mean()\n",
    "        \n",
    "        # Calculate variance explained\n",
    "        total_variance = torch.var(x_flat, dim=1).sum()\n",
    "        residual_variance = torch.var(x_flat - x_recon_flat, dim=1).sum()\n",
    "        variance_explained = 1 - (residual_variance / total_variance)\n",
    "        \n",
    "        # Calculate MSE and MAE\n",
    "        mse = F.mse_loss(x_flat, x_recon_flat)\n",
    "        mae = F.l1_loss(x_flat, x_recon_flat)\n",
    "        \n",
    "        return avg_pearson_r, variance_explained, mse, mae\n",
    "        \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        video, audio, text, fmri = batch['video'], batch['audio'], batch['language'], batch['fmri']\n",
    "        logits = self(video, audio, text)\n",
    "        fmri_tokens = self.fmri_tokenizer.encode(fmri).to(dtype=logits.dtype)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(logits, fmri_tokens)\n",
    "        recon_fmri = self.fmri_tokenizer.decode(logits.round().long())\n",
    "\n",
    "        avg_pearson_r, variance_explained, mse, mae = self.calculate_metrics(fmri, recon_fmri)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_pearson_r', avg_pearson_r)\n",
    "        self.log('train_variance_explained', variance_explained)\n",
    "        self.log('train_mse', mse)\n",
    "        self.log('train_mae', mae)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        video, audio, text, fmri = batch['video'], batch['audio'], batch['language'], batch['fmri']\n",
    "        logits = self(video, audio, text)\n",
    "        fmri_tokens = self.fmri_tokenizer.encode(fmri).to(dtype=logits.dtype)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(logits, fmri_tokens)\n",
    "        val_recon_fmri = self.fmri_tokenizer.decode(logits.round().long())\n",
    "        val_avg_pearson_r, val_variance_explained, val_mse, val_mae = self.calculate_metrics(fmri, val_recon_fmri)\n",
    "        \n",
    "        self.log('val_loss', val_loss)\n",
    "        self.log('val_pearson_r', val_avg_pearson_r)\n",
    "        self.log('val_variance_explained', val_variance_explained)\n",
    "        self.log('val_mse', val_mse)\n",
    "        self.log('val_mae', val_mae)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            betas=(0.9, 0.999), \n",
    "            eps=1e-8, \n",
    "            weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "batch_size = 4\n",
    "video_dim = 8192\n",
    "audio_dim = 128\n",
    "text_dim = 768\n",
    "hidden_dim = 1024\n",
    "num_tokens = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.01 \n",
    "num_workers = 4\n",
    "fmri_tok_dir = \"/home/pranav/mihir/algonauts_challenge/algonauts2025/checkpoints/fmri_vqvae_res_gelu_8qdim_1024/epoch=44_val_loss=0.137.ckpt\"\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  129516\n",
      "Validation samples:  22924\n"
     ]
    }
   ],
   "source": [
    "print(\"Training samples: \", len(train_ds))\n",
    "print(\"Validation samples: \", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "run_name = \"MM_MLP_1024dim_sub01\"\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"algonauts2025\",\n",
    "    name=run_name,\n",
    "    save_dir=\"wandb_logs/\"\n",
    ")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    precision=32,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            dirpath=f'checkpoints/{run_name}',\n",
    "            filename='{epoch:02d}_{val_loss:.3f}',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=1,\n",
    "            save_last=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name           | Type       | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | fmri_tokenizer | VQVAE      | 5.4 M  | eval \n",
      "1 | mlp            | Sequential | 10.4 M | train\n",
      "  | other params   | n/a        | 768    | n/a  \n",
      "------------------------------------------------------\n",
      "10.4 M    Trainable params\n",
      "5.4 M     Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.157    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "79        Modules in eval mode\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "torch.set_float32_matmul_precision('high')\n",
    "mm_mlp = MultiModalMLP(\n",
    "    video_dim=video_dim,\n",
    "    audio_dim=audio_dim,\n",
    "    text_dim=text_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_tokens=num_tokens,\n",
    "    fmri_tok_dir=fmri_tok_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "summary = ModelSummary(mm_mlp, max_depth=1)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmihir-neal\u001b[0m (\u001b[33mmihirneal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb_logs/wandb/run-20250128_120709-kebftq02</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mihirneal/algonauts2025/runs/kebftq02' target=\"_blank\">MM_MLP_1024dim_sub01</a></strong> to <a href='https://wandb.ai/mihirneal/algonauts2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mihirneal/algonauts2025' target=\"_blank\">https://wandb.ai/mihirneal/algonauts2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mihirneal/algonauts2025/runs/kebftq02' target=\"_blank\">https://wandb.ai/mihirneal/algonauts2025/runs/kebftq02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type       | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | fmri_tokenizer | VQVAE      | 5.4 M  | eval \n",
      "1 | mlp            | Sequential | 10.4 M | train\n",
      "  | other params   | n/a        | 768    | n/a  \n",
      "------------------------------------------------------\n",
      "10.4 M    Trainable params\n",
      "5.4 M     Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.157    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "79        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f63b3f74bf6429292e0840bd3bfb8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7b2caa2b1442f6931544006918cf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/miniconda3/envs/fmri/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float32).\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(mm_mlp, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
