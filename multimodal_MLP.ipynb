{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "from torchmetrics.functional import pearson_corrcoef\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import preprocess_features, count_total_files, load_fmri, perform_pca, align_features_and_fmri_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlgonautsDataset(Dataset):\n",
    "#     def __init__(self, features_dir, fmri_dir, movies, subjects, excluded_samples_start=5, excluded_samples_end=5, hrf_delay=3, stimulus_window=5):\n",
    "#         self.features_dir = features_dir\n",
    "#         self.fmri_dir = fmri_dir\n",
    "#         self.movies = movies\n",
    "#         self.subjects = subjects\n",
    "#         self.excluded_samples_start = excluded_samples_start\n",
    "#         self.excluded_samples_end = excluded_samples_end\n",
    "#         self.hrf_delay = hrf_delay\n",
    "#         self.stimulus_window = stimulus_window\n",
    "#         self.partition_indices = defaultdict(list)\n",
    "        \n",
    "#         audio_data, video_data, language_data, fmri_data = [], [], [], []\n",
    "#         current_idx = 0\n",
    "        \n",
    "#         # Load fMRI data for current subject\n",
    "#         for subject in self.subjects:\n",
    "#             fmri_dict = load_fmri(self.fmri_dir, subject)\n",
    "        \n",
    "#         total_files = count_total_files(self.features_dir, self.movies)\n",
    "#         pbar = tqdm(desc='Loading dataset', total=total_files)\n",
    "        \n",
    "#         for movie in self.movies:\n",
    "#             start_idx = current_idx\n",
    "#             test_arr = []\n",
    "#             fmri_test = []\n",
    "#             if 'friends' in movie:\n",
    "#                 season = movie.split('-')[1]\n",
    "#                 dir_list = sorted(os.listdir(self.features_dir + 'audio')) #List of all audio for each subset of dataset\n",
    "#                 for episode in dir_list:\n",
    "#                     if f\"{season}e\" in episode and '_features_' in episode:\n",
    "#                         episode_base = episode.split('_features_')[0] # friends_s01e01 and so on....\n",
    "                        \n",
    "#                         features = {'audio': None, 'visual': None, 'language': None}\n",
    "                        \n",
    "#                         # Load all modalities\n",
    "#                         for modality in ['audio', 'visual', 'language']:\n",
    "#                             with h5py.File(os.path.join(self.features_dir, modality, f\"{episode_base}_features_{modality}.h5\"), 'r') as f:\n",
    "#                                 try:\n",
    "#                                     key = 'language_pooler_output' if modality == 'language' else modality\n",
    "#                                     st_season_episode = episode_base.split('_')[1]\n",
    "#                                     test_arr.append(st_season_episode)\n",
    "#                                     features[modality] = f[st_season_episode][key][:]\n",
    "#                                 except:\n",
    "#                                     f.visit(lambda x: print(x))\n",
    "                                    \n",
    "#                         # Get fMRI data\n",
    "#                         try:\n",
    "#                             fmri_season_episode = episode_base.split(\"_\")[1]\n",
    "#                             fmri = fmri_dict[fmri_season_episode]\n",
    "#                             assert fmri_season_episode == st_season_episode\n",
    "#                         except:\n",
    "#                             print(fmri_dict.keys())\n",
    "#                             print(\"Current key:\", episode_base.split(\"_\")[1])\n",
    "                            \n",
    "#                         # Align features with fMRI using sliding window\n",
    "#                         if all(v is not None for v in features.values()):\n",
    "#                             valid_fmri = fmri[excluded_samples_start:-excluded_samples_end]\n",
    "#                             for s in range(len(valid_fmri)):\n",
    "#                                 aligned_features = []\n",
    "                                \n",
    "#                                 # Handle audio and video with sliding window\n",
    "#                                 for mod in ['audio', 'visual']:\n",
    "#                                     if s < (stimulus_window + hrf_delay):\n",
    "#                                         idx_start = excluded_samples_start\n",
    "#                                         idx_end = idx_start + stimulus_window\n",
    "#                                     else:\n",
    "#                                         idx_start = s + excluded_samples_start - hrf_delay - stimulus_window + 1\n",
    "#                                         idx_end = idx_start + stimulus_window\n",
    "                                        \n",
    "#                                     if idx_end > len(features[mod]):\n",
    "#                                         idx_end = len(features[mod])\n",
    "#                                         idx_start = idx_end - stimulus_window\n",
    "                                        \n",
    "#                                     feat = features[mod][idx_start:idx_end]\n",
    "#                                     aligned_features.append(feat)\n",
    "                                \n",
    "#                                 # Handle language features\n",
    "#                                 idx = s + excluded_samples_start - hrf_delay\n",
    "#                                 if idx >= len(features['language']):\n",
    "#                                     lang_feat = features['language'][-1]\n",
    "#                                 else:\n",
    "#                                     lang_feat = features['language'][idx]\n",
    "#                                 aligned_features.append(lang_feat)\n",
    "                                \n",
    "#                                 # Store aligned features\n",
    "#                                 audio_data.append(aligned_features[0])\n",
    "#                                 video_data.append(aligned_features[1])\n",
    "#                                 language_data.append(aligned_features[2])\n",
    "#                                 fmri_data.append(fmri[s + excluded_samples_start])\n",
    "#                                 current_idx += 1\n",
    "                                \n",
    "#                         pbar.update(1)\n",
    "                        \n",
    "#             else:\n",
    "#                 # Handle movies similarly...\n",
    "#                 movie_name = movie.replace('movie10-', '')\n",
    "#                 partitions = sorted([f for f in os.listdir(self.features_dir + 'audio') if movie_name in f and '_features_' in f])\n",
    "                \n",
    "#                 for partition in partitions:\n",
    "#                     partition_base = partition.split('_features_')[0]\n",
    "                    \n",
    "#                     features = {'audio': None, 'visual': None, 'language': None}\n",
    "                    \n",
    "#                     # Load features\n",
    "#                     for modality in ['audio', 'visual', 'language']:\n",
    "#                         with h5py.File(os.path.join(self.features_dir, modality, f\"{partition_base}_features_{modality}.h5\"), 'r') as f:\n",
    "#                             try:\n",
    "#                                 key = 'language_pooler_output' if modality == 'language' else modality\n",
    "#                                 features[modality] = f[partition_base][key][:]\n",
    "#                                 test_arr.append(partition_base)\n",
    "#                             except:\n",
    "#                                 f.visit(lambda x: print(x))\n",
    "                                \n",
    "#                     # Get fMRI data\n",
    "#                     fmri = fmri_dict[partition_base]\n",
    "#                     fmri_test.append(partition_base)\n",
    "#                     # Align features with fMRI\n",
    "#                     if all(v is not None for v in features.values()):\n",
    "#                         valid_fmri = fmri[excluded_samples_start:-excluded_samples_end]\n",
    "#                         for s in range(len(valid_fmri)):\n",
    "#                             aligned_features = []\n",
    "                            \n",
    "#                             # Handle audio and video\n",
    "#                             for mod in ['audio', 'visual']:\n",
    "#                                 if s < (stimulus_window + hrf_delay):\n",
    "#                                     idx_start = excluded_samples_start\n",
    "#                                     idx_end = idx_start + stimulus_window\n",
    "#                                 else:\n",
    "#                                     idx_start = s + excluded_samples_start - hrf_delay - stimulus_window + 1\n",
    "#                                     idx_end = idx_start + stimulus_window\n",
    "                                    \n",
    "#                                 if idx_end > len(features[mod]):\n",
    "#                                     idx_end = len(features[mod])\n",
    "#                                     idx_start = idx_end - stimulus_window\n",
    "                                    \n",
    "#                                 feat = features[mod][idx_start:idx_end]\n",
    "#                                 aligned_features.append(feat)\n",
    "                            \n",
    "#                             # Handle language\n",
    "#                             idx = s + excluded_samples_start - hrf_delay\n",
    "#                             if idx >= len(features['language']):\n",
    "#                                 lang_feat = features['language'][-1]\n",
    "#                             else:\n",
    "#                                 lang_feat = features['language'][idx]\n",
    "#                             aligned_features.append(lang_feat)\n",
    "                            \n",
    "#                             # Store aligned features\n",
    "#                             audio_data.append(aligned_features[0])\n",
    "#                             video_data.append(aligned_features[1])\n",
    "#                             language_data.append(aligned_features[2])\n",
    "#                             fmri_data.append(fmri[s + excluded_samples_start])\n",
    "#                             current_idx += 1\n",
    "                            \n",
    "#                     pbar.update(1)\n",
    "            \n",
    "#             self.partition_indices[movie] = (start_idx, current_idx)\n",
    "#         pbar.close()\n",
    "#         # Convert to tensors\n",
    "\n",
    "#         video_preproc = preprocess_features(np.stack(video_data).reshape(-1, 8192))\n",
    "#         language_preproc = preprocess_features(np.stack(language_data).reshape(-1, 768))\n",
    "#         print(\"Video shape: \", video_preproc.shape)\n",
    "#         print(\"Language shape: \", language_preproc.shape)\n",
    "\n",
    "#         video_pca = perform_pca(video_preproc, n_components=250)\n",
    "#         language_pca = perform_pca(language_preproc, n_components=250)\n",
    "#         print(\"Video shape: \", video_pca.shape)\n",
    "#         print(\"Language shape: \", language_pca.shape)\n",
    "#         import sys; sys.exit()\n",
    "\n",
    "#         self.audio = torch.from_numpy(np.stack(audio_data))\n",
    "#         self.video = torch.from_numpy(video_pca)\n",
    "#         self.language = torch.from_numpy(language_pca)\n",
    "#         self.fmri = torch.from_numpy(np.stack(fmri_data))\n",
    "        \n",
    "#         # Verify loading order\n",
    "#         for movie in self.movies:\n",
    "#             start, end = self.partition_indices[movie]\n",
    "#             assert start < end, f\"Invalid index range for {movie}\"\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return len(self.audio)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'audio': self.audio[idx],\n",
    "#             'video': self.video[idx],\n",
    "#             'language': self.language[idx],\n",
    "#             'fmri': self.fmri[idx]\n",
    "#         }\n",
    "        \n",
    "#     def get_partition_indices(self):\n",
    "#         return self.partition_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = '1'\n",
    "fmri_dir = '/home/pranav/mihir/algonauts_challenge/algonauts_2025.competitors/fmri/'\n",
    "fmri = load_fmri(fmri_dir, subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1 fMRI movies splits name and shape:\n",
      "s01e02a (482, 1000)\n",
      "s01e02b (482, 1000)\n",
      "s01e03a (472, 1000)\n",
      "s01e03b (472, 1000)\n",
      "s01e04a (503, 1000)\n",
      "s01e04b (503, 1000)\n",
      "s01e05a (468, 1000)\n",
      "s01e05b (468, 1000)\n",
      "s01e01a (592, 1000)\n",
      "s01e01b (592, 1000)\n",
      "s01e06a (488, 1000)\n",
      "s01e06b (488, 1000)\n",
      "s01e07a (493, 1000)\n",
      "s01e07b (493, 1000)\n",
      "s01e08a (476, 1000)\n",
      "s01e08b (476, 1000)\n",
      "s01e09a (468, 1000)\n",
      "s01e09b (468, 1000)\n",
      "s01e10a (496, 1000)\n",
      "s01e10b (496, 1000)\n",
      "s01e11a (477, 1000)\n",
      "s01e11b (477, 1000)\n",
      "s01e12a (472, 1000)\n",
      "s01e12b (471, 1000)\n",
      "s01e13a (468, 1000)\n",
      "s01e13b (468, 1000)\n",
      "s01e14a (491, 1000)\n",
      "s01e14b (491, 1000)\n",
      "s01e15a (477, 1000)\n",
      "s01e15b (477, 1000)\n",
      "s01e16a (483, 1000)\n",
      "s01e16b (483, 1000)\n",
      "s01e17a (483, 1000)\n",
      "s01e17b (483, 1000)\n",
      "s01e18a (497, 1000)\n",
      "s01e18b (497, 1000)\n",
      "s01e19a (460, 1000)\n",
      "s01e19b (460, 1000)\n",
      "s01e20a (460, 1000)\n",
      "s01e20b (460, 1000)\n",
      "s01e21a (477, 1000)\n",
      "s01e21b (477, 1000)\n",
      "s01e22a (474, 1000)\n",
      "s01e22b (474, 1000)\n",
      "s01e23a (462, 1000)\n",
      "s01e23b (462, 1000)\n",
      "s01e24a (515, 1000)\n",
      "s01e24b (515, 1000)\n",
      "s02e01a (477, 1000)\n",
      "s02e01b (477, 1000)\n",
      "s02e02a (450, 1000)\n",
      "s02e02b (450, 1000)\n",
      "s02e03a (464, 1000)\n",
      "s02e03b (464, 1000)\n",
      "s02e04a (454, 1000)\n",
      "s02e04b (454, 1000)\n",
      "s02e05a (455, 1000)\n",
      "s02e05b (455, 1000)\n",
      "s02e06a (500, 1000)\n",
      "s02e06b (500, 1000)\n",
      "s02e07a (496, 1000)\n",
      "s02e07b (496, 1000)\n",
      "s02e08a (458, 1000)\n",
      "s02e08b (458, 1000)\n",
      "s02e09a (452, 1000)\n",
      "s02e09b (452, 1000)\n",
      "s02e10a (479, 1000)\n",
      "s02e10b (479, 1000)\n",
      "s02e11a (488, 1000)\n",
      "s02e11b (488, 1000)\n",
      "s02e12a (488, 1000)\n",
      "s02e12b (488, 1000)\n",
      "s02e13a (488, 1000)\n",
      "s02e13b (488, 1000)\n",
      "s02e14a (474, 1000)\n",
      "s02e14b (474, 1000)\n",
      "s02e15a (466, 1000)\n",
      "s02e15b (466, 1000)\n",
      "s02e16a (478, 1000)\n",
      "s02e16b (478, 1000)\n",
      "s02e17a (489, 1000)\n",
      "s02e17b (489, 1000)\n",
      "s02e18a (460, 1000)\n",
      "s02e18b (460, 1000)\n",
      "s02e19a (478, 1000)\n",
      "s02e19b (478, 1000)\n",
      "s02e20a (449, 1000)\n",
      "s02e20b (449, 1000)\n",
      "s02e21a (470, 1000)\n",
      "s02e21b (470, 1000)\n",
      "s02e22a (453, 1000)\n",
      "s02e22b (453, 1000)\n",
      "s02e23a (456, 1000)\n",
      "s02e23b (456, 1000)\n",
      "s02e24a (469, 1000)\n",
      "s02e24b (469, 1000)\n",
      "s03e01a (491, 1000)\n",
      "s03e01b (491, 1000)\n",
      "s03e02a (475, 1000)\n",
      "s03e02b (475, 1000)\n",
      "s03e03a (454, 1000)\n",
      "s03e03b (454, 1000)\n",
      "s03e04a (473, 1000)\n",
      "s03e04b (473, 1000)\n",
      "s03e05a (484, 1000)\n",
      "s03e05b (484, 1000)\n",
      "s03e06a (470, 1000)\n",
      "s03e06b (470, 1000)\n",
      "s03e07a (467, 1000)\n",
      "s03e07b (467, 1000)\n",
      "s03e08a (460, 1000)\n",
      "s03e08b (460, 1000)\n",
      "s03e09a (493, 1000)\n",
      "s03e09b (493, 1000)\n",
      "s03e10a (451, 1000)\n",
      "s03e10b (451, 1000)\n",
      "s03e11a (457, 1000)\n",
      "s03e11b (456, 1000)\n",
      "s03e12a (464, 1000)\n",
      "s03e12b (464, 1000)\n",
      "s03e13a (474, 1000)\n",
      "s03e13b (473, 1000)\n",
      "s03e14a (458, 1000)\n",
      "s03e14b (458, 1000)\n",
      "s03e15a (458, 1000)\n",
      "s03e15b (458, 1000)\n",
      "s03e16a (468, 1000)\n",
      "s03e16b (468, 1000)\n",
      "s03e17a (454, 1000)\n",
      "s03e17b (453, 1000)\n",
      "s03e18a (469, 1000)\n",
      "s03e18b (469, 1000)\n",
      "s03e19a (468, 1000)\n",
      "s03e19b (468, 1000)\n",
      "s03e20a (464, 1000)\n",
      "s03e20b (464, 1000)\n",
      "s03e21a (460, 1000)\n",
      "s03e21b (460, 1000)\n",
      "s03e22a (463, 1000)\n",
      "s03e22b (463, 1000)\n",
      "s03e23a (470, 1000)\n",
      "s03e23b (470, 1000)\n",
      "s03e24a (454, 1000)\n",
      "s03e24b (454, 1000)\n",
      "s03e25a (479, 1000)\n",
      "s03e25b (479, 1000)\n",
      "s04e01a (468, 1000)\n",
      "s04e01b (468, 1000)\n",
      "s04e02a (478, 1000)\n",
      "s04e02b (478, 1000)\n",
      "s04e03a (445, 1000)\n",
      "s04e03b (445, 1000)\n",
      "s04e04a (453, 1000)\n",
      "s04e04b (453, 1000)\n",
      "s04e05a (471, 1000)\n",
      "s04e05b (471, 1000)\n",
      "s04e06a (465, 1000)\n",
      "s04e06b (465, 1000)\n",
      "s04e07a (497, 1000)\n",
      "s04e07b (497, 1000)\n",
      "s04e08a (503, 1000)\n",
      "s04e08b (503, 1000)\n",
      "s04e09a (441, 1000)\n",
      "s04e09b (441, 1000)\n",
      "s04e10a (449, 1000)\n",
      "s04e10b (449, 1000)\n",
      "s04e11a (483, 1000)\n",
      "s04e11b (483, 1000)\n",
      "s04e12a (466, 1000)\n",
      "s04e12b (466, 1000)\n",
      "s04e13a (448, 1000)\n",
      "s04e13b (448, 1000)\n",
      "s04e14a (447, 1000)\n",
      "s04e14b (447, 1000)\n",
      "s04e15a (444, 1000)\n",
      "s04e15b (444, 1000)\n",
      "s04e16a (487, 1000)\n",
      "s04e16b (487, 1000)\n",
      "s04e17a (478, 1000)\n",
      "s04e17b (478, 1000)\n",
      "s04e18a (450, 1000)\n",
      "s04e18b (450, 1000)\n",
      "s04e19a (437, 1000)\n",
      "s04e19b (437, 1000)\n",
      "s04e20a (447, 1000)\n",
      "s04e20b (447, 1000)\n",
      "s04e21a (448, 1000)\n",
      "s04e21b (448, 1000)\n",
      "s04e22a (459, 1000)\n",
      "s04e22b (459, 1000)\n",
      "s04e23a (505, 1000)\n",
      "s04e23b (504, 1000)\n",
      "s04e23c (504, 1000)\n",
      "s04e23d (504, 1000)\n",
      "s05e01a (439, 1000)\n",
      "s05e01b (474, 1000)\n",
      "s05e02a (495, 1000)\n",
      "s05e02b (531, 1000)\n",
      "s05e03a (444, 1000)\n",
      "s05e03b (474, 1000)\n",
      "s05e04a (488, 1000)\n",
      "s05e04b (524, 1000)\n",
      "s05e05a (486, 1000)\n",
      "s05e05b (521, 1000)\n",
      "s05e06a (459, 1000)\n",
      "s05e06b (493, 1000)\n",
      "s05e07a (459, 1000)\n",
      "s05e07b (493, 1000)\n",
      "s05e08a (446, 1000)\n",
      "s05e08b (480, 1000)\n",
      "s05e09a (456, 1000)\n",
      "s05e09b (490, 1000)\n",
      "s05e10a (450, 1000)\n",
      "s05e10b (484, 1000)\n",
      "s05e11a (476, 1000)\n",
      "s05e11b (511, 1000)\n",
      "s05e12a (445, 1000)\n",
      "s05e12b (480, 1000)\n",
      "s05e13a (460, 1000)\n",
      "s05e13b (494, 1000)\n",
      "s05e14a (486, 1000)\n",
      "s05e14b (520, 1000)\n",
      "s05e15a (443, 1000)\n",
      "s05e15b (477, 1000)\n",
      "s05e16a (485, 1000)\n",
      "s05e16b (519, 1000)\n",
      "s05e17a (452, 1000)\n",
      "s05e17b (486, 1000)\n",
      "s05e18a (501, 1000)\n",
      "s05e18b (535, 1000)\n",
      "s05e19a (446, 1000)\n",
      "s05e19b (480, 1000)\n",
      "s05e20a (456, 1000)\n",
      "s05e20b (490, 1000)\n",
      "s05e21a (444, 1000)\n",
      "s05e21b (478, 1000)\n",
      "s05e22a (450, 1000)\n",
      "s05e22b (484, 1000)\n",
      "s05e23a (454, 1000)\n",
      "s05e23b (458, 1000)\n",
      "s05e23c (458, 1000)\n",
      "s05e23d (488, 1000)\n",
      "s06e01a (465, 1000)\n",
      "s06e01b (499, 1000)\n",
      "s06e02a (453, 1000)\n",
      "s06e02b (487, 1000)\n",
      "s06e03a (439, 1000)\n",
      "s06e03b (473, 1000)\n",
      "s06e04a (439, 1000)\n",
      "s06e04b (474, 1000)\n",
      "s06e05a (452, 1000)\n",
      "s06e05b (486, 1000)\n",
      "s06e06a (443, 1000)\n",
      "s06e06b (478, 1000)\n",
      "s06e07a (445, 1000)\n",
      "s06e07b (479, 1000)\n",
      "s06e08a (438, 1000)\n",
      "s06e08b (472, 1000)\n",
      "s06e09a (504, 1000)\n",
      "s06e09b (537, 1000)\n",
      "s06e10a (466, 1000)\n",
      "s06e10b (500, 1000)\n",
      "s06e11a (450, 1000)\n",
      "s06e11b (484, 1000)\n",
      "s06e12a (435, 1000)\n",
      "s06e12b (470, 1000)\n",
      "s06e13a (470, 1000)\n",
      "s06e13b (504, 1000)\n",
      "s06e14a (461, 1000)\n",
      "s06e14b (495, 1000)\n",
      "s06e15a (456, 1000)\n",
      "s06e15b (460, 1000)\n",
      "s06e15c (460, 1000)\n",
      "s06e15d (490, 1000)\n",
      "s06e17a (428, 1000)\n",
      "s06e17b (462, 1000)\n",
      "s06e18a (435, 1000)\n",
      "s06e18b (470, 1000)\n",
      "s06e19a (434, 1000)\n",
      "s06e19b (468, 1000)\n",
      "s06e20a (431, 1000)\n",
      "s06e20b (465, 1000)\n",
      "s06e21a (460, 1000)\n",
      "s06e21b (494, 1000)\n",
      "s06e22a (452, 1000)\n",
      "s06e22b (486, 1000)\n",
      "s06e23a (486, 1000)\n",
      "s06e23b (520, 1000)\n",
      "s06e24a (457, 1000)\n",
      "s06e24b (461, 1000)\n",
      "s06e24c (461, 1000)\n",
      "s06e24d (490, 1000)\n",
      "bourne01 (403, 1000)\n",
      "bourne02 (405, 1000)\n",
      "bourne03 (405, 1000)\n",
      "bourne04 (405, 1000)\n",
      "bourne05 (405, 1000)\n",
      "bourne06 (405, 1000)\n",
      "bourne07 (405, 1000)\n",
      "bourne08 (405, 1000)\n",
      "bourne09 (405, 1000)\n",
      "bourne10 (380, 1000)\n",
      "wolf01 (406, 1000)\n",
      "wolf02 (406, 1000)\n",
      "wolf03 (406, 1000)\n",
      "wolf04 (406, 1000)\n",
      "wolf05 (406, 1000)\n",
      "wolf06 (406, 1000)\n",
      "wolf07 (406, 1000)\n",
      "wolf08 (406, 1000)\n",
      "wolf09 (406, 1000)\n",
      "wolf10 (406, 1000)\n",
      "wolf11 (406, 1000)\n",
      "wolf12 (406, 1000)\n",
      "wolf13 (406, 1000)\n",
      "wolf14 (406, 1000)\n",
      "wolf15 (406, 1000)\n",
      "wolf16 (405, 1000)\n",
      "wolf17 (498, 1000)\n",
      "figures01 (402, 1000)\n",
      "figures02 (409, 1000)\n",
      "figures03 (410, 1000)\n",
      "figures04 (409, 1000)\n",
      "figures05 (408, 1000)\n",
      "figures06 (408, 1000)\n",
      "figures07 (408, 1000)\n",
      "figures08 (409, 1000)\n",
      "figures09 (409, 1000)\n",
      "figures10 (409, 1000)\n",
      "figures11 (409, 1000)\n",
      "figures12 (373, 1000)\n",
      "life01 (406, 1000)\n",
      "life02 (406, 1000)\n",
      "life03 (406, 1000)\n",
      "life04 (406, 1000)\n",
      "life05 (384, 1000)\n",
      "(155800, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Subject {subject} fMRI movies splits name and shape:\")\n",
    "[print(f\"{k} {v.shape}\") or v for k,v in fmri.items()]; print(np.concatenate([v for k,v in fmri.items()], axis=0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgonautsDataset(Dataset):\n",
    "    def __init__(self, features_dir, fmri_dir, movies, subject, excluded_samples_start=5, excluded_samples_end=5, hrf_delay=3, stimulus_window=5):\n",
    "        self.features_dir = features_dir\n",
    "        self.fmri_dir = fmri_dir\n",
    "        self.movies = movies\n",
    "        self.subject = subject\n",
    "        self.excluded_samples_start = excluded_samples_start\n",
    "        self.excluded_samples_end = excluded_samples_end\n",
    "        self.hrf_delay = hrf_delay\n",
    "        self.stimulus_window = stimulus_window\n",
    "        self.partition_indices = defaultdict(list)\n",
    "        \n",
    "        # First load all raw features\n",
    "        stimuli_features = {\"visual\": {}, \"audio\": {}, \"language\": {}}\n",
    "        raw_audio, raw_video = [], []\n",
    "        raw_language = []\n",
    "        \n",
    "        \n",
    "        total_files = count_total_files(self.features_dir, self.movies)\n",
    "        pbar = tqdm(desc='Loading dataset', total=total_files)\n",
    "        \n",
    "        # Load audio and video features first\n",
    "        for movie in self.movies:\n",
    "            if 'friends' in movie:\n",
    "                season = movie.split('-')[1]\n",
    "                dir_list = sorted(os.listdir(self.features_dir + 'audio')) #List of all audio for each subset of dataset\n",
    "                for episode in dir_list:\n",
    "                    if f\"{season}e\" in episode and '_features_' in episode:\n",
    "                        episode_base = episode.split('_features_')[0] # friends_s01e01 and so on....\n",
    "                        \n",
    "                        # Load audio and video features\n",
    "                        features = {'audio': None, 'visual': None}\n",
    "                        \n",
    "                        for modality in ['audio', 'visual']:\n",
    "                            with h5py.File(os.path.join(self.features_dir, modality, f\"{episode_base}_features_{modality}.h5\"), 'r') as f:\n",
    "                                try:\n",
    "                                    stimuli_features[modality][episode_base.split('_')[1]] = f[episode_base.split('_')[1]][modality][:]\n",
    "                                except:\n",
    "                                    f.visit(lambda x: print(x))\n",
    "                                    \n",
    "                        # if all(v is not None for v in features.values()):\n",
    "                        #     raw_audio.extend(features['audio'])\n",
    "                        #     raw_video.extend(features['visual'])\n",
    "                        # else:\n",
    "                        #     print(f\"Could not load features for {episode_base}\")\n",
    "\n",
    "\n",
    "                lang_dir_list = sorted(os.listdir(self.features_dir + 'language'))\n",
    "                for episode in lang_dir_list:\n",
    "                    if f\"{season}e\" in episode and '_features_' in episode:\n",
    "                        episode_base = episode.split('_features_')[0]\n",
    "                        \n",
    "                        with h5py.File(os.path.join(self.features_dir, 'language', f\"{episode_base}_features_language.h5\"), 'r') as f:\n",
    "                            try:\n",
    "                                st_season_episode = episode_base.split('_')[1]\n",
    "                                stimuli_features['language'][st_season_episode] = f[st_season_episode]['language_pooler_output'][:]\n",
    "                            except:\n",
    "                                f.visit(lambda x: print(x))\n",
    "            else:\n",
    "                movie_name = movie.replace('movie10-', '')\n",
    "                partitions = sorted([f for f in os.listdir(self.features_dir + 'audio') if movie_name in f and '_features_' in f])\n",
    "                \n",
    "                for partition in partitions:\n",
    "                    partition_base = partition.split('_features_')[0]\n",
    "                    \n",
    "                    features = {'audio': None, 'visual': None}\n",
    "                    \n",
    "                    for modality in ['audio', 'visual']:\n",
    "                        with h5py.File(os.path.join(self.features_dir, modality, f\"{partition_base}_features_{modality}.h5\"), 'r') as f:\n",
    "                            try:\n",
    "                                stimuli_features[modality][partition_base] = f[partition_base][modality][:]\n",
    "                            except:\n",
    "                                f.visit(lambda x: print(x))\n",
    "\n",
    "                    # if all(v is not None for v in features.values()):\n",
    "                    #     raw_audio.extend(features['audio'])\n",
    "                    #     raw_video.extend(features['visual'])\n",
    "                    # else:\n",
    "                    #     print(f\"Could not load features for {partition_base}\")  \n",
    "                                \n",
    "                lang_partitions = sorted([f for f in os.listdir(self.features_dir + 'language') if movie_name in f and '_features_' in f])\n",
    "                \n",
    "                for partition in lang_partitions:\n",
    "                    partition_base = partition.split('_features_')[0]\n",
    "                    \n",
    "                    with h5py.File(os.path.join(self.features_dir, 'language', f\"{partition_base}_features_language.h5\"), 'r') as f:\n",
    "                        try:\n",
    "                            stimuli_features['language'][partition_base] = f[partition_base]['language_pooler_output'][:]\n",
    "                        except:\n",
    "                            f.visit(lambda x: print(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        fmri_data = load_fmri(self.fmri_dir, self.subject)\n",
    "\n",
    "        feature_train, fmri_train = align_features_and_fmri_samples(\n",
    "            stimuli_features, \n",
    "            fmri_data, \n",
    "            self.excluded_samples_start, \n",
    "            self.excluded_samples_end, \n",
    "            self.hrf_delay, \n",
    "            self.stimulus_window, \n",
    "            self.movies\n",
    "        )\n",
    "\n",
    "        print(\"Training fMRI responses shape:\")\n",
    "        print(fmri_train.shape)\n",
    "        print('(Train samples × Parcels)')\n",
    "        print(\"\\nTraining stimulus features shape:\")\n",
    "        print(feature_train.shape)\n",
    "        print('(Train samples × Features)')\n",
    "        # print(stimuli_features.keys())\n",
    "        # print(stimuli_features['visual'].keys())\n",
    "        # print(stimuli_features['audio'].keys())\n",
    "        # print(stimuli_features['language'].keys())\n",
    "        # print(f\"Subject {subject} fMRI movies splits name and shape:\")\n",
    "        # for key, value in fmri_data.items():\n",
    "        #     print(key + \" \" + str(value.shape))\n",
    "        \n",
    "        # stimuli_features['visual'] = raw_video\n",
    "        # stimuli_features['audio'] = raw_audio\n",
    "        # stimuli_features['language'] = raw_language\n",
    "        # for key_modality, value_modality in stimuli_features.items():\n",
    "        #     print(f\"\\n{key_modality} features movie splits name and shape:\")\n",
    "        #     for key_movie, value_movie in value_modality.items():\n",
    "        #         print(key_movie + \" \" + str(value_movie.shape))\n",
    "        import sys; sys.exit()\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        print(\"Raw video shape: \", np.stack(stimuli_features['visual']).shape)\n",
    "        print(\"Raw audio shape: \", np.stack(stimuli_features['audio']).shape)\n",
    "        print(\"Raw language shape: \", np.stack(stimuli_features['language']).shape)\n",
    "        print(\"Raw fMRI shape: \", np.stack(fmri_data).shape)\n",
    "        \n",
    "        video_preproc = preprocess_features(np.stack(raw_video))\n",
    "        language_preproc = preprocess_features(np.stack(raw_language))\n",
    "        print(\"Video shape: \", video_preproc.shape)\n",
    "        print(\"Language shape: \", language_preproc.shape)\n",
    "\n",
    "        video_pca = perform_pca(video_preproc, n_components=250)\n",
    "        language_pca = perform_pca(language_preproc, n_components=250)\n",
    "        print(\"Video shape after PCA: \", video_pca.shape)   \n",
    "        print(\"Language shape after PCA: \", language_pca.shape)\n",
    "\n",
    "        # Now align features with fMRI using sliding window and apply exclusion\n",
    "        aligned_audio, aligned_video, aligned_language = [], [], []\n",
    "        valid_fmri = []\n",
    "        \n",
    "        current_pos = 0\n",
    "        for movie in self.movies:\n",
    "            start_idx = current_pos\n",
    "            movie_fmri = fmri_data[start_idx:start_idx+len(raw_audio)]\n",
    "            \n",
    "            # Apply exclusion to fMRI\n",
    "            valid_movie_fmri = movie_fmri[excluded_samples_start:-excluded_samples_end]\n",
    "            valid_fmri.extend(valid_movie_fmri)\n",
    "            \n",
    "            for s in range(len(valid_movie_fmri)):\n",
    "                # Handle audio and video with sliding window\n",
    "                if s < (stimulus_window + hrf_delay):\n",
    "                    idx_start = current_pos\n",
    "                    idx_end = idx_start + stimulus_window\n",
    "                else:\n",
    "                    idx_start = current_pos - hrf_delay - stimulus_window + 1\n",
    "                    idx_end = idx_start + stimulus_window\n",
    "                    \n",
    "                # Align audio features\n",
    "                if idx_end > len(raw_audio):\n",
    "                    idx_end = len(raw_audio)\n",
    "                    idx_start = idx_end - stimulus_window\n",
    "                aligned_audio.append(raw_audio[idx_start:idx_end])\n",
    "                \n",
    "                # Align video features\n",
    "                if idx_end > len(video_pca):\n",
    "                    idx_end = len(video_pca)\n",
    "                    idx_start = idx_end - stimulus_window\n",
    "                aligned_video.append(video_pca[idx_start:idx_end])\n",
    "                \n",
    "                # Handle language features\n",
    "                idx = current_pos - hrf_delay\n",
    "                if idx >= len(language_pca):\n",
    "                    aligned_language.append(language_pca[-1])\n",
    "                else:\n",
    "                    aligned_language.append(language_pca[idx])\n",
    "                    \n",
    "                current_pos += 1\n",
    "            \n",
    "            # Update partition indices after alignment\n",
    "            self.partition_indices[movie] = (start_idx, current_pos)\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.audio = torch.from_numpy(np.stack(aligned_audio))\n",
    "        self.video = torch.from_numpy(np.stack(aligned_video))\n",
    "        self.language = torch.from_numpy(np.stack(aligned_language))\n",
    "        self.fmri = torch.from_numpy(np.stack(valid_fmri))\n",
    "        \n",
    "        # Verify loading order\n",
    "        for movie in self.movies:\n",
    "            start, end = self.partition_indices[movie]\n",
    "            assert start < end, f\"Invalid index range for {movie}\"\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.audio)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'audio': self.audio[idx],\n",
    "            'video': self.video[idx],\n",
    "            'language': self.language[idx],\n",
    "            'fmri': self.fmri[idx]\n",
    "        }\n",
    "        \n",
    "    def get_partition_indices(self):\n",
    "        return self.partition_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fMRI responses shape:\n",
      "(22787, 1000)\n",
      "(Train samples × Parcels)\n",
      "\n",
      "Training stimulus features shape:\n",
      "(22787, 42368)\n",
      "(Train samples × Features)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "features_dir = '/home/pranav/mihir/algonauts_challenge/AlgonautsDS-features/developer_kit/stimulus_features/raw/'\n",
    "fmri_dir = '/home/pranav/mihir/algonauts_challenge/algonauts_2025.competitors/fmri/'\n",
    "movies_train = [\"friends-s01\"]\n",
    "# movies_train = [\"friends-s01\", \"friends-s02\", \"friends-s03\", \"friends-s04\", \"friends-s05\", \"movie10-bourne\", \"movie10-figures\", \"movie10-life\", \"movie10-wolf\"]\n",
    "movies_val = [\"friends-s06\"]\n",
    "modality = \"all\"  #@param [\"visual\", \"audio\", \"language\", \"all\"]\n",
    "\n",
    "excluded_samples_start = 5  #@param {type:\"slider\", min:0, max:20, step:1}\n",
    "excluded_samples_end = 5  #@param {type:\"slider\", min:0, max:20, step:1}\n",
    "hrf_delay = 3  #@param {type:\"slider\", min:0, max:10, step:1}\n",
    "stimulus_window = 5\n",
    "\n",
    "subject = 1 #@param [\"1\", \"2\", \"3\", \"5\"] {type:\"raw\", allow-input: true}\n",
    "\n",
    "train_ds = AlgonautsDataset(features_dir, fmri_dir, movies=movies_train, subject=subject, excluded_samples_start=excluded_samples_start, excluded_samples_end=excluded_samples_end, hrf_delay=hrf_delay, stimulus_window=stimulus_window)\n",
    "val_ds = AlgonautsDataset(features_dir, fmri_dir, movies=movies_val, subject=subject, excluded_samples_start=excluded_samples_start, excluded_samples_end=excluded_samples_end, hrf_delay=hrf_delay, stimulus_window=stimulus_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: friends_s01e01a_features_audio.h5, shape: (592, 128)\n",
      "Loading file: friends_s01e01b_features_audio.h5, shape: (591, 128)\n",
      "Loading file: friends_s01e02a_features_audio.h5, shape: (483, 128)\n",
      "Loading file: friends_s01e02b_features_audio.h5, shape: (483, 128)\n",
      "Loading file: friends_s01e03a_features_audio.h5, shape: (473, 128)\n",
      "Loading file: friends_s01e03b_features_audio.h5, shape: (473, 128)\n",
      "Loading file: friends_s01e04a_features_audio.h5, shape: (503, 128)\n",
      "Loading file: friends_s01e04b_features_audio.h5, shape: (503, 128)\n",
      "Loading file: friends_s01e05a_features_audio.h5, shape: (469, 128)\n",
      "Loading file: friends_s01e05b_features_audio.h5, shape: (469, 128)\n",
      "Loading file: friends_s01e06a_features_audio.h5, shape: (489, 128)\n",
      "Loading file: friends_s01e06b_features_audio.h5, shape: (489, 128)\n",
      "Loading file: friends_s01e07a_features_audio.h5, shape: (493, 128)\n",
      "Loading file: friends_s01e07b_features_audio.h5, shape: (493, 128)\n",
      "Loading file: friends_s01e08a_features_audio.h5, shape: (476, 128)\n",
      "Loading file: friends_s01e08b_features_audio.h5, shape: (476, 128)\n",
      "Loading file: friends_s01e09a_features_audio.h5, shape: (468, 128)\n",
      "Loading file: friends_s01e09b_features_audio.h5, shape: (468, 128)\n",
      "Loading file: friends_s01e10a_features_audio.h5, shape: (496, 128)\n",
      "Loading file: friends_s01e10b_features_audio.h5, shape: (496, 128)\n",
      "Loading file: friends_s01e11a_features_audio.h5, shape: (477, 128)\n",
      "Loading file: friends_s01e11b_features_audio.h5, shape: (477, 128)\n",
      "Loading file: friends_s01e12a_features_audio.h5, shape: (472, 128)\n",
      "Loading file: friends_s01e12b_features_audio.h5, shape: (472, 128)\n",
      "Loading file: friends_s01e13a_features_audio.h5, shape: (469, 128)\n",
      "Loading file: friends_s01e13b_features_audio.h5, shape: (468, 128)\n",
      "Loading file: friends_s01e14a_features_audio.h5, shape: (492, 128)\n",
      "Loading file: friends_s01e14b_features_audio.h5, shape: (492, 128)\n",
      "Loading file: friends_s01e15a_features_audio.h5, shape: (477, 128)\n",
      "Loading file: friends_s01e15b_features_audio.h5, shape: (477, 128)\n",
      "Loading file: friends_s01e16a_features_audio.h5, shape: (483, 128)\n",
      "Loading file: friends_s01e16b_features_audio.h5, shape: (483, 128)\n",
      "Loading file: friends_s01e17a_features_audio.h5, shape: (483, 128)\n",
      "Loading file: friends_s01e17b_features_audio.h5, shape: (483, 128)\n",
      "Loading file: friends_s01e18a_features_audio.h5, shape: (497, 128)\n",
      "Loading file: friends_s01e18b_features_audio.h5, shape: (497, 128)\n",
      "Loading file: friends_s01e19a_features_audio.h5, shape: (461, 128)\n",
      "Loading file: friends_s01e19b_features_audio.h5, shape: (461, 128)\n",
      "Loading file: friends_s01e20a_features_audio.h5, shape: (461, 128)\n",
      "Loading file: friends_s01e20b_features_audio.h5, shape: (461, 128)\n",
      "Loading file: friends_s01e21a_features_audio.h5, shape: (477, 128)\n",
      "Loading file: friends_s01e21b_features_audio.h5, shape: (477, 128)\n",
      "Loading file: friends_s01e22a_features_audio.h5, shape: (474, 128)\n",
      "Loading file: friends_s01e22b_features_audio.h5, shape: (474, 128)\n",
      "Loading file: friends_s01e23a_features_audio.h5, shape: (462, 128)\n",
      "Loading file: friends_s01e23b_features_audio.h5, shape: (462, 128)\n",
      "Loading file: friends_s01e24a_features_audio.h5, shape: (515, 128)\n",
      "Loading file: friends_s01e24b_features_audio.h5, shape: (515, 128)\n",
      "Loaded 48 files for friends s01\n",
      "\n",
      "Number of samples in friends s01 language features: 23282\n",
      "Shape of language features: (23282, 128)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "language_dir = '/home/pranav/mihir/algonauts_challenge/AlgonautsDS-features/developer_kit/stimulus_features/raw/audio'\n",
    "\n",
    "# Load all .h5 files for friends s01\n",
    "language_features = []\n",
    "for file in sorted(os.listdir(language_dir)):\n",
    "    if 'friends' in file and 's01' in file and file.endswith('_features_audio.h5'):\n",
    "        \n",
    "        with h5py.File(os.path.join(language_dir, file), 'r') as f:\n",
    "            feature = f[file.split('_')[1]]['audio'][:]\n",
    "            print(f\"Loading file: {file}, shape: {feature.shape}\")\n",
    "            language_features.append(feature)\n",
    "            # Note: This loop will run for each .h5 file in friends s01 that matches the pattern\n",
    "            # Based on the dataset structure, there should be 48 files (one per episode)\n",
    "            # You may want to add a check:\n",
    "if len(language_features) != 48:\n",
    "    print(f\"Warning: Expected 48 files but found {len(language_features)}\")\n",
    "else:\n",
    "    print(f\"Loaded {len(language_features)} files for friends s01\")\n",
    "\n",
    "language_features = np.concatenate(language_features, axis=0)\n",
    "print(f\"\\nNumber of samples in friends s01 language features: {len(language_features)}\")\n",
    "print(f\"Shape of language features: {language_features.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48 files for friends s01\n",
      "\n",
      "Number of samples in friends s01 language features: 23267\n",
      "Shape of language features: (23267, 1000)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/pranav/mihir/algonauts_challenge/algonauts_2025.competitors/fmri/sub-01/func/sub-01_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5\"\n",
    "fmri_features = []\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    key_arr = sorted(list(f.keys()))\n",
    "    for i in key_arr:\n",
    "        if i.split('_')[1].split('-')[1][:3] == 's01':\n",
    "            output = f[i][:]\n",
    "            fmri_features.append(output)\n",
    "\n",
    "if len(fmri_features) != 48:\n",
    "    print(f\"Warning: Expected 48 files but found {len(fmri_features)}\")\n",
    "else:\n",
    "    print(f\"Loaded {len(fmri_features)} files for friends s01\")\n",
    "\n",
    "fmri_features = np.concatenate(fmri_features, axis=0)\n",
    "print(f\"\\nNumber of samples in friends s01 language features: {len(fmri_features)}\")\n",
    "print(f\"Shape of language features: {fmri_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22787\n",
      "22924\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([250])\n",
      "torch.Size([250])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "audio, video, language, fmri = train_ds[0]['audio'], train_ds[0]['video'], train_ds[0]['language'], train_ds[0]['fmri']\n",
    "print(audio.shape)\n",
    "print(video.shape)\n",
    "print(language.shape)\n",
    "print(fmri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 4\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'friends-s01': (0, 22787)})\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.get_partition_indices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ses-001_task-s01e02a', 'ses-001_task-s01e02b', 'ses-001_task-s01e03a', 'ses-001_task-s01e03b', 'ses-002_task-s01e04a', 'ses-002_task-s01e04b', 'ses-002_task-s01e05a', 'ses-002_task-s01e05b', 'ses-003_task-s01e01a', 'ses-003_task-s01e01b', 'ses-003_task-s01e06a', 'ses-003_task-s01e06b', 'ses-004_task-s01e07a', 'ses-004_task-s01e07b', 'ses-004_task-s01e08a', 'ses-004_task-s01e08b', 'ses-004_task-s01e09a', 'ses-004_task-s01e09b', 'ses-005_task-s01e10a', 'ses-005_task-s01e10b', 'ses-005_task-s01e11a', 'ses-005_task-s01e11b', 'ses-006_task-s01e12a', 'ses-006_task-s01e12b', 'ses-006_task-s01e13a', 'ses-006_task-s01e13b', 'ses-006_task-s01e14a', 'ses-006_task-s01e14b', 'ses-007_task-s01e15a', 'ses-007_task-s01e15b', 'ses-007_task-s01e16a', 'ses-007_task-s01e16b', 'ses-007_task-s01e17a', 'ses-007_task-s01e17b', 'ses-008_task-s01e18a', 'ses-008_task-s01e18b', 'ses-008_task-s01e19a', 'ses-008_task-s01e19b', 'ses-009_task-s01e20a', 'ses-009_task-s01e20b', 'ses-009_task-s01e21a', 'ses-009_task-s01e21b', 'ses-009_task-s01e22a', 'ses-009_task-s01e22b', 'ses-010_task-s01e23a', 'ses-010_task-s01e23b', 'ses-010_task-s01e24a', 'ses-010_task-s01e24b', 'ses-010_task-s02e01a', 'ses-010_task-s02e01b', 'ses-011_task-s02e02a', 'ses-011_task-s02e02b', 'ses-011_task-s02e03a', 'ses-011_task-s02e03b', 'ses-011_task-s02e04a', 'ses-011_task-s02e04b', 'ses-011_task-s02e05a', 'ses-011_task-s02e05b', 'ses-012_task-s02e06a', 'ses-012_task-s02e06b', 'ses-012_task-s02e07a', 'ses-012_task-s02e07b', 'ses-013_task-s02e08a', 'ses-013_task-s02e08b', 'ses-013_task-s02e09a', 'ses-013_task-s02e09b', 'ses-013_task-s02e10a', 'ses-013_task-s02e10b', 'ses-014_task-s02e11a', 'ses-014_task-s02e11b', 'ses-014_task-s02e12a', 'ses-014_task-s02e12b', 'ses-014_task-s02e13a', 'ses-014_task-s02e13b', 'ses-015_task-s02e14a', 'ses-015_task-s02e14b', 'ses-015_task-s02e15a', 'ses-015_task-s02e15b', 'ses-016_task-s02e16a', 'ses-016_task-s02e16b', 'ses-016_task-s02e17a', 'ses-016_task-s02e17b', 'ses-017_task-s02e18a', 'ses-017_task-s02e18b', 'ses-017_task-s02e19a', 'ses-017_task-s02e19b', 'ses-017_task-s02e20a', 'ses-017_task-s02e20b', 'ses-017_task-s02e21a', 'ses-017_task-s02e21b', 'ses-017_task-s02e22a', 'ses-017_task-s02e22b', 'ses-018_task-s02e23a', 'ses-018_task-s02e23b', 'ses-018_task-s02e24a', 'ses-018_task-s02e24b', 'ses-019_task-s03e01a', 'ses-019_task-s03e01b', 'ses-019_task-s03e02a', 'ses-019_task-s03e02b', 'ses-019_task-s03e03a', 'ses-019_task-s03e03b', 'ses-020_task-s03e04a', 'ses-020_task-s03e04b', 'ses-020_task-s03e05a', 'ses-020_task-s03e05b', 'ses-020_task-s03e06a', 'ses-020_task-s03e06b', 'ses-021_task-s03e07a', 'ses-021_task-s03e07b', 'ses-021_task-s03e08a', 'ses-021_task-s03e08b', 'ses-021_task-s03e09a', 'ses-021_task-s03e09b', 'ses-022_task-s03e10a', 'ses-022_task-s03e10b', 'ses-022_task-s03e11a', 'ses-022_task-s03e11b', 'ses-022_task-s03e12a', 'ses-022_task-s03e12b', 'ses-023_task-s03e13a', 'ses-023_task-s03e13b', 'ses-023_task-s03e14a', 'ses-023_task-s03e14b', 'ses-024_task-s03e15a', 'ses-024_task-s03e15b', 'ses-024_task-s03e16a', 'ses-024_task-s03e16b', 'ses-024_task-s03e17a', 'ses-024_task-s03e17b', 'ses-025_task-s03e18a', 'ses-025_task-s03e18b', 'ses-025_task-s03e19a', 'ses-025_task-s03e19b', 'ses-025_task-s03e20a', 'ses-025_task-s03e20b', 'ses-026_task-s03e21a', 'ses-026_task-s03e21b', 'ses-026_task-s03e22a', 'ses-026_task-s03e22b', 'ses-027_task-s03e23a', 'ses-027_task-s03e23b', 'ses-027_task-s03e24a', 'ses-027_task-s03e24b', 'ses-027_task-s03e25a', 'ses-027_task-s03e25b', 'ses-027_task-s04e01a', 'ses-027_task-s04e01b', 'ses-027_task-s04e02a', 'ses-027_task-s04e02b', 'ses-028_task-s04e03a', 'ses-028_task-s04e03b', 'ses-028_task-s04e04a', 'ses-028_task-s04e04b', 'ses-028_task-s04e05a', 'ses-028_task-s04e05b', 'ses-028_task-s04e06a', 'ses-028_task-s04e06b', 'ses-028_task-s04e07a', 'ses-028_task-s04e07b', 'ses-029_task-s04e08a', 'ses-029_task-s04e08b', 'ses-029_task-s04e09a', 'ses-029_task-s04e09b', 'ses-029_task-s04e10a', 'ses-029_task-s04e10b', 'ses-029_task-s04e11a', 'ses-029_task-s04e11b', 'ses-030_task-s04e12a', 'ses-030_task-s04e12b', 'ses-030_task-s04e13a', 'ses-030_task-s04e13b', 'ses-030_task-s04e14a', 'ses-030_task-s04e14b', 'ses-031_task-s04e15a', 'ses-031_task-s04e15b', 'ses-031_task-s04e16a', 'ses-031_task-s04e16b', 'ses-031_task-s04e17a', 'ses-031_task-s04e17b', 'ses-032_task-s04e18a', 'ses-032_task-s04e18b', 'ses-032_task-s04e19a', 'ses-032_task-s04e19b', 'ses-033_task-s04e20a', 'ses-033_task-s04e20b', 'ses-034_task-s04e21a', 'ses-034_task-s04e21b', 'ses-034_task-s04e22a', 'ses-034_task-s04e22b', 'ses-035_task-s04e23a', 'ses-035_task-s04e23b', 'ses-035_task-s04e23c', 'ses-035_task-s04e23d', 'ses-036_task-s05e01a', 'ses-036_task-s05e01b', 'ses-036_task-s05e02a', 'ses-036_task-s05e02b', 'ses-037_task-s05e03a', 'ses-037_task-s05e03b', 'ses-037_task-s05e04a', 'ses-037_task-s05e04b', 'ses-041_task-s05e05a', 'ses-041_task-s05e05b', 'ses-041_task-s05e06a', 'ses-041_task-s05e06b', 'ses-042_task-s05e07a', 'ses-042_task-s05e07b', 'ses-043_task-s05e08a', 'ses-043_task-s05e08b', 'ses-044_task-s05e09a', 'ses-044_task-s05e09b', 'ses-044_task-s05e10a', 'ses-044_task-s05e10b', 'ses-045_task-s05e11a', 'ses-045_task-s05e11b', 'ses-045_task-s05e12a', 'ses-045_task-s05e12b', 'ses-046_task-s05e13a', 'ses-046_task-s05e13b', 'ses-046_task-s05e14a', 'ses-046_task-s05e14b', 'ses-047_task-s05e15a', 'ses-047_task-s05e15b', 'ses-047_task-s05e16a', 'ses-047_task-s05e16b', 'ses-048_task-s05e17a', 'ses-048_task-s05e17b', 'ses-048_task-s05e18a', 'ses-048_task-s05e18b', 'ses-049_task-s05e19a', 'ses-049_task-s05e19b', 'ses-049_task-s05e20a', 'ses-049_task-s05e20b', 'ses-050_task-s05e21a', 'ses-050_task-s05e21b', 'ses-050_task-s05e22a', 'ses-050_task-s05e22b', 'ses-051_task-s05e23a', 'ses-051_task-s05e23b', 'ses-052_task-s05e23c', 'ses-052_task-s05e23d', 'ses-052_task-s06e01a', 'ses-052_task-s06e01b', 'ses-053_task-s06e02a', 'ses-053_task-s06e02b', 'ses-053_task-s06e03a', 'ses-053_task-s06e03b', 'ses-053_task-s06e04a', 'ses-054_task-s06e04b', 'ses-054_task-s06e05a', 'ses-054_task-s06e05b', 'ses-055_task-s06e06a', 'ses-055_task-s06e06b', 'ses-055_task-s06e07a', 'ses-055_task-s06e07b', 'ses-056_task-s06e08a', 'ses-056_task-s06e08b', 'ses-056_task-s06e09a', 'ses-056_task-s06e09b', 'ses-056_task-s06e10a', 'ses-057_task-s06e10b', 'ses-057_task-s06e11a', 'ses-057_task-s06e11b', 'ses-057_task-s06e12a', 'ses-057_task-s06e12b', 'ses-058_task-s06e13a', 'ses-058_task-s06e13b', 'ses-058_task-s06e14a', 'ses-058_task-s06e14b', 'ses-059_task-s06e15a', 'ses-060_task-s06e15b', 'ses-060_task-s06e15c', 'ses-061_task-s06e15d', 'ses-061_task-s06e17a', 'ses-061_task-s06e17b', 'ses-062_task-s06e18a', 'ses-062_task-s06e18b', 'ses-062_task-s06e19a', 'ses-062_task-s06e19b', 'ses-063_task-s06e20a', 'ses-063_task-s06e20b', 'ses-063_task-s06e21a', 'ses-064_task-s06e21b', 'ses-064_task-s06e22a', 'ses-064_task-s06e22b', 'ses-065_task-s06e23a', 'ses-065_task-s06e23b', 'ses-065_task-s06e24a', 'ses-066_task-s06e24b', 'ses-066_task-s06e24c', 'ses-066_task-s06e24d']\n"
     ]
    }
   ],
   "source": [
    "file = \"/home/pranav/mihir/algonauts_challenge/algonauts_2025.competitors/fmri/sub-01/func/sub-01_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5\"\n",
    "with h5py.File(file, 'r') as f:\n",
    "    key_arr = list(f.keys())\n",
    "    data = []\n",
    "    data_keys = []\n",
    "    for key in sorted(key_arr):\n",
    "        if 's01' in key:\n",
    "            data_keys.append(key)\n",
    "            data.append(f[key][:])\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    # print(f['sub-01_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold'].shape)\n",
    "print(sorted(key_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(data_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for i in train_ds:\n",
    "    # print(i['fmri'].shape)\n",
    "    # print(i['audio'].shape)\n",
    "    # print(i['video'].shape)\n",
    "    # print(i['language'].shape)\n",
    "    num += 1\n",
    "print(num)\n",
    "# print(i['fmri'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "   def __init__(self, in_dim, out_dim):\n",
    "       super().__init__()\n",
    "       self.downsample = in_dim != out_dim\n",
    "       self.net = nn.Sequential(\n",
    "           nn.Linear(in_dim, out_dim),\n",
    "           nn.LayerNorm(out_dim),\n",
    "           nn.GELU(),\n",
    "           nn.Linear(out_dim, out_dim),\n",
    "           nn.LayerNorm(out_dim),\n",
    "           nn.GELU()\n",
    "       )\n",
    "       if self.downsample:\n",
    "           self.proj = nn.Linear(in_dim, out_dim)\n",
    "   \n",
    "   def forward(self, x):\n",
    "       if self.downsample:\n",
    "           return self.proj(x) + self.net(x)\n",
    "       return x + self.net(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "   def __init__(self, input_dim=1000, hidden_dims=[512, 384, 256], num_tokens=32, codebook_dim=64):\n",
    "       super().__init__()\n",
    "       \n",
    "       # Initial projection with one residual block\n",
    "       self.input_proj = ResidualBlock(input_dim, hidden_dims[0])\n",
    "       \n",
    "       # Main network with one residual block per layer\n",
    "       layers = []\n",
    "       for i in range(len(hidden_dims)-1):\n",
    "           layers.append(ResidualBlock(hidden_dims[i], hidden_dims[i+1]))\n",
    "       self.layers = nn.Sequential(*layers)\n",
    "       \n",
    "       # Project to token space with one residual block\n",
    "       self.token_proj = ResidualBlock(hidden_dims[-1], num_tokens * codebook_dim)\n",
    "       \n",
    "       self.num_tokens = num_tokens\n",
    "       self.codebook_dim = codebook_dim\n",
    "       \n",
    "   def forward(self, x):\n",
    "       x = self.input_proj(x)\n",
    "       x = self.layers(x)\n",
    "       x = self.token_proj(x)\n",
    "       return x.view(x.shape[0], self.num_tokens, self.codebook_dim)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "   def __init__(self, output_dim=1000, hidden_dims=[256, 384, 512], num_tokens=32, codebook_dim=64):\n",
    "       super().__init__()\n",
    "       \n",
    "       # Process tokens with one residual block\n",
    "       self.token_proj = ResidualBlock(num_tokens * codebook_dim, hidden_dims[0])\n",
    "       \n",
    "       # Main network with one residual block per layer\n",
    "       layers = []\n",
    "       for i in range(len(hidden_dims)-1):\n",
    "           layers.append(ResidualBlock(hidden_dims[i], hidden_dims[i+1]))\n",
    "       self.layers = nn.Sequential(*layers)\n",
    "       \n",
    "       # Final projection with one residual block\n",
    "       self.output_proj = ResidualBlock(hidden_dims[-1], output_dim)\n",
    "       \n",
    "   def forward(self, x):\n",
    "       # x shape: [batch_size, num_tokens, codebook_dim] \n",
    "       x = x.reshape(x.shape[0], -1)  # Flatten tokens\n",
    "       x = self.token_proj(x)\n",
    "       x = self.layers(x)\n",
    "       return self.output_proj(x)\n",
    "\n",
    "class VQVAE(L.LightningModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim=1000, \n",
    "            hidden_dims=[512, 384, 256], \n",
    "            num_tokens=32, \n",
    "            codebook_size=1024, \n",
    "            codebook_dim=8,\n",
    "            commitment_weight=0.25,\n",
    "            quantizer_decay=0.99,\n",
    "            learning_rate=3e-4,\n",
    "            weight_decay=0.01\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_dim, hidden_dims, num_tokens, codebook_dim)\n",
    "        self.decoder = Decoder(input_dim, hidden_dims[::-1], num_tokens, codebook_dim)\n",
    "        self.quantizer = VectorQuantize(\n",
    "                dim=codebook_dim,\n",
    "                codebook_size=codebook_size,\n",
    "                decay=quantizer_decay,\n",
    "                commitment_weight=commitment_weight\n",
    "                )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, indices, commitment_loss = self.quantizer(z)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, commitment_loss, indices\n",
    "    \n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        _, indices, _ = self.quantizer(z)\n",
    "        return indices\n",
    "        \n",
    "    def decode(self, indices):\n",
    "        z_q = self.quantizer.get_codes_from_indices(indices)\n",
    "        return self.decoder(z_q)\n",
    "    \n",
    "class MultiModalMLP(L.LightningModule):\n",
    "    def __init__(self, video_dim, audio_dim, text_dim, hidden_dim, num_tokens, fmri_tok_dir, learning_rate, weight_decay):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dimensions for each modality\n",
    "        self.video_dim = video_dim \n",
    "        self.audio_dim = audio_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.fmri_tokenizer = VQVAE.load_from_checkpoint(fmri_tok_dir)\n",
    "        \n",
    "        for param in self.fmri_tokenizer.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.fmri_tokenizer.eval()\n",
    "        \n",
    "        # Trainable token for missing text\n",
    "        self.missing_text_token = nn.Parameter(torch.randn(1, text_dim))\n",
    "        \n",
    "        # MLP layers\n",
    "        total_dim = video_dim + audio_dim + text_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(total_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_tokens)\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, video, audio, text):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video: [batch_size, video_dim]\n",
    "            audio: [batch_size, audio_dim] \n",
    "            text: [batch_size, text_dim]\n",
    "        \"\"\"\n",
    "        batch_size = video.shape[0]\n",
    "        # Generate text mask based on NaN values\n",
    "        text_mask = ~torch.isnan(text).any(dim=1, keepdim=True)\n",
    "        \n",
    "        # Replace missing text with learned token\n",
    "        text = torch.where(\n",
    "            text_mask,\n",
    "            text,\n",
    "            self.missing_text_token.expand(batch_size, -1)\n",
    "        )\n",
    "        \n",
    "        # Concatenate all features\n",
    "        text = text.unsqueeze(1)\n",
    "        x = torch.cat([video, audio, text], dim=-1).squeeze(1)\n",
    "        # Predict tokens\n",
    "        return self.mlp(x)\n",
    "\n",
    "    def calculate_metrics(self, x, x_recon):\n",
    "        # Flatten the tensors for correlation calculation\n",
    "        x_flat = x.reshape(x.shape[0], -1)\n",
    "        x_recon_flat = x_recon.reshape(x_recon.shape[0], -1)\n",
    "        \n",
    "        # Calculate Pearson R for each sample in batch\n",
    "        correlations = torch.stack([\n",
    "            pearson_corrcoef(x_flat[i], x_recon_flat[i])\n",
    "            for i in range(x_flat.shape[0])\n",
    "        ])\n",
    "        avg_pearson_r = correlations.mean()\n",
    "        \n",
    "        # Calculate variance explained\n",
    "        total_variance = torch.var(x_flat, dim=1).sum()\n",
    "        residual_variance = torch.var(x_flat - x_recon_flat, dim=1).sum()\n",
    "        variance_explained = 1 - (residual_variance / total_variance)\n",
    "        \n",
    "        # Calculate MSE and MAE\n",
    "        mse = F.mse_loss(x_flat, x_recon_flat)\n",
    "        mae = F.l1_loss(x_flat, x_recon_flat)\n",
    "        \n",
    "        return avg_pearson_r, variance_explained, mse, mae\n",
    "        \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        video, audio, text, fmri = batch['video'], batch['audio'], batch['language'], batch['fmri']\n",
    "        logits = self(video, audio, text)\n",
    "        fmri_tokens = self.fmri_tokenizer.encode(fmri).to(dtype=logits.dtype)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(logits, fmri_tokens)\n",
    "        recon_fmri = self.fmri_tokenizer.decode(logits.round().long())\n",
    "\n",
    "        avg_pearson_r, variance_explained, mse, mae = self.calculate_metrics(fmri, recon_fmri)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_pearson_r', avg_pearson_r)\n",
    "        self.log('train_variance_explained', variance_explained)\n",
    "        self.log('train_mse', mse)\n",
    "        self.log('train_mae', mae)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        video, audio, text, fmri = batch['video'], batch['audio'], batch['language'], batch['fmri']\n",
    "        logits = self(video, audio, text)\n",
    "        fmri_tokens = self.fmri_tokenizer.encode(fmri).to(dtype=logits.dtype)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(logits, fmri_tokens)\n",
    "        val_recon_fmri = self.fmri_tokenizer.decode(logits.round().long())\n",
    "        val_avg_pearson_r, val_variance_explained, val_mse, val_mae = self.calculate_metrics(fmri, val_recon_fmri)\n",
    "        \n",
    "        self.log('val_loss', val_loss)\n",
    "        self.log('val_pearson_r', val_avg_pearson_r)\n",
    "        self.log('val_variance_explained', val_variance_explained)\n",
    "        self.log('val_mse', val_mse)\n",
    "        self.log('val_mae', val_mae)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            betas=(0.9, 0.999), \n",
    "            eps=1e-8, \n",
    "            weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "batch_size = 4\n",
    "video_dim = 8192\n",
    "audio_dim = 128\n",
    "text_dim = 768\n",
    "hidden_dim = 1024\n",
    "num_tokens = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.01 \n",
    "num_workers = 4\n",
    "fmri_tok_dir = \"/home/pranav/mihir/algonauts_challenge/algonauts2025/checkpoints/fmri_vqvae_res_gelu_8qdim_1024/epoch=44_val_loss=0.137.ckpt\"\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  22787\n",
      "Validation samples:  22924\n"
     ]
    }
   ],
   "source": [
    "print(\"Training samples: \", len(train_ds))\n",
    "print(\"Validation samples: \", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "run_name = \"MM_MLP_1024dim_only_sub01\"\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"algonauts2025\",\n",
    "    name=run_name,\n",
    "    save_dir=\"wandb_logs/\"\n",
    ")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    precision=32,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            dirpath=f'checkpoints/{run_name}',\n",
    "            filename='{epoch:02d}_{val_loss:.3f}',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=1,\n",
    "            save_last=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name           | Type       | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | fmri_tokenizer | VQVAE      | 5.4 M  | eval \n",
      "1 | mlp            | Sequential | 10.4 M | train\n",
      "  | other params   | n/a        | 768    | n/a  \n",
      "------------------------------------------------------\n",
      "10.4 M    Trainable params\n",
      "5.4 M     Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.157    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "79        Modules in eval mode\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "torch.set_float32_matmul_precision('high')\n",
    "mm_mlp = MultiModalMLP(\n",
    "    video_dim=video_dim,\n",
    "    audio_dim=audio_dim,\n",
    "    text_dim=text_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_tokens=num_tokens,\n",
    "    fmri_tok_dir=fmri_tok_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "summary = ModelSummary(mm_mlp, max_depth=1)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmihir-neal\u001b[0m (\u001b[33mmihirneal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb_logs/wandb/run-20250128_195247-5dpepia6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mihirneal/algonauts2025/runs/5dpepia6' target=\"_blank\">MM_MLP_1024dim_only_sub01</a></strong> to <a href='https://wandb.ai/mihirneal/algonauts2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mihirneal/algonauts2025' target=\"_blank\">https://wandb.ai/mihirneal/algonauts2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mihirneal/algonauts2025/runs/5dpepia6' target=\"_blank\">https://wandb.ai/mihirneal/algonauts2025/runs/5dpepia6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type       | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | fmri_tokenizer | VQVAE      | 5.4 M  | eval \n",
      "1 | mlp            | Sequential | 10.4 M | train\n",
      "  | other params   | n/a        | 768    | n/a  \n",
      "------------------------------------------------------\n",
      "10.4 M    Trainable params\n",
      "5.4 M     Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.157    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "79        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12e9aa4c87a4090be45f15b8bc0a4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889b4631a724400089dc35d1d0e78098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd424b2bd714b38b879550de612ef8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94c35b28cfc4fea87e57f788b3e805f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840dd40bcd614e60ade5ec6f1aee2ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bcfad4bfc540e991e319cfd88323a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5e1104b4154a6b92cbc56a3b0587a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1462da9ebd8a4329abbff38a980551c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58408ed5675b4cbc889ee4b56439c45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d14569a2ef4929a47d760121f6988d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6051b0b127264b3782f3b4e3f88ac890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061c45360c21430d884b499af01deb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74054471598a477387f6d2e92d51c59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a322d4ba24e144e48657ed8a496939a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3fd9894b5145ea9f5bef753d1afac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29694b1d88534da4a4ce1ec9a92f17f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2d363da43e49a2aa1007605d0d20b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3599d321a8458f9c25a790e710ac9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0362edcf1c33432ca1c59427227539a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fb7df2d2b446da963e4c4b556931fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f475414037d4368a49591bb2eded23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a3e35e823f49e2872bed302067daf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a410fe73a674bc6b091676dcf1d55b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b041ba833b94cbaad5b9e4a94893f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb0f3be41034841b6d593cc7e09ea60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9476d4fd76e4f5bb379f97a354006db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c549b88498b04e75b82181e891e37e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21b87a8a3d640c091f67878e614ee65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e61ca1d5a345a99b322b46565f2485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382dee5ce60a442495f2712e35ee75cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11dd77b14414e209c3638049f991244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61610f041a5d4902b7693f6039d1d3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd0bb73ab92431e9202a1083d863813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb34e42a30c4e1fae65b89c87aec8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d430df1ba51440b86ccbbf0a0adb775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d1d282a6964c48a21e4b64ce4f0a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025c6ac514014115bfd3421f6034b973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418e71783d8c4818a916679eb9510ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4629c85c979f40d6a60984e8860c3c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1200132816414987b4bc26f7d34e61a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c91d7dd9b843839b4cf9409ee11d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c3b072d7c0414a8c10dbe91f493cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a2d157383a4a3b8c79a0634e67c2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8878ab4935a40bcbb62edce18bf81b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b604abb3e9b849bd8b8c6819959ee7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c40f41f14248588ca70a01040420a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec0ff547527411e9f423165556c7f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f56b159a7f4b968d0d15805dd673a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fa23d6b122466eb13d6dc3e43e698d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511e454d9df64e969452c392907db431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da53340fc4d48ad83df27cd3e6af64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e33430e98b4e419e7112723170f0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(mm_mlp, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
