{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71ed593",
   "metadata": {},
   "source": [
    " == Stiching Model Performances Based on Test Movies == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32505818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cindy/algonauts2025\n",
      "Saving weights to ./output/feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123/ensemble_weights.npy\n",
      "Saving weights to file done!\n",
      "/home/cindy/algonauts2025\n",
      "Saving weights to ./output/feature_encoding_data-life_replace_internvl3-14B_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121216/ensemble_weights.npy\n",
      "Saving weights to file done!\n",
      "/home/cindy/algonauts2025\n",
      "Saving weights to ./output/feature_encoding_data-all_replace_internvl3-14B_llama-3B_add_qwen3b_vjepa2_enc-kernel-45_2507112307/ensemble_weights.npy\n",
      "Saving weights to file done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sub-01': {'chaplin1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'chaplin2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'mononoke1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'mononoke2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'passepartout1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'passepartout2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'planetearth1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'planetearth2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'pulpfiction1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'pulpfiction2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'wot1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'wot2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)},\n",
       " 'sub-02': {'chaplin1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'chaplin2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'mononoke1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'mononoke2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'passepartout1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'passepartout2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'planetearth1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'planetearth2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'pulpfiction1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'pulpfiction2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'wot1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'wot2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)},\n",
       " 'sub-03': {'chaplin1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'chaplin2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'mononoke1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'mononoke2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'passepartout1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'passepartout2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'planetearth1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'planetearth2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'pulpfiction1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'pulpfiction2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'wot1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'wot2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)},\n",
       " 'sub-05': {'chaplin1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'chaplin2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'mononoke1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'mononoke2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'passepartout1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'passepartout2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'planetearth1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'planetearth2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'pulpfiction1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'pulpfiction2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'wot1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       "  'wot2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "def make_ensemble_weights(target_filename=\"fmri_predictions_ood.npy\", foldername = \"\",filter_movies=[\"chaplin\"]):\n",
    "    \"\"\"\n",
    "    Create a dictionary of ensemble weights for different models, formatted the same way as the submission file.\n",
    "    \"\"\"\n",
    "    print(os.getcwd())\n",
    "    weights = {'sub-01':{}, 'sub-02':{}, 'sub-03':{}, 'sub-05':{}}\n",
    "    submission_predictions = np.load(os.path.join(\"./output\",foldername,target_filename),allow_pickle=True).item()\n",
    "    for subject, episodes_dict in submission_predictions.items():\n",
    "            for episode,values in episodes_dict.items():\n",
    "                if any(fm in episode for fm in filter_movies):\n",
    "                    weights[subject][episode] = np.ones_like(values)\n",
    "                else:\n",
    "                    weights[subject][episode] = np.zeros_like(values)\n",
    "    save_path = os.path.join(\"./output\", foldername, \"ensemble_weights.npy\")\n",
    "    np.save(save_path, weights,allow_pickle=True)\n",
    "    print(f'Saving weights to {save_path}')\n",
    "    print('Saving weights to file done!')\n",
    "    return weights\n",
    "\n",
    "# Use no llama for Chaplin\n",
    "make_ensemble_weights(target_filename=\"fmri_predictions_ood.npy\", foldername = \"feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123\",filter_movies=[\"chaplin\"])\n",
    "# Use extra training on life for planet earth\n",
    "make_ensemble_weights(target_filename=\"fmri_predictions_ood.npy\", foldername = \"feature_encoding_data-life_replace_internvl3-14B_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121216\",filter_movies=[\"planetearth\"])\n",
    "# Use the canonical model for others\n",
    "make_ensemble_weights(target_filename=\"fmri_predictions_ood.npy\", foldername = \"feature_encoding_data-all_replace_internvl3-14B_llama-3B_add_qwen3b_vjepa2_enc-kernel-45_2507112307\",filter_movies=[\"mononoke\",\"passepartout\",\"pulpfiction\",\"wot\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb04d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def save_weighted_predictions(weights_file, predictions_file, foldernames=[], output_file_stem='_'):\n",
    "    \"\"\"\n",
    "    Load weights and predictions, apply weights, and save the weighted predictions.\n",
    "    \"\"\"\n",
    "    for ii, foldername in enumerate(foldernames):\n",
    "        weights = np.load(os.path.join(\"./output\", foldername, weights_file), allow_pickle=True).item()\n",
    "        predictions = np.load(os.path.join(\"./output\", foldername, predictions_file), allow_pickle=True).item()\n",
    "        \n",
    "        # Replace the ii == 0 block with:\n",
    "        if ii == 0:\n",
    "            all_submissions = {subj: {ep: np.zeros_like(vals) for ep, vals in eps.items()}\n",
    "                            for subj, eps in predictions.items()}\n",
    "            all_weights = {subj: {ep: np.zeros_like(vals) for ep, vals in eps.items()}\n",
    "                        for subj, eps in predictions.items()}\n",
    "        # Then accumulate for all models (including first):\n",
    "        for subject, episodes_dict in predictions.items():\n",
    "            for episode, values in episodes_dict.items():\n",
    "                all_submissions[subject][episode] += np.multiply(values, weights[subject][episode])\n",
    "                all_weights[subject][episode] += weights[subject][episode]\n",
    "    # Normalize by weights (with small epsilon to avoid division by zero)\n",
    "    epsilon = 1e-10\n",
    "    for subject, episodes_dict in all_submissions.items():\n",
    "        for episode, values in episodes_dict.items():\n",
    "            all_submissions[subject][episode] = np.divide(values, all_weights[subject][episode] + epsilon)\n",
    "\n",
    "    # Rest of your saving code remains the same...\n",
    "    ny_time = datetime.now(pytz.timezone(\"America/New_York\"))\n",
    "    now = ny_time.strftime(\"%y%m%d%H%M\") \n",
    "    ensemble_output_path = './output/ensemble_output'+ output_file_stem + now \n",
    "    os.makedirs(ensemble_output_path, exist_ok=True)  \n",
    "    np.save(os.path.join(ensemble_output_path, predictions_file), all_submissions, allow_pickle=True)\n",
    "    print(f'Saving weighted predictions to {ensemble_output_path}')\n",
    "    \n",
    "    # Zip the saved file for submission\n",
    "    zip_file = os.path.join(ensemble_output_path, predictions_file.replace(\"npy\",\"zip\"))\n",
    "    with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "        zipf.write(os.path.join(ensemble_output_path, predictions_file), os.path.basename(os.path.join(ensemble_output_path, predictions_file)))\n",
    "    print(f\"Submission file successfully zipped as: {zip_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "832e7436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final = np.load(\"./output/ensemble_output_weighted-prediction-by-movies_2507121402/fmri_predictions_ood.npy\",allow_pickle=True).item()\n",
    "nollama = np.load(\"./output/feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123/fmri_predictions_ood.npy\",allow_pickle=True).item()\n",
    "extralife = np.load(\"./output/feature_encoding_data-life_replace_internvl3-14B_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121216/fmri_predictions_ood.npy\",allow_pickle=True).item()\n",
    "other = np.load(\"./output/feature_encoding_data-all_replace_internvl3-14B_llama-3B_add_qwen3b_vjepa2_enc-kernel-45_2507112307/fmri_predictions_ood.npy\",allow_pickle=True).item()\n",
    "print(np.allclose(final['sub-01']['chaplin1'],nollama['sub-01']['chaplin1']))\n",
    "print(np.allclose(final['sub-01']['planetearth1'],extralife['sub-01']['planetearth1']))\n",
    "print(np.allclose(final['sub-01']['mononoke1'],other['sub-01']['mononoke1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76399fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# For the nollama model\n",
    "nollama_weights = np.load(\"./output/feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123/ensemble_weights.npy\", allow_pickle=True).item()\n",
    "print(nollama_weights['sub-01']['chaplin1'][:10])  # Should be all 1s\n",
    "\n",
    "# For other models\n",
    "other_weights = np.load(\"./output/feature_encoding_data-all_replace_internvl3-14B_llama-3B_add_qwen3b_vjepa2_enc-kernel-45_2507112307/ensemble_weights.npy\", allow_pickle=True).item()\n",
    "print(other_weights['sub-01']['chaplin1'][:10])  # Should be all 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930b3b2",
   "metadata": {},
   "source": [
    "Select the best one for submission among many submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc3f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission0:output/feature_encoding_data-all_replace_internvl3-14B_llama-3B_add_qwen3b_vjepa2_enc-kernel-45_2507112307\n",
      "Submission1:output/Mihirs_model_submission1\n",
      "Submission2:output/feature_encoding_remove_llama_qwen3b_vjepa2_enc-kernel-45_2507102030\n",
      "Submission3:output/ensenmble_results-Cesar44models\n",
      "Subject: sub-01, Expected OOD Mean Accuracy: 0.2315\n",
      "Subject: sub-02, Expected OOD Mean Accuracy: 0.1972\n",
      "Subject: sub-03, Expected OOD Mean Accuracy: 0.2295\n",
      "Subject: sub-05, Expected OOD Mean Accuracy: 0.1700\n",
      "Across-subject Expected OOD Mean Accuracy: 0.2070\n",
      "{'sub-01': {'chaplin': 0.23129592835903168, 'mononoke': 0.28672534227371216, 'passepartout': 0.2574006915092468, 'planetearth': 0.12944163382053375, 'pulpfiction': 0.2536613345146179, 'wot': 0.2303098440170288}, 'sub-02': {'chaplin': 0.14667394757270813, 'mononoke': 0.2444467693567276, 'passepartout': 0.24178491532802582, 'planetearth': 0.17622965574264526, 'pulpfiction': 0.22582681477069855, 'wot': 0.1479874849319458}, 'sub-03': {'chaplin': 0.20571883022785187, 'mononoke': 0.28494587540626526, 'passepartout': 0.228009894490242, 'planetearth': 0.2025085985660553, 'pulpfiction': 0.22552992403507233, 'wot': 0.23013629019260406}, 'sub-05': {'chaplin': 0.13128207623958588, 'mononoke': 0.16565744578838348, 'passepartout': 0.16744735836982727, 'planetearth': 0.15240900218486786, 'pulpfiction': 0.24383355677127838, 'wot': 0.15957772731781006}}\n",
      "{'sub-01': {'chaplin': 2, 'mononoke': 3, 'passepartout': 3, 'planetearth': 3, 'pulpfiction': 0, 'wot': 0}, 'sub-02': {'chaplin': 0, 'mononoke': 0, 'passepartout': 3, 'planetearth': 3, 'pulpfiction': 0, 'wot': 3}, 'sub-03': {'chaplin': 0, 'mononoke': 0, 'passepartout': 3, 'planetearth': 0, 'pulpfiction': 3, 'wot': 0}, 'sub-05': {'chaplin': 2, 'mononoke': 0, 'passepartout': 0, 'planetearth': 0, 'pulpfiction': 0, 'wot': 3}}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "def find_scores_json_parents(root_dir=\".\"):\n",
    "    root_path = Path(root_dir)\n",
    "    # Find all 'scores.json' files recursively\n",
    "    matches = list(root_path.glob(\"**/scores.json\"))\n",
    "    # Extract parent folders\n",
    "    parent_folders = [str(file.parent) for file in matches]\n",
    "    return parent_folders\n",
    "\n",
    "all_submission_dirs = find_scores_json_parents(\".\")\n",
    "for ii, dir in enumerate(all_submission_dirs):\n",
    "    print(f\"Submission{ii}:{dir}\")\n",
    "    json_path = os.path.join(dir, \"scores.json\")\n",
    "    with open(json_path,'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # Returns a Python dict/list\n",
    "        if ii == 0:\n",
    "            acc = {}\n",
    "            index = {}\n",
    "        for key,values in data.items():\n",
    "            if '_' in key:\n",
    "                # Split into ['sub-01', 'movie-chaplin']\n",
    "                subject, movie_with_prefix = key.split('_')\n",
    "                \n",
    "                # Remove 'movie-' prefix to get 'chaplin'\n",
    "                movie = movie_with_prefix.replace('movie-', '')\n",
    "                \n",
    "                # Initialize subject if not exists\n",
    "                if subject not in acc:\n",
    "                    acc[subject] = {}\n",
    "                    index[subject] = {}\n",
    "                # Initialize movie if not exists\n",
    "                if movie not in acc[subject]:\n",
    "                    acc[subject][movie] = 0\n",
    "\n",
    "                # keep the best accuracy\n",
    "                if values>acc[subject][movie]:\n",
    "                    acc[subject][movie] = values\n",
    "                    index[subject][movie] = ii\n",
    "grand_mean = []\n",
    "for subject,subject_acc in acc.items():\n",
    "    movie_mean = np.mean(list(subject_acc.values()))\n",
    "    print(f\"Subject: {subject}, Expected OOD Mean Accuracy: {movie_mean:.4f}\")\n",
    "    grand_mean.append(movie_mean)\n",
    "\n",
    "grand_mean = np.mean(grand_mean)\n",
    "print(f\"Across-subject Expected OOD Mean Accuracy: {grand_mean:.4f}\")\n",
    "print(acc)\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337e8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_encoding_default_2507061512', 'feature_encoding_enc_kernel-38_2507061602', 'feature_encoding_default_2506300839', 'ensemble_output_2507061554', 'feature_encoding_enc_kernel-45_2507061610', 'feature_encoding_enc_kernel-20_2507061542']\n",
      "Submission file successfully zipped as: ./output/ensemble_output_2507061621/fmri_predictions_friends_s7.zip\n"
     ]
    }
   ],
   "source": [
    "# First let's just take the average of several model predictions regardless of their architecture\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "target_filename = \"fmri_predictions_friends_s7.npy\" # CHANGE THIS\n",
    "\n",
    "ny_time = datetime.now(pytz.timezone(\"America/New_York\"))\n",
    "now = ny_time.strftime(\"%y%m%d%H%M\") \n",
    "ensemble_output_path = './output/ensemble_output_' + now\n",
    "foldernames = [] # [\"feature_encoding_enc_kernel-20_2507061542\",\"feature_encoding_default_2507061512\",\"feature_encoding_default_2506300839\"] # CHANGE THIS \n",
    "\n",
    "matching_folders = []\n",
    "for root, dirs, files in os.walk(\"./output/\"):\n",
    "    if target_filename in files:\n",
    "        # Get the folder name (the last part of the root path)\n",
    "        folder_name = os.path.basename(root)\n",
    "        matching_folders.append(folder_name)\n",
    "\n",
    "if not foldernames:\n",
    "     foldernames = matching_folders\n",
    "print(foldernames)\n",
    "\n",
    "os.makedirs(ensemble_output_path,exist_ok = True)  \n",
    "for ii,foldername in enumerate(foldernames):\n",
    "    submission_predictions = np.load(os.path.join(\"./output\",foldername,target_filename),allow_pickle=True).item()\n",
    "    if ii == 0:\n",
    "        all_submissions = submission_predictions\n",
    "    else:\n",
    "        # Add everything\n",
    "        for subject, episodes_dict in submission_predictions.items():\n",
    "            for episode, values in episodes_dict.items():\n",
    "                all_submissions[subject][episode] += values\n",
    "            # Optional: Display the structure and shapes of the predicted fMRI responses dictionary\n",
    "            # # Print the subject and episode number for Friends season 7\n",
    "            # print(f\"Subject: {subject}\")\n",
    "            # print(f\"  Number of Episodes: {len(episodes_dict)}\")\n",
    "            # # Print the predicted fMRI response shape for each episode\n",
    "            # for episode, predictions in episodes_dict.items():\n",
    "            #     print(f\"    - Episode: {episode}, Predicted fMRI shape: {predictions.shape}\")\n",
    "            # print(\"-\" * 40)  # Separator for clarity\n",
    "\n",
    "# Divide by total number of model predictions\n",
    "for subject, episodes_dict in submission_predictions.items():\n",
    "            for episode, values in episodes_dict.items():\n",
    "                all_submissions[subject][episode] /= len(foldernames)  \n",
    "# Save the output\n",
    "np.save(os.path.join(ensemble_output_path,target_filename),all_submissions,allow_pickle=True)\n",
    "\n",
    "# Zip the saved file for submission\n",
    "zip_file = os.path.join(ensemble_output_path,target_filename.replace(\"npy\",\"zip\"))\n",
    "with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "    zipf.write(os.path.join(ensemble_output_path,target_filename), os.path.basename(os.path.join(ensemble_output_path,target_filename)))\n",
    "print(f\"Submission file successfully zipped as: {zip_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19b805",
   "metadata": {},
   "source": [
    "==== Previous Notes that are unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf511ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.linear_model import LinearRegression\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import time\n",
    "# from collections import defaultdict\n",
    "# from pathlib import Path\n",
    "\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import IterableDataset\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b18207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(submission_predictions['sub-01']['s07e01a'],all_submissions['sub-01']['s07e01a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algonauts2025Dataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fmri_data: dict[str, np.ndarray],\n",
    "        feat_data: list[dict[str, np.ndarray]] | None = None,\n",
    "        sample_length: int | None = 128,\n",
    "        num_samples: int | None = None,\n",
    "        shuffle: bool = True,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        self.fmri_data = fmri_data\n",
    "        self.feat_data = feat_data\n",
    "\n",
    "        self.episode_list = list(fmri_data)\n",
    "        self.sample_length = sample_length\n",
    "        self.num_samples = num_samples\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def _iter_shuffle(self):\n",
    "        sample_idx = 0\n",
    "        while True:\n",
    "            episode_order = self._rng.permutation(len(self.episode_list))\n",
    "\n",
    "            for ii in episode_order:\n",
    "                episode = self.episode_list[ii]\n",
    "                feat_episode = episode[0] if isinstance(episode, tuple) else episode\n",
    "\n",
    "                fmri = torch.from_numpy(self.fmri_data[episode]).float()\n",
    "    \n",
    "                if self.feat_data:\n",
    "                    feats = [torch.from_numpy(data[feat_episode]).float() for data in self.feat_data]\n",
    "                else:\n",
    "                    feats = feat_samples = None\n",
    "\n",
    "                # Nb, fmri and feature length often off by 1 or 2.\n",
    "                # But assuming time locked to start.\n",
    "                length = fmri.shape[1]\n",
    "                if feats:\n",
    "                    length = min(length, min(feat.shape[0] for feat in feats))\n",
    "\n",
    "                if self.sample_length:\n",
    "                    # Random segment of run\n",
    "                    offset = int(self._rng.integers(0, length - self.sample_length + 1))\n",
    "                    fmri_sample = fmri[:, offset: offset + self.sample_length]\n",
    "                    if feats:\n",
    "                        feat_samples = [\n",
    "                            feat[offset: offset + self.sample_length] for feat in feats\n",
    "                        ]\n",
    "                else:\n",
    "                    # Take full run\n",
    "                    # Nb this only works for batch size 1 since runs are different length\n",
    "                    fmri_sample = fmri[:, :length]\n",
    "                    if feats:\n",
    "                        feat_samples = [feat[:length] for feat in feats]\n",
    "\n",
    "                if feat_samples:\n",
    "                    yield episode, fmri_sample, feat_samples\n",
    "                else:\n",
    "                    yield episode, fmri_sample\n",
    "\n",
    "                sample_idx += 1\n",
    "                if self.num_samples and sample_idx >= self.num_samples:\n",
    "                    return\n",
    "\n",
    "    def _iter_ordered(self):\n",
    "        sample_idx = 0\n",
    "        for episode in self.episode_list:\n",
    "            feat_episode = episode[0] if isinstance(episode, tuple) else episode\n",
    "            fmri = torch.from_numpy(self.fmri_data[episode]).float()\n",
    "            if self.feat_data:\n",
    "                feats = [torch.from_numpy(data[feat_episode]).float() for data in self.feat_data]\n",
    "            else:\n",
    "                feats = feat_samples = None\n",
    "\n",
    "            length = fmri.shape[1]\n",
    "            if feats:\n",
    "                length = min(length, min(feat.shape[0] for feat in feats))\n",
    "\n",
    "            sample_length = self.sample_length or length\n",
    "\n",
    "            for offset in range(0, length - sample_length + 1, sample_length):\n",
    "                fmri_sample = fmri[:, offset: offset + sample_length]\n",
    "                if feats:\n",
    "                    feat_samples = [feat[offset: offset + sample_length] for feat in feats]\n",
    "\n",
    "                if feat_samples:\n",
    "                    yield episode, fmri_sample, feat_samples\n",
    "                else:\n",
    "                    yield episode, fmri_sample\n",
    "\n",
    "                sample_idx += 1\n",
    "                if self.num_samples and sample_idx >= self.num_samples:\n",
    "                    return\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            yield from self._iter_shuffle()\n",
    "        else:\n",
    "            yield from self._iter_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dd571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Algonauts2025Dataset(\n",
    "    friends_val_fmri,\n",
    "    list(stimuli_features_friends.values()),\n",
    "    sample_length=None,\n",
    "    shuffle=False,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    *,\n",
    "    epoch: int,\n",
    "    model: torch.nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    use_cuda = device.type == \"cuda\"\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    step_time_m = AverageMeter()\n",
    "\n",
    "    samples = []\n",
    "    outputs = []\n",
    "\n",
    "    end = time.monotonic()\n",
    "    for batch_idx, (_, sample, feats) in enumerate(val_loader):\n",
    "        sample = sample.to(device)\n",
    "        feats = [feat.to(device) for feat in feats]\n",
    "        batch_size = sample.size(0)\n",
    "        data_time = time.monotonic() - end\n",
    "\n",
    "        # forward pass\n",
    "        output = model(feats)\n",
    "        loss = F.mse_loss(output, sample)\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        # end of iteration timing\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        step_time = time.monotonic() - end\n",
    "\n",
    "        loss_m.update(loss_item, batch_size)\n",
    "        data_time_m.update(data_time, batch_size)\n",
    "        step_time_m.update(step_time, batch_size)\n",
    "\n",
    "        N, S, L, C = sample.shape\n",
    "        assert N, S == (1, 4)\n",
    "        samples.append(sample.cpu().numpy().swapaxes(0, 1).reshape((S, N*L, C)))\n",
    "        outputs.append(output.cpu().numpy().swapaxes(0, 1).reshape((S, N*L, C)))\n",
    "\n",
    "        # Reset timer\n",
    "        end = time.monotonic()\n",
    "\n",
    "    # (S, N, C)\n",
    "    samples = np.concatenate(samples, axis=1)\n",
    "    outputs = np.concatenate(outputs, axis=1)\n",
    "\n",
    "    metrics = {}\n",
    "    model_pred = {}\n",
    "\n",
    "    # Encoding accuracy metrics\n",
    "    dim = samples.shape[-1]\n",
    "    acc = 0.0\n",
    "    acc_map = np.zeros(dim)\n",
    "    for ii, sub in enumerate(SUBJECTS):\n",
    "        y_true = samples[ii].reshape(-1, dim)\n",
    "        y_pred = outputs[ii].reshape(-1, dim)\n",
    "        metrics[f\"acc_map_sub-{sub}\"] = acc_map_i = pearsonr_score(y_true, y_pred)\n",
    "        metrics[f\"acc_sub-{sub}\"] = acc_i = np.mean(acc_map_i)\n",
    "        acc_map += acc_map_i / len(SUBJECTS)\n",
    "        acc += acc_i / len(SUBJECTS)\n",
    "        \n",
    "\n",
    "    metrics[\"acc_map_avg\"] = acc_map\n",
    "    metrics[\"acc_avg\"] = acc\n",
    "    accs_fmt = \",\".join(\n",
    "        f\"{val:.3f}\" for key, val in metrics.items() if key.startswith(\"acc_sub-\")\n",
    "    )\n",
    "\n",
    "    tput = batch_size / step_time_m.avg\n",
    "    print(\n",
    "        f\"Val: {epoch:>3d}\"\n",
    "        f\"  Loss: {loss_m.avg:#.3g}\"\n",
    "        f\"  Acc: {accs_fmt} ({acc:.3f})\"\n",
    "        f\"  Time: {data_time_m.avg:.3f},{step_time_m.avg:.3f} {tput:.0f}/s\"\n",
    "    )\n",
    "\n",
    "    return acc, metrics\n",
    "\n",
    "\n",
    "def pearsonr_score(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-7\n",
    ") -> np.ndarray:\n",
    "    assert y_true.ndim == y_pred.ndim == 2\n",
    "\n",
    "    y_true = y_true - y_true.mean(axis=0)\n",
    "    y_true = y_true / (np.linalg.norm(y_true, axis=0) + eps)\n",
    "\n",
    "    y_pred = y_pred - y_pred.mean(axis=0)\n",
    "    y_pred = y_pred / (np.linalg.norm(y_pred, axis=0) + eps)\n",
    "\n",
    "    score = (y_true * y_pred).sum(axis=0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume:\n",
    "# - X_val: Validation features (shape [n_samples, ...])\n",
    "# - y_val: Validation labels (shape [n_samples, ...])\n",
    "# - model_preds: Predictions from 5 models (shape [5, n_samples, ...])\n",
    "\n",
    "# Stack predictions (shape [n_samples, 5])\n",
    "X_stack = np.stack([model_preds[i] for i in range(5)], axis=-1)\n",
    "\n",
    "# Train a regressor to learn weights\n",
    "ensemble = LinearRegression()\n",
    "ensemble.fit(X_stack, y_val)  # Learns weights w1, ..., w5\n",
    "\n",
    "# For new data, combine predictions: final_pred = w1 * m1 + ... + w5 * m5\n",
    "new_preds = np.stack([model.predict(new_data) for model in models], axis=-1)\n",
    "final_pred = ensemble.predict(new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48bb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
