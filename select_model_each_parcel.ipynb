{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71ed593",
   "metadata": {},
   "source": [
    " == Stiching Model Performances Based on Test Movies == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32505818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cindy/algonauts2025\n",
      "Saving weights to ./output/feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123/ensemble_weights.npy\n",
      "Saving weights to file done!\n",
      "/home/cindy/algonauts2025\n",
      "Saving weights to ./output/feature_encoding_data-life_replace_internvl3-14B_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121216/ensemble_weights.npy\n",
      "Saving weights to file done!\n",
      "/home/cindy/algonauts2025\n",
      "Saving weights to ./output/feature_encoding_data-all_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121900/ensemble_weights.npy\n",
      "Saving weights to file done!\n",
      "{'sub-01': {'chaplin1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'chaplin2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'mononoke1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'mononoke2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'passepartout1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'passepartout2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'planetearth1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'planetearth2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pulpfiction1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'pulpfiction2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'wot1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'wot2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)}, 'sub-02': {'chaplin1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'chaplin2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'mononoke1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'mononoke2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'passepartout1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'passepartout2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'planetearth1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'planetearth2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pulpfiction1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'pulpfiction2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'wot1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'wot2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)}, 'sub-03': {'chaplin1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'chaplin2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'mononoke1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'mononoke2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'passepartout1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'passepartout2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'planetearth1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'planetearth2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pulpfiction1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'pulpfiction2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'wot1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'wot2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)}, 'sub-05': {'chaplin1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'chaplin2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'mononoke1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'mononoke2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'passepartout1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'passepartout2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'planetearth1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'planetearth2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pulpfiction1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'pulpfiction2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'wot1': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32), 'wot2': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "def make_ensemble_weights(target_filename=\"fmri_predictions_ood.npy\", foldername = \"\",filter_movies=[\"chaplin\"]):\n",
    "    \"\"\"\n",
    "    Create a dictionary of ensemble weights for different models, formatted the same way as the submission file.\n",
    "    \"\"\"\n",
    "    print(os.getcwd())\n",
    "    weights = {'sub-01':{}, 'sub-02':{}, 'sub-03':{}, 'sub-05':{}}\n",
    "    submission_predictions = np.load(os.path.join(\"./output\",foldername,target_filename),allow_pickle=True).item()\n",
    "    for subject, episodes_dict in submission_predictions.items():\n",
    "            for episode,values in episodes_dict.items():\n",
    "                if any(fm in episode for fm in filter_movies):\n",
    "                    weights[subject][episode] = np.ones_like(values)\n",
    "                else:\n",
    "                    weights[subject][episode] = np.zeros_like(values)\n",
    "    save_path = os.path.join(\"./output\", foldername, \"ensemble_weights.npy\")\n",
    "    np.save(save_path, weights,allow_pickle=True)\n",
    "    print(f'Saving weights to {save_path}')\n",
    "    print('Saving weights to file done!')\n",
    "    return weights\n",
    "\n",
    "# Use no llama for Chaplin\n",
    "make_ensemble_weights(target_filename=\"fmri_predictions_ood.npy\", foldername = \"feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123\",filter_movies=[\"chaplin\"])\n",
    "# Use extra training on life for planet earth\n",
    "make_ensemble_weights(target_filename=\"fmri_predictions_ood.npy\", foldername = \"feature_encoding_data-life_replace_internvl3-14B_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121216\",filter_movies=[\"planetearth\"])\n",
    "# Use the canonical model for others (internvl8b)\n",
    "weights = make_ensemble_weights(target_filename=\"fmri_predictions_ood.npy\", foldername = \"feature_encoding_data-all_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121900\",filter_movies=[\"mononoke\",\"passepartout\",\"pulpfiction\",\"wot\"])\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb04d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def save_weighted_predictions(weights_file, predictions_file, foldernames=[], output_file_stem='_'):\n",
    "    \"\"\"\n",
    "    Load weights and predictions, apply weights, and save the weighted predictions.\n",
    "    \"\"\"\n",
    "    for ii, foldername in enumerate(foldernames):\n",
    "        weights = np.load(os.path.join(\"./output\", foldername, weights_file), allow_pickle=True).item()\n",
    "        predictions = np.load(os.path.join(\"./output\", foldername, predictions_file), allow_pickle=True).item()\n",
    "        \n",
    "        # Replace the ii == 0 block with:\n",
    "        if ii == 0:\n",
    "            all_submissions = {subj: {ep: np.zeros_like(vals) for ep, vals in eps.items()}\n",
    "                            for subj, eps in predictions.items()}\n",
    "            all_weights = {subj: {ep: np.zeros_like(vals) for ep, vals in eps.items()}\n",
    "                        for subj, eps in predictions.items()}\n",
    "        # Then accumulate for all models (including first):\n",
    "        for subject, episodes_dict in predictions.items():\n",
    "            for episode, values in episodes_dict.items():\n",
    "                all_submissions[subject][episode] += np.multiply(values, weights[subject][episode])\n",
    "                all_weights[subject][episode] += weights[subject][episode]\n",
    "    # Normalize by weights (with small epsilon to avoid division by zero)\n",
    "    epsilon = 1e-10\n",
    "    for subject, episodes_dict in all_submissions.items():\n",
    "        for episode, values in episodes_dict.items():\n",
    "            all_submissions[subject][episode] = np.divide(values, all_weights[subject][episode] + epsilon)\n",
    "\n",
    "    # Rest of your saving code remains the same...\n",
    "    ny_time = datetime.now(pytz.timezone(\"America/New_York\"))\n",
    "    now = ny_time.strftime(\"%y%m%d%H%M\") \n",
    "    ensemble_output_path = './output/ensemble_output'+ output_file_stem + now \n",
    "    os.makedirs(ensemble_output_path, exist_ok=True)  \n",
    "    np.save(os.path.join(ensemble_output_path, predictions_file), all_submissions, allow_pickle=True)\n",
    "    print(f'Saving weighted predictions to {ensemble_output_path}')\n",
    "    \n",
    "    # Zip the saved file for submission\n",
    "    zip_file = os.path.join(ensemble_output_path, predictions_file.replace(\"npy\",\"zip\"))\n",
    "    with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "        zipf.write(os.path.join(ensemble_output_path, predictions_file), os.path.basename(os.path.join(ensemble_output_path, predictions_file)))\n",
    "    print(f\"Submission file successfully zipped as: {zip_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "832e7436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save_weighted_predictions(weights_file='ensemble_weights.npy',\n",
    "#                           predictions_file='fmri_predictions_ood.npy',\n",
    "#                           foldernames=[\n",
    "#                               \"feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123\",\n",
    "#                               \"feature_encoding_data-life_replace_internvl3-14B_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121216\",\n",
    "#                               \"feature_encoding_data-all_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121900\"\n",
    "#                           ],\n",
    "#                           output_file_stem='_weighted-prediction-by-movies_')\n",
    "final = np.load(\"./output/ensemble_output_weighted-prediction-by-movies_2507121913/fmri_predictions_ood.npy\",allow_pickle=True).item()\n",
    "nollama = np.load(\"./output/feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123/fmri_predictions_ood.npy\",allow_pickle=True).item()\n",
    "extralife = np.load(\"./output/feature_encoding_data-life_replace_internvl3-14B_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121216/fmri_predictions_ood.npy\",allow_pickle=True).item()\n",
    "other = np.load(\"./output/feature_encoding_data-all_llama-3B_qwen3b_vjepa2_enc-kernel-45_2507121900/fmri_predictions_ood.npy\",allow_pickle=True).item()\n",
    "print(np.allclose(final['sub-01']['chaplin1'],nollama['sub-01']['chaplin1']))\n",
    "print(np.allclose(final['sub-01']['planetearth1'],extralife['sub-01']['planetearth1']))\n",
    "print(np.allclose(final['sub-01']['mononoke1'],other['sub-01']['mononoke1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76399fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# For the nollama model\n",
    "nollama_weights = np.load(\"./output/feature_encoding_data-all-but-life_replace_internvl3-14B_remove_llama_add_qwen3b_vjepa2_enc-kernel-45_2507121123/ensemble_weights.npy\", allow_pickle=True).item()\n",
    "print(nollama_weights['sub-01']['chaplin1'][:10])  # Should be all 1s\n",
    "\n",
    "# For other models\n",
    "other_weights = np.load(\"./output/feature_encoding_data-all_replace_internvl3-14B_llama-3B_add_qwen3b_vjepa2_enc-kernel-45_2507112307/ensemble_weights.npy\", allow_pickle=True).item()\n",
    "print(other_weights['sub-01']['chaplin1'][:10])  # Should be all 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930b3b2",
   "metadata": {},
   "source": [
    "Select the best one for submission among many submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bc3f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission0:output/ensemble_output_weighted-prediction-by-movies_2507121913\n",
      "Submission1:output/feature_encoding_data-all_replace_internvl3-14B_llama-3B_add_qwen3b_vjepa2_enc-kernel-45_2507112307\n",
      "Submission2:output/Mihirs_model_submission1\n",
      "Submission3:output/feature_encoding_remove_llama_qwen3b_vjepa2_enc-kernel-45_2507102030\n",
      "Submission4:output/ensenmble_results-Cesar44models\n",
      "Subject: sub-01, Expected OOD Mean Accuracy: 0.2339\n",
      "Subject: sub-02, Expected OOD Mean Accuracy: 0.1982\n",
      "Subject: sub-03, Expected OOD Mean Accuracy: 0.2295\n",
      "Subject: sub-05, Expected OOD Mean Accuracy: 0.1727\n",
      "Across-subject Expected OOD Mean Accuracy: 0.2086\n",
      "{'sub-01': {'chaplin': 0.23936162889003754, 'mononoke': 0.28672534227371216, 'passepartout': 0.2574006915092468, 'planetearth': 0.12944163382053375, 'pulpfiction': 0.2536613345146179, 'wot': 0.23686395585536957}, 'sub-02': {'chaplin': 0.14667394757270813, 'mononoke': 0.24935907125473022, 'passepartout': 0.24178491532802582, 'planetearth': 0.17622965574264526, 'pulpfiction': 0.2269793301820755, 'wot': 0.1479874849319458}, 'sub-03': {'chaplin': 0.20571883022785187, 'mononoke': 0.28494587540626526, 'passepartout': 0.228009894490242, 'planetearth': 0.2025085985660553, 'pulpfiction': 0.22552992403507233, 'wot': 0.23013629019260406}, 'sub-05': {'chaplin': 0.14094766974449158, 'mononoke': 0.16565744578838348, 'passepartout': 0.16834783554077148, 'planetearth': 0.15240900218486786, 'pulpfiction': 0.24423867464065552, 'wot': 0.1648513227701187}}\n",
      "{'sub-01': {'chaplin': 0, 'mononoke': 4, 'passepartout': 4, 'planetearth': 4, 'pulpfiction': 1, 'wot': 0}, 'sub-02': {'chaplin': 1, 'mononoke': 0, 'passepartout': 4, 'planetearth': 4, 'pulpfiction': 0, 'wot': 4}, 'sub-03': {'chaplin': 1, 'mononoke': 1, 'passepartout': 4, 'planetearth': 1, 'pulpfiction': 4, 'wot': 1}, 'sub-05': {'chaplin': 0, 'mononoke': 1, 'passepartout': 0, 'planetearth': 1, 'pulpfiction': 0, 'wot': 0}}\n",
      "Stiching files from submissions...\n",
      "Saving final submission file...\n",
      "Saving final submission to ./output/bestof5submissions/fmri_predictions_ood_combined.npy\n",
      "Submission file successfully zipped as: ./output/bestof5submissions/fmri_predictions_ood_combined.zip\n",
      "[[-1.0512622e-01 -2.0492224e-01 -1.7910561e-01 ...  9.8571584e-02\n",
      "   2.4206191e-04  3.7062012e-02]\n",
      " [-1.3663875e-01 -2.8796634e-01 -2.5454664e-01 ...  4.9250904e-02\n",
      "  -1.5464898e-02  2.3723869e-02]\n",
      " [-1.6553652e-01 -4.6099013e-01 -3.3905885e-01 ...  9.7206146e-02\n",
      "   1.6422240e-01  1.4359069e-01]\n",
      " [-1.7571995e-01 -5.4548883e-01 -3.0374822e-01 ...  9.8804995e-02\n",
      "   2.9195458e-01  2.2179511e-01]\n",
      " [-5.7403378e-02 -3.1795031e-01 -2.0877677e-01 ... -2.9015493e-02\n",
      "   1.9300550e-01  1.0513382e-01]]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "def find_scores_json_parents(root_dir=\".\"):\n",
    "    root_path = Path(root_dir)\n",
    "    # Find all 'scores.json' files recursively\n",
    "    matches = list(root_path.glob(\"**/scores.json\"))\n",
    "    # Extract parent folders\n",
    "    parent_folders = [str(file.parent) for file in matches]\n",
    "    return parent_folders\n",
    "\n",
    "all_submission_dirs = find_scores_json_parents(\".\")\n",
    "npys = []\n",
    "acc = {}\n",
    "index = {}\n",
    "for ii, dir in enumerate(all_submission_dirs):\n",
    "    print(f\"Submission{ii}:{dir}\")\n",
    "    json_path = os.path.join(dir, \"scores.json\")\n",
    "    npy_files = [f for f in os.listdir(dir) if f.endswith('.npy')]\n",
    "    npys.append(np.load(os.path.join(dir, npy_files[0]),allow_pickle=True).item())\n",
    "\n",
    "    with open(json_path,'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # Returns a Python dict/list\n",
    "        for key,values in data.items():\n",
    "            if '_' in key:\n",
    "                # Split into ['sub-01', 'movie-chaplin']\n",
    "                subject, movie_with_prefix = key.split('_')\n",
    "                \n",
    "                # Remove 'movie-' prefix to get 'chaplin'\n",
    "                movie = movie_with_prefix.replace('movie-', '')\n",
    "                \n",
    "                # Initialize subject if not exists\n",
    "                if subject not in acc:\n",
    "                    acc[subject] = {}\n",
    "                    index[subject] = {}\n",
    "                # Initialize movie if not exists\n",
    "                if movie not in acc[subject]:\n",
    "                    acc[subject][movie] = 0\n",
    "\n",
    "                # keep the best accuracy\n",
    "                if values>acc[subject][movie]:\n",
    "                    acc[subject][movie] = values\n",
    "                    index[subject][movie] = ii\n",
    "grand_mean = []\n",
    "for subject,subject_acc in acc.items():\n",
    "    movie_mean = np.mean(list(subject_acc.values()))\n",
    "    print(f\"Subject: {subject}, Expected OOD Mean Accuracy: {movie_mean:.4f}\")\n",
    "    grand_mean.append(movie_mean)\n",
    "\n",
    "grand_mean = np.mean(grand_mean)\n",
    "print(f\"Across-subject Expected OOD Mean Accuracy: {grand_mean:.4f}\")\n",
    "print(acc)\n",
    "print(index)\n",
    "\n",
    "# Now we get the actual submission file\n",
    "final_submission = npys[0].copy()\n",
    "print(f\"Stiching files from submissions...\")\n",
    "for subject, episodes_dict in final_submission.items():\n",
    "    for episode, values in episodes_dict.items():\n",
    "        matching_key = next(\n",
    "            (key for key in index[subject].keys() if episode.startswith(key)),\n",
    "            None\n",
    "        )\n",
    "        if matching_key is not None:\n",
    "            curr_index = index[subject][matching_key]\n",
    "        else:\n",
    "            ValueError(f\"No matching key found for episode '{episode}'.\")\n",
    "        \n",
    "        final_submission[subject][episode] = npys[curr_index][subject][episode]\n",
    "\n",
    "print(f\"Saving final submission file...\")\n",
    "os.makedirs(f\"./output/bestof{len(all_submission_dirs)}submissions\", exist_ok=True)\n",
    "save_path = os.path.join(f\"./output/bestof{len(all_submission_dirs)}submissions\", \"fmri_predictions_ood_combined.npy\")\n",
    "np.save(save_path, final_submission, allow_pickle=True)\n",
    "print(f'Saving final submission to {save_path}')\n",
    "zip_file = save_path.replace(\"npy\",\"zip\")\n",
    "with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "    zipf.write(save_path, os.path.basename(save_path))\n",
    "print(f\"Submission file successfully zipped as: {zip_file}\")\n",
    "print(final_submission['sub-01']['chaplin1'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d298d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(final_submission['sub-01']['chaplin1'],npys[2]['sub-01']['chaplin1'])  # Should be True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337e8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_encoding_default_2507061512', 'feature_encoding_enc_kernel-38_2507061602', 'feature_encoding_default_2506300839', 'ensemble_output_2507061554', 'feature_encoding_enc_kernel-45_2507061610', 'feature_encoding_enc_kernel-20_2507061542']\n",
      "Submission file successfully zipped as: ./output/ensemble_output_2507061621/fmri_predictions_friends_s7.zip\n"
     ]
    }
   ],
   "source": [
    "# First let's just take the average of several model predictions regardless of their architecture\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "target_filename = \"fmri_predictions_friends_s7.npy\" # CHANGE THIS\n",
    "\n",
    "ny_time = datetime.now(pytz.timezone(\"America/New_York\"))\n",
    "now = ny_time.strftime(\"%y%m%d%H%M\") \n",
    "ensemble_output_path = './output/ensemble_output_' + now\n",
    "foldernames = [] # [\"feature_encoding_enc_kernel-20_2507061542\",\"feature_encoding_default_2507061512\",\"feature_encoding_default_2506300839\"] # CHANGE THIS \n",
    "\n",
    "matching_folders = []\n",
    "for root, dirs, files in os.walk(\"./output/\"):\n",
    "    if target_filename in files:\n",
    "        # Get the folder name (the last part of the root path)\n",
    "        folder_name = os.path.basename(root)\n",
    "        matching_folders.append(folder_name)\n",
    "\n",
    "if not foldernames:\n",
    "     foldernames = matching_folders\n",
    "print(foldernames)\n",
    "\n",
    "os.makedirs(ensemble_output_path,exist_ok = True)  \n",
    "for ii,foldername in enumerate(foldernames):\n",
    "    submission_predictions = np.load(os.path.join(\"./output\",foldername,target_filename),allow_pickle=True).item()\n",
    "    if ii == 0:\n",
    "        all_submissions = submission_predictions\n",
    "    else:\n",
    "        # Add everything\n",
    "        for subject, episodes_dict in submission_predictions.items():\n",
    "            for episode, values in episodes_dict.items():\n",
    "                all_submissions[subject][episode] += values\n",
    "            # Optional: Display the structure and shapes of the predicted fMRI responses dictionary\n",
    "            # # Print the subject and episode number for Friends season 7\n",
    "            # print(f\"Subject: {subject}\")\n",
    "            # print(f\"  Number of Episodes: {len(episodes_dict)}\")\n",
    "            # # Print the predicted fMRI response shape for each episode\n",
    "            # for episode, predictions in episodes_dict.items():\n",
    "            #     print(f\"    - Episode: {episode}, Predicted fMRI shape: {predictions.shape}\")\n",
    "            # print(\"-\" * 40)  # Separator for clarity\n",
    "\n",
    "# Divide by total number of model predictions\n",
    "for subject, episodes_dict in submission_predictions.items():\n",
    "            for episode, values in episodes_dict.items():\n",
    "                all_submissions[subject][episode] /= len(foldernames)  \n",
    "# Save the output\n",
    "np.save(os.path.join(ensemble_output_path,target_filename),all_submissions,allow_pickle=True)\n",
    "\n",
    "# Zip the saved file for submission\n",
    "zip_file = os.path.join(ensemble_output_path,target_filename.replace(\"npy\",\"zip\"))\n",
    "with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "    zipf.write(os.path.join(ensemble_output_path,target_filename), os.path.basename(os.path.join(ensemble_output_path,target_filename)))\n",
    "print(f\"Submission file successfully zipped as: {zip_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19b805",
   "metadata": {},
   "source": [
    "==== Previous Notes that are unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf511ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.linear_model import LinearRegression\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import time\n",
    "# from collections import defaultdict\n",
    "# from pathlib import Path\n",
    "\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import IterableDataset\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b18207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(submission_predictions['sub-01']['s07e01a'],all_submissions['sub-01']['s07e01a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algonauts2025Dataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fmri_data: dict[str, np.ndarray],\n",
    "        feat_data: list[dict[str, np.ndarray]] | None = None,\n",
    "        sample_length: int | None = 128,\n",
    "        num_samples: int | None = None,\n",
    "        shuffle: bool = True,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        self.fmri_data = fmri_data\n",
    "        self.feat_data = feat_data\n",
    "\n",
    "        self.episode_list = list(fmri_data)\n",
    "        self.sample_length = sample_length\n",
    "        self.num_samples = num_samples\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def _iter_shuffle(self):\n",
    "        sample_idx = 0\n",
    "        while True:\n",
    "            episode_order = self._rng.permutation(len(self.episode_list))\n",
    "\n",
    "            for ii in episode_order:\n",
    "                episode = self.episode_list[ii]\n",
    "                feat_episode = episode[0] if isinstance(episode, tuple) else episode\n",
    "\n",
    "                fmri = torch.from_numpy(self.fmri_data[episode]).float()\n",
    "    \n",
    "                if self.feat_data:\n",
    "                    feats = [torch.from_numpy(data[feat_episode]).float() for data in self.feat_data]\n",
    "                else:\n",
    "                    feats = feat_samples = None\n",
    "\n",
    "                # Nb, fmri and feature length often off by 1 or 2.\n",
    "                # But assuming time locked to start.\n",
    "                length = fmri.shape[1]\n",
    "                if feats:\n",
    "                    length = min(length, min(feat.shape[0] for feat in feats))\n",
    "\n",
    "                if self.sample_length:\n",
    "                    # Random segment of run\n",
    "                    offset = int(self._rng.integers(0, length - self.sample_length + 1))\n",
    "                    fmri_sample = fmri[:, offset: offset + self.sample_length]\n",
    "                    if feats:\n",
    "                        feat_samples = [\n",
    "                            feat[offset: offset + self.sample_length] for feat in feats\n",
    "                        ]\n",
    "                else:\n",
    "                    # Take full run\n",
    "                    # Nb this only works for batch size 1 since runs are different length\n",
    "                    fmri_sample = fmri[:, :length]\n",
    "                    if feats:\n",
    "                        feat_samples = [feat[:length] for feat in feats]\n",
    "\n",
    "                if feat_samples:\n",
    "                    yield episode, fmri_sample, feat_samples\n",
    "                else:\n",
    "                    yield episode, fmri_sample\n",
    "\n",
    "                sample_idx += 1\n",
    "                if self.num_samples and sample_idx >= self.num_samples:\n",
    "                    return\n",
    "\n",
    "    def _iter_ordered(self):\n",
    "        sample_idx = 0\n",
    "        for episode in self.episode_list:\n",
    "            feat_episode = episode[0] if isinstance(episode, tuple) else episode\n",
    "            fmri = torch.from_numpy(self.fmri_data[episode]).float()\n",
    "            if self.feat_data:\n",
    "                feats = [torch.from_numpy(data[feat_episode]).float() for data in self.feat_data]\n",
    "            else:\n",
    "                feats = feat_samples = None\n",
    "\n",
    "            length = fmri.shape[1]\n",
    "            if feats:\n",
    "                length = min(length, min(feat.shape[0] for feat in feats))\n",
    "\n",
    "            sample_length = self.sample_length or length\n",
    "\n",
    "            for offset in range(0, length - sample_length + 1, sample_length):\n",
    "                fmri_sample = fmri[:, offset: offset + sample_length]\n",
    "                if feats:\n",
    "                    feat_samples = [feat[offset: offset + sample_length] for feat in feats]\n",
    "\n",
    "                if feat_samples:\n",
    "                    yield episode, fmri_sample, feat_samples\n",
    "                else:\n",
    "                    yield episode, fmri_sample\n",
    "\n",
    "                sample_idx += 1\n",
    "                if self.num_samples and sample_idx >= self.num_samples:\n",
    "                    return\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            yield from self._iter_shuffle()\n",
    "        else:\n",
    "            yield from self._iter_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dd571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Algonauts2025Dataset(\n",
    "    friends_val_fmri,\n",
    "    list(stimuli_features_friends.values()),\n",
    "    sample_length=None,\n",
    "    shuffle=False,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    *,\n",
    "    epoch: int,\n",
    "    model: torch.nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    use_cuda = device.type == \"cuda\"\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    step_time_m = AverageMeter()\n",
    "\n",
    "    samples = []\n",
    "    outputs = []\n",
    "\n",
    "    end = time.monotonic()\n",
    "    for batch_idx, (_, sample, feats) in enumerate(val_loader):\n",
    "        sample = sample.to(device)\n",
    "        feats = [feat.to(device) for feat in feats]\n",
    "        batch_size = sample.size(0)\n",
    "        data_time = time.monotonic() - end\n",
    "\n",
    "        # forward pass\n",
    "        output = model(feats)\n",
    "        loss = F.mse_loss(output, sample)\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        # end of iteration timing\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        step_time = time.monotonic() - end\n",
    "\n",
    "        loss_m.update(loss_item, batch_size)\n",
    "        data_time_m.update(data_time, batch_size)\n",
    "        step_time_m.update(step_time, batch_size)\n",
    "\n",
    "        N, S, L, C = sample.shape\n",
    "        assert N, S == (1, 4)\n",
    "        samples.append(sample.cpu().numpy().swapaxes(0, 1).reshape((S, N*L, C)))\n",
    "        outputs.append(output.cpu().numpy().swapaxes(0, 1).reshape((S, N*L, C)))\n",
    "\n",
    "        # Reset timer\n",
    "        end = time.monotonic()\n",
    "\n",
    "    # (S, N, C)\n",
    "    samples = np.concatenate(samples, axis=1)\n",
    "    outputs = np.concatenate(outputs, axis=1)\n",
    "\n",
    "    metrics = {}\n",
    "    model_pred = {}\n",
    "\n",
    "    # Encoding accuracy metrics\n",
    "    dim = samples.shape[-1]\n",
    "    acc = 0.0\n",
    "    acc_map = np.zeros(dim)\n",
    "    for ii, sub in enumerate(SUBJECTS):\n",
    "        y_true = samples[ii].reshape(-1, dim)\n",
    "        y_pred = outputs[ii].reshape(-1, dim)\n",
    "        metrics[f\"acc_map_sub-{sub}\"] = acc_map_i = pearsonr_score(y_true, y_pred)\n",
    "        metrics[f\"acc_sub-{sub}\"] = acc_i = np.mean(acc_map_i)\n",
    "        acc_map += acc_map_i / len(SUBJECTS)\n",
    "        acc += acc_i / len(SUBJECTS)\n",
    "        \n",
    "\n",
    "    metrics[\"acc_map_avg\"] = acc_map\n",
    "    metrics[\"acc_avg\"] = acc\n",
    "    accs_fmt = \",\".join(\n",
    "        f\"{val:.3f}\" for key, val in metrics.items() if key.startswith(\"acc_sub-\")\n",
    "    )\n",
    "\n",
    "    tput = batch_size / step_time_m.avg\n",
    "    print(\n",
    "        f\"Val: {epoch:>3d}\"\n",
    "        f\"  Loss: {loss_m.avg:#.3g}\"\n",
    "        f\"  Acc: {accs_fmt} ({acc:.3f})\"\n",
    "        f\"  Time: {data_time_m.avg:.3f},{step_time_m.avg:.3f} {tput:.0f}/s\"\n",
    "    )\n",
    "\n",
    "    return acc, metrics\n",
    "\n",
    "\n",
    "def pearsonr_score(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-7\n",
    ") -> np.ndarray:\n",
    "    assert y_true.ndim == y_pred.ndim == 2\n",
    "\n",
    "    y_true = y_true - y_true.mean(axis=0)\n",
    "    y_true = y_true / (np.linalg.norm(y_true, axis=0) + eps)\n",
    "\n",
    "    y_pred = y_pred - y_pred.mean(axis=0)\n",
    "    y_pred = y_pred / (np.linalg.norm(y_pred, axis=0) + eps)\n",
    "\n",
    "    score = (y_true * y_pred).sum(axis=0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume:\n",
    "# - X_val: Validation features (shape [n_samples, ...])\n",
    "# - y_val: Validation labels (shape [n_samples, ...])\n",
    "# - model_preds: Predictions from 5 models (shape [5, n_samples, ...])\n",
    "\n",
    "# Stack predictions (shape [n_samples, 5])\n",
    "X_stack = np.stack([model_preds[i] for i in range(5)], axis=-1)\n",
    "\n",
    "# Train a regressor to learn weights\n",
    "ensemble = LinearRegression()\n",
    "ensemble.fit(X_stack, y_val)  # Learns weights w1, ..., w5\n",
    "\n",
    "# For new data, combine predictions: final_pred = w1 * m1 + ... + w5 * m5\n",
    "new_preds = np.stack([model.predict(new_data) for model in models], axis=-1)\n",
    "final_pred = ensemble.predict(new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48bb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
