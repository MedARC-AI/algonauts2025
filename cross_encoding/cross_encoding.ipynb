{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-subject encoding\n",
    "\n",
    "Predicting each subject's activity using the synchronized activity of the other subjects. (Idk if any papers have done this, probably they have. Please someone lmk if they know of a reference.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS = (1, 2, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to: /home/connor/algonauts2025/cross_encoding/output/cross_encoding_v3\n"
     ]
    }
   ],
   "source": [
    "root_dir = Path(\"..\").resolve()\n",
    "\n",
    "data_dir = root_dir / \"algonauts_2025.competitors\"\n",
    "\n",
    "out_dir = Path(\".\") / \"output/cross_encoding_v3\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(\"Saving output to:\", out_dir.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Aligned cross-subject fmri data loader. The data loader samples clips of synchronized activity from the same friends episodes across subjects. Each clip is shape `(n_subs, sample_length, dim)`.\n",
    "\n",
    "We also load pre-extracted features of the shape `(sample_length, dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_friends_run(run: str):\n",
    "    match = re.match(r\"s([0-9]+)e([0-9]+)([a-z])\", run)\n",
    "    if match is None:\n",
    "        raise ValueError(f\"Invalid friends run {run}\")\n",
    "\n",
    "    season = int(match.group(1))\n",
    "    episode = int(match.group(2))\n",
    "    part = match.group(3)\n",
    "    return season, episode, part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_algonauts2025_friends_fmri(\n",
    "    root: str | Path,\n",
    "    subjects: list[int] | None = None,\n",
    "    seasons: list[int] | None = None,\n",
    ") -> dict[str, np.ndarray]:\n",
    "    subjects = subjects or SUBJECTS\n",
    "    seasons = seasons or list(range(1, 7))\n",
    "\n",
    "    files = {\n",
    "        sub: h5py.File(\n",
    "            Path(root)\n",
    "            / f\"fmri/sub-{sub:02d}/func\"\n",
    "            / f\"sub-{sub:02d}_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5\"\n",
    "        )\n",
    "        for sub in subjects\n",
    "    }\n",
    "\n",
    "    episode_key_maps = defaultdict(dict)\n",
    "    seasons_set = set(seasons)\n",
    "    for sub, file in files.items():\n",
    "        for key in file.keys():\n",
    "            entities = dict([ent.split(\"-\", 1) for ent in key.split(\"_\")])\n",
    "            episode = entities[\"task\"]\n",
    "            season, _, _ = parse_friends_run(episode)\n",
    "            if season in seasons_set:\n",
    "                episode_key_maps[episode][sub] = key\n",
    "\n",
    "    episode_list = sorted(\n",
    "        [\n",
    "            episode for episode, map in episode_key_maps.items()\n",
    "            if len(map) == len(subjects)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data = {}\n",
    "    for episode in episode_list:\n",
    "        samples = []\n",
    "        length = None\n",
    "        for sub in subjects:\n",
    "            key = episode_key_maps[episode][sub]\n",
    "            sample = files[sub][key][:]\n",
    "            sub_length = len(sample)\n",
    "            samples.append(sample)\n",
    "            length = min(length, sub_length) if length else sub_length\n",
    "        data[episode] = np.stack([sample[:length] for sample in samples])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie10_run(run: str):\n",
    "    match = re.match(r\"([a-z]+)([0-9]+)\", run)\n",
    "    if match is None:\n",
    "        raise ValueError(f\"Invalid movie run {run}\")\n",
    "\n",
    "    movie = match.group(1)\n",
    "    part = int(match.group(2))\n",
    "    return movie, part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_algonauts2025_movie10_fmri(\n",
    "    root: str | Path,\n",
    "    subjects: list[int] | None = None,\n",
    "    movies: list[str] | None = None,\n",
    "    runs: list[int] | None = None,\n",
    ") -> dict[str, np.ndarray]:\n",
    "    subjects = subjects or SUBJECTS\n",
    "    movies = movies or [\"bourne\", \"wolf\", \"figures\", \"life\"]\n",
    "    runs = runs or [1, 2]\n",
    "\n",
    "    files = {\n",
    "        sub: h5py.File(\n",
    "            Path(root)\n",
    "            / f\"fmri/sub-{sub:02d}/func\"\n",
    "            / f\"sub-{sub:02d}_task-movie10_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_bold.h5\"\n",
    "        )\n",
    "        for sub in subjects\n",
    "    }\n",
    "\n",
    "    episode_key_maps = defaultdict(dict)\n",
    "    movies_set = set(movies)\n",
    "    for sub, file in files.items():\n",
    "        for key in file.keys():\n",
    "            entities = dict([ent.split(\"-\", 1) for ent in key.split(\"_\")])\n",
    "            episode = entities[\"task\"]\n",
    "            run = int(entities.get(\"run\", 1))\n",
    "            movie, _ = parse_movie10_run(episode)\n",
    "            if movie in movies_set and run in runs:\n",
    "                episode_key_maps[(episode, run)][sub] = key\n",
    "\n",
    "    episode_list = sorted(\n",
    "        [\n",
    "            episode for episode, map in episode_key_maps.items()\n",
    "            if len(map) == len(subjects)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data = {}\n",
    "    for episode in episode_list:\n",
    "        samples = []\n",
    "        length = None\n",
    "        for sub in subjects:\n",
    "            key = episode_key_maps[episode][sub]\n",
    "            sample = files[sub][key][:]\n",
    "            sub_length = len(sample)\n",
    "            samples.append(sample)\n",
    "            length = min(length, sub_length) if length else sub_length\n",
    "        data[episode] = np.stack([sample[:length] for sample in samples])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_train_fmri = load_algonauts2025_friends_fmri(data_dir, seasons=range(1, 6))\n",
    "friends_val_fmri = load_algonauts2025_friends_fmri(data_dir, seasons=[6])\n",
    "movie10_test_fmri = load_algonauts2025_movie10_fmri(data_dir, runs=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['s01e01a', 's01e01b', 's01e02a', 's01e02b', 's01e03a', 's01e03b', 's01e04a', 's01e04b', 's01e05a', 's01e05b', 's01e06a', 's01e06b', 's01e07a', 's01e07b', 's01e08a', 's01e08b', 's01e09a', 's01e09b', 's01e10a', 's01e10b', 's01e11a', 's01e11b', 's01e12a', 's01e12b', 's01e13a', 's01e13b', 's01e14a', 's01e14b', 's01e15a', 's01e15b', 's01e16a', 's01e16b', 's01e17a', 's01e17b', 's01e18a', 's01e18b', 's01e19a', 's01e19b', 's01e20a', 's01e20b', 's01e21a', 's01e21b', 's01e22a', 's01e22b', 's01e23a', 's01e23b', 's01e24a', 's01e24b', 's02e01a', 's02e01b', 's02e02a', 's02e02b', 's02e03a', 's02e03b', 's02e04a', 's02e04b', 's02e05a', 's02e05b', 's02e06a', 's02e06b', 's02e07a', 's02e07b', 's02e08a', 's02e08b', 's02e09a', 's02e09b', 's02e10a', 's02e10b', 's02e11a', 's02e11b', 's02e12a', 's02e12b', 's02e13a', 's02e13b', 's02e14a', 's02e14b', 's02e15a', 's02e15b', 's02e16a', 's02e16b', 's02e17a', 's02e17b', 's02e18a', 's02e18b', 's02e19a', 's02e19b', 's02e20a', 's02e20b', 's02e21a', 's02e21b', 's02e22a', 's02e22b', 's02e23a', 's02e23b', 's02e24a', 's02e24b', 's03e01a', 's03e01b', 's03e02a', 's03e02b', 's03e03a', 's03e03b', 's03e04a', 's03e04b', 's03e05a', 's03e05b', 's03e06a', 's03e06b', 's03e07a', 's03e07b', 's03e08a', 's03e08b', 's03e09a', 's03e09b', 's03e10a', 's03e10b', 's03e11a', 's03e11b', 's03e12a', 's03e12b', 's03e13a', 's03e13b', 's03e14a', 's03e14b', 's03e15a', 's03e15b', 's03e16a', 's03e16b', 's03e17a', 's03e17b', 's03e18a', 's03e18b', 's03e19a', 's03e19b', 's03e20a', 's03e20b', 's03e21a', 's03e21b', 's03e22a', 's03e22b', 's03e23a', 's03e23b', 's03e24a', 's03e24b', 's03e25a', 's03e25b', 's04e02a', 's04e02b', 's04e03a', 's04e03b', 's04e04a', 's04e04b', 's04e05a', 's04e05b', 's04e06a', 's04e06b', 's04e07a', 's04e07b', 's04e08a', 's04e08b', 's04e09a', 's04e09b', 's04e10a', 's04e10b', 's04e11a', 's04e11b', 's04e12a', 's04e12b', 's04e13a', 's04e14a', 's04e14b', 's04e15a', 's04e15b', 's04e16a', 's04e16b', 's04e17a', 's04e17b', 's04e18a', 's04e18b', 's04e19a', 's04e19b', 's04e20a', 's04e20b', 's04e21a', 's04e21b', 's04e22a', 's04e22b', 's04e23a', 's04e23b', 's04e23c', 's04e23d', 's05e01a', 's05e01b', 's05e02a', 's05e02b', 's05e03a', 's05e03b', 's05e04a', 's05e04b', 's05e05a', 's05e05b', 's05e06a', 's05e06b', 's05e07a', 's05e07b', 's05e08a', 's05e08b', 's05e09a', 's05e09b', 's05e10a', 's05e10b', 's05e11a', 's05e11b', 's05e12a', 's05e12b', 's05e13a', 's05e13b', 's05e14a', 's05e14b', 's05e15a', 's05e15b', 's05e16a', 's05e16b', 's05e17a', 's05e17b', 's05e18a', 's05e18b', 's05e19a', 's05e19b', 's05e20b', 's05e21a', 's05e21b', 's05e22a', 's05e22b', 's05e23a', 's05e23b', 's05e23c', 's05e23d'])\n",
      "dict_keys(['s06e01a', 's06e01b', 's06e02a', 's06e02b', 's06e03b', 's06e04a', 's06e04b', 's06e05a', 's06e05b', 's06e06a', 's06e06b', 's06e07a', 's06e07b', 's06e08a', 's06e08b', 's06e09a', 's06e09b', 's06e10a', 's06e10b', 's06e11a', 's06e11b', 's06e12a', 's06e12b', 's06e13a', 's06e13b', 's06e14a', 's06e14b', 's06e15a', 's06e15b', 's06e15c', 's06e15d', 's06e17a', 's06e17b', 's06e18a', 's06e18b', 's06e19a', 's06e19b', 's06e20a', 's06e20b', 's06e21a', 's06e21b', 's06e22a', 's06e22b', 's06e23a', 's06e23b', 's06e24a', 's06e24b', 's06e24c', 's06e24d'])\n",
      "dict_keys([('bourne01', 1), ('bourne02', 1), ('bourne03', 1), ('bourne04', 1), ('bourne05', 1), ('bourne06', 1), ('bourne07', 1), ('bourne08', 1), ('bourne09', 1), ('bourne10', 1), ('figures01', 1), ('figures02', 1), ('figures03', 1), ('figures04', 1), ('figures05', 1), ('figures06', 1), ('figures07', 1), ('figures08', 1), ('figures09', 1), ('figures10', 1), ('figures11', 1), ('figures12', 1), ('life01', 1), ('life02', 1), ('life03', 1), ('life04', 1), ('life05', 1), ('wolf01', 1), ('wolf02', 1), ('wolf03', 1), ('wolf04', 1), ('wolf05', 1), ('wolf06', 1), ('wolf07', 1), ('wolf08', 1), ('wolf09', 1), ('wolf10', 1), ('wolf11', 1), ('wolf12', 1), ('wolf13', 1), ('wolf14', 1), ('wolf15', 1), ('wolf16', 1), ('wolf17', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(friends_train_fmri.keys())\n",
    "print(friends_val_fmri.keys())\n",
    "print(movie10_test_fmri.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape (NTC): (4, 468, 1000) float32\n"
     ]
    }
   ],
   "source": [
    "sample = friends_train_fmri[\"s01e05b\"]\n",
    "print(\"Sample shape (NTC):\", sample.shape, sample.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algonauts2025Dataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fmri_data: dict[str, np.ndarray],\n",
    "        feat_data: list[dict[str, np.ndarray]] | None = None,\n",
    "        sample_length: int | None = 128,\n",
    "        num_samples: int | None = None,\n",
    "        shuffle: bool = True,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        self.fmri_data = fmri_data\n",
    "        self.feat_data = feat_data\n",
    "\n",
    "        self.episode_list = list(fmri_data)\n",
    "        self.sample_length = sample_length\n",
    "        self.num_samples = num_samples\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def _iter_shuffle(self):\n",
    "        sample_idx = 0\n",
    "        while True:\n",
    "            episode_order = self._rng.permutation(len(self.episode_list))\n",
    "\n",
    "            for ii in episode_order:\n",
    "                episode = self.episode_list[ii]\n",
    "                fmri = torch.from_numpy(self.fmri_data[episode])\n",
    "\n",
    "                if self.feat_data:\n",
    "                    feats = [torch.from_numpy(data[episode]) for data in self.feat_data]\n",
    "                else:\n",
    "                    feats = feat_samples = None\n",
    "\n",
    "                # Nb, fmri and feature length often off by 1 or 2.\n",
    "                # But assuming time locked to start.\n",
    "                length = fmri.shape[1]\n",
    "                if feats:\n",
    "                    length = min(length, min(feat.shape[0] for feat in feats))\n",
    "\n",
    "                if self.sample_length:\n",
    "                    # Random segment of run\n",
    "                    offset = int(self._rng.integers(0, length - self.sample_length + 1))\n",
    "                    fmri_sample = fmri[:, offset: offset + self.sample_length]\n",
    "                    if feats:\n",
    "                        feat_samples = [\n",
    "                            feat[offset: offset + self.sample_length] for feat in feats\n",
    "                        ]\n",
    "                else:\n",
    "                    # Take full run\n",
    "                    # Nb this only works for batch size 1 since runs are different length\n",
    "                    fmri_sample = fmri[:, :length]\n",
    "                    if feats:\n",
    "                        feat_samples = [feat[:length] for feat in feats]\n",
    "\n",
    "                if feat_samples:\n",
    "                    yield episode, fmri_sample, feat_samples\n",
    "                else:\n",
    "                    yield episode, fmri_sample\n",
    "\n",
    "                sample_idx += 1\n",
    "                if self.num_samples and sample_idx >= self.num_samples:\n",
    "                    return\n",
    "\n",
    "    def _iter_ordered(self):\n",
    "        sample_idx = 0\n",
    "        for episode in self.episode_list:\n",
    "            fmri = torch.from_numpy(self.fmri_data[episode])\n",
    "            if self.feat_data:\n",
    "                feats = [torch.from_numpy(data[episode]) for data in self.feat_data]\n",
    "            else:\n",
    "                feats = feat_samples = None\n",
    "\n",
    "            length = fmri.shape[1]\n",
    "            if feats:\n",
    "                length = min(length, min(feat.shape[0] for feat in feats))\n",
    "\n",
    "            sample_length = self.sample_length or length\n",
    "\n",
    "            for offset in range(0, length - sample_length + 1, sample_length):\n",
    "                fmri_sample = fmri[:, offset: offset + sample_length]\n",
    "                if feats:\n",
    "                    feat_samples = [feat[offset: offset + sample_length] for feat in feats]\n",
    "\n",
    "                if feat_samples:\n",
    "                    yield episode, fmri_sample, feat_samples\n",
    "                else:\n",
    "                    yield episode, fmri_sample\n",
    "\n",
    "                sample_idx += 1\n",
    "                if self.num_samples and sample_idx >= self.num_samples:\n",
    "                    return\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            yield from self._iter_shuffle()\n",
    "        else:\n",
    "            yield from self._iter_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_train_dataset = Algonauts2025Dataset(\n",
    "    friends_train_fmri,\n",
    "    sample_length=64,\n",
    "    num_samples=10000,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 170811.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time=0.061s, MB/s=160208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_bytes = 0\n",
    "tic = time.monotonic()\n",
    "for task, fmri_sample in tqdm(friends_train_dataset):\n",
    "    total_bytes += fmri_sample.numel() * 4\n",
    "rt = time.monotonic() - tic\n",
    "tput = total_bytes / 1024 ** 2 / rt \n",
    "print(f\"run time={rt:.3f}s, MB/s={tput:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Model architecture is a simple linear encoder and decoder for each subject. The encoder/decoder is \"factorized\" into a depthwise conv1d (to align data temporally), and a linear projection (to align data spatially).\n",
    "\n",
    "For each subject, the input to the decoder is the average of the latents for the other three subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    \"\"\"Conv1d layer with a causal mask, to only \"attend\" to past time points.\"\"\"\n",
    "    attn_mask: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: str | int = 0,\n",
    "        dilation: int = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        assert kernel_size % 2 == 1, \"causal conv requires odd kernel size\"\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        attn_mask = torch.zeros(kernel_size)\n",
    "        attn_mask[:kernel_size // 2 + 1] = 1.0\n",
    "        self.weight.data.mul_(attn_mask)\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        weight = self.weight * self.attn_mask\n",
    "        return F.conv1d(\n",
    "            input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        kernel_size: int = 11,\n",
    "        causal: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        conv_layer = CausalConv1d if causal else nn.Conv1d\n",
    "        self.conv = conv_layer(\n",
    "            in_features,\n",
    "            in_features,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            groups=in_features,\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (N, L, C)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        kernel_size: int = 11,\n",
    "        causal: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        conv_layer = CausalConv1d if causal else nn.Conv1d\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.conv = conv_layer(\n",
    "            out_features,\n",
    "            out_features,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            groups=out_features,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (N, L, C)\n",
    "        x = self.fc(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLinear(\n",
      "  (conv): CausalConv1d(1000, 1000, kernel_size=(11,), stride=(1,), padding=same, groups=1000)\n",
      "  (fc): Linear(in_features=1000, out_features=256, bias=True)\n",
      ")\n",
      "torch.Size([16, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "encoder = ConvLinear(\n",
    "    in_features=1000,\n",
    "    out_features=256,\n",
    "    causal=True\n",
    ")\n",
    "print(encoder)\n",
    "\n",
    "# (N, L, C)\n",
    "x = torch.randn(16, 64, 1000)\n",
    "embed = encoder.forward(x)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossSubjectConvLinearEncoderV2(nn.Module):\n",
    "    \"\"\"\n",
    "    - Minor refactoring\n",
    "    - Added subject shared linear projections\n",
    "    \"\"\"\n",
    "    weight: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_subjects: int = 4,\n",
    "        fmri_dim: int = 1000,\n",
    "        embed_dim: int = 256,\n",
    "        encoder_kernel_size: int = 11,\n",
    "        decoder_kernel_size: int = 11,\n",
    "        normalize: bool = False,\n",
    "        with_shared_encoder: bool = True,\n",
    "        with_shared_decoder: bool = True,\n",
    "        with_subject_encoders: bool = True,\n",
    "        with_subject_decoders: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert with_shared_encoder or with_subject_encoders\n",
    "        assert with_shared_decoder or with_subject_decoders\n",
    "\n",
    "        self.num_subjects = num_subjects\n",
    "\n",
    "        if with_shared_encoder:\n",
    "            self.shared_encoder = nn.Linear(fmri_dim, embed_dim)\n",
    "        else:\n",
    "            self.register_module(\"shared_encoder\", None)\n",
    "\n",
    "        if with_subject_encoders:\n",
    "            if encoder_kernel_size > 1:\n",
    "                encoder_fn = partial(LinearConv, kernel_size=encoder_kernel_size)\n",
    "            else:\n",
    "                encoder_fn = nn.Linear\n",
    "            self.subject_encoders = nn.ModuleList(\n",
    "                [encoder_fn(fmri_dim, embed_dim) for _ in range(num_subjects)]\n",
    "            )\n",
    "        else:\n",
    "            self.register_module(\"subject_encoders\", None)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim) if normalize else nn.Identity()\n",
    "\n",
    "        if with_shared_decoder:\n",
    "            self.shared_decoder = nn.Linear(embed_dim, fmri_dim)\n",
    "        else:\n",
    "            self.register_module(\"shared_decoder\", None)\n",
    "        \n",
    "        if with_subject_decoders:\n",
    "            if decoder_kernel_size > 1:\n",
    "                decoder_fn = partial(ConvLinear, kernel_size=decoder_kernel_size)\n",
    "            else:\n",
    "                decoder_fn = nn.Linear\n",
    "            self.subject_decoders = nn.ModuleList(\n",
    "                [decoder_fn(embed_dim, fmri_dim) for _ in range(num_subjects)]\n",
    "            )\n",
    "        else:\n",
    "            self.register_module(\"subject_decoders\", None)\n",
    "\n",
    "        # todo: could learn the averaging weights\n",
    "        weight = (1.0 - torch.eye(self.num_subjects)) / (self.num_subjects - 1.0)\n",
    "        self.register_buffer(\"weight\", weight)\n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # input: (N, S, L, C)\n",
    "        # subject specific encoders\n",
    "\n",
    "        if self.shared_encoder is not None:\n",
    "            shared_embed = self.shared_encoder(input)\n",
    "        else:\n",
    "            shared_embed = 0.0\n",
    "        \n",
    "        if self.subject_encoders is not None:\n",
    "            subject_embeds = torch.stack(\n",
    "                [encoder(input[:, ii]) for ii, encoder in enumerate(self.subject_encoders)],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            subject_embeds = 0.0\n",
    "\n",
    "        embed = self.norm(shared_embed + subject_embeds)\n",
    "\n",
    "        # average pool the latents for all but target subject\n",
    "        embed = torch.einsum(\"nslc,ts->ntlc\", embed, self.weight)\n",
    "\n",
    "        # subject specific decoders\n",
    "        if self.shared_decoder is not None:\n",
    "            shared_output = self.shared_decoder(embed)\n",
    "        else:\n",
    "            shared_output = 0.0\n",
    "        \n",
    "        if self.subject_decoders is not None:\n",
    "            subject_outputs = torch.stack(\n",
    "                [decoder(embed[:, ii]) for ii, decoder in enumerate(self.subject_decoders)],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            subject_outputs = 0.0\n",
    "        output = shared_output + subject_outputs\n",
    "        return output\n",
    "    \n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "        nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossSubjectConvLinearEncoderV2(\n",
      "  (shared_encoder): Linear(in_features=1000, out_features=256, bias=True)\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x LinearConv(\n",
      "      (fc): Linear(in_features=1000, out_features=256, bias=True)\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (shared_decoder): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  (subject_decoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([16, 4, 64, 1000])\n"
     ]
    }
   ],
   "source": [
    "cross_encoder = CrossSubjectConvLinearEncoderV2()\n",
    "print(cross_encoder)\n",
    "\n",
    "# (N, S, L, C)\n",
    "x = torch.randn(16, 4, 64, 1000)\n",
    "z = cross_encoder.forward(x)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Basic training loop, AdamW, no lr decay, no bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.utils import AverageMeter, random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    *,\n",
    "    epoch: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch_batches: int | None,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.train()\n",
    "    \n",
    "    use_cuda = device.type == \"cuda\"\n",
    "    if use_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    step_time_m = AverageMeter()\n",
    "\n",
    "    epoch_batches = len(train_loader) if epoch_batches is None else epoch_batches\n",
    "    first_step = epoch * epoch_batches\n",
    "\n",
    "    end = time.monotonic()\n",
    "    for batch_idx, (_, sample) in enumerate(train_loader):\n",
    "        step = first_step + batch_idx\n",
    "        sample = sample.to(device)\n",
    "        batch_size = sample.size(0)\n",
    "        data_time = time.monotonic() - end\n",
    "\n",
    "        # forward pass\n",
    "        output = model(sample)\n",
    "        loss = F.mse_loss(output, sample)\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        if math.isnan(loss_item) or math.isinf(loss_item):\n",
    "            raise RuntimeError(\"NaN/Inf loss encountered on step %d; exiting\", step)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # end of iteration timing\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        step_time = time.monotonic() - end\n",
    "\n",
    "        loss_m.update(loss_item, batch_size)\n",
    "        data_time_m.update(data_time, batch_size)\n",
    "        step_time_m.update(step_time, batch_size)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            tput = batch_size / step_time_m.avg\n",
    "            if use_cuda:\n",
    "                alloc_mem_gb = torch.cuda.max_memory_allocated() / 1e9\n",
    "                res_mem_gb = torch.cuda.max_memory_reserved() / 1e9\n",
    "            else:\n",
    "                alloc_mem_gb = res_mem_gb = 0.0\n",
    "\n",
    "            print(\n",
    "                f\"Train: {epoch:>3d} [{batch_idx:>3d}/{epoch_batches}][{step:>6d}]\"\n",
    "                f\"  Loss: {loss_m.val:#.3g} ({loss_m.avg:#.3g})\"\n",
    "                f\"  Time: {data_time_m.avg:.3f},{step_time_m.avg:.3f} {tput:.0f}/s\"\n",
    "                f\"  Mem: {alloc_mem_gb:.2f},{res_mem_gb:.2f} GB\"\n",
    "            )\n",
    "\n",
    "        # Restart timer for next iteration\n",
    "        end = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(\n",
    "    *,\n",
    "    epoch: int,\n",
    "    model: torch.nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    use_cuda = device.type == \"cuda\"\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    step_time_m = AverageMeter()\n",
    "\n",
    "    samples = []\n",
    "    outputs = []\n",
    "\n",
    "    end = time.monotonic()\n",
    "    for batch_idx, (_, sample) in enumerate(val_loader):\n",
    "        sample = sample.to(device)\n",
    "        batch_size = sample.size(0)\n",
    "        data_time = time.monotonic() - end\n",
    "\n",
    "        # forward pass\n",
    "        output = model(sample)\n",
    "        loss = F.mse_loss(output, sample)\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        # end of iteration timing\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        step_time = time.monotonic() - end\n",
    "\n",
    "        loss_m.update(loss_item, batch_size)\n",
    "        data_time_m.update(data_time, batch_size)\n",
    "        step_time_m.update(step_time, batch_size)\n",
    "\n",
    "        N, S, L, C = sample.shape\n",
    "        assert N, S == (1, 4)\n",
    "        samples.append(sample.cpu().numpy().swapaxes(0, 1).reshape((S, N*L, C)))\n",
    "        outputs.append(output.cpu().numpy().swapaxes(0, 1).reshape((S, N*L, C)))\n",
    "\n",
    "        # Reset timer\n",
    "        end = time.monotonic()\n",
    "\n",
    "    # (S, N, C)\n",
    "    samples = np.concatenate(samples, axis=1)\n",
    "    outputs = np.concatenate(outputs, axis=1)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Encoding accuracy metrics\n",
    "    dim = samples.shape[-1]\n",
    "    acc = 0.0\n",
    "    acc_map = np.zeros(dim)\n",
    "    for ii, sub in enumerate(SUBJECTS):\n",
    "        y_true = samples[ii].reshape(-1, dim)\n",
    "        y_pred = outputs[ii].reshape(-1, dim)\n",
    "        metrics[f\"acc_map_sub-{sub}\"] = acc_map_i = pearsonr_score(y_true, y_pred)\n",
    "        metrics[f\"acc_sub-{sub}\"] = acc_i = np.mean(acc_map_i)\n",
    "        acc_map += acc_map_i / len(SUBJECTS)\n",
    "        acc += acc_i / len(SUBJECTS)\n",
    "\n",
    "    metrics[\"acc_map_avg\"] = acc_map\n",
    "    metrics[\"acc_avg\"] = acc\n",
    "    accs_fmt = \",\".join(\n",
    "        f\"{val:.3f}\" for key, val in metrics.items() if key.startswith(\"acc_sub-\")\n",
    "    )\n",
    "\n",
    "    tput = batch_size / step_time_m.avg\n",
    "    print(\n",
    "        f\"Val: {epoch:>3d}\"\n",
    "        f\"  Loss: {loss_m.avg:#.3g}\"\n",
    "        f\"  Acc: {accs_fmt} ({acc:.3f})\"\n",
    "        f\"  Time: {data_time_m.avg:.3f},{step_time_m.avg:.3f} {tput:.0f}/s\"\n",
    "    )\n",
    "\n",
    "    return acc, metrics\n",
    "\n",
    "\n",
    "def pearsonr_score(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-7\n",
    ") -> np.ndarray:\n",
    "    assert y_true.ndim == y_pred.ndim == 2\n",
    "\n",
    "    y_true = y_true - y_true.mean(axis=0)\n",
    "    y_true = y_true / (np.linalg.norm(y_true, axis=0) + eps)\n",
    "\n",
    "    y_pred = y_pred - y_pred.mean(axis=0)\n",
    "    y_pred = y_pred / (np.linalg.norm(y_pred, axis=0) + eps)\n",
    "\n",
    "    score = (y_true * y_pred).sum(axis=0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3315\n",
    "batch_size = 16\n",
    "sample_length = 64\n",
    "n_train_samples = 2000\n",
    "lr = 3e-4\n",
    "weight_decay = 0.001\n",
    "epochs = 10\n",
    "\n",
    "embed_dim = 64\n",
    "encoder_kernel_size = 7\n",
    "decoder_kernel_size = 0\n",
    "with_shared = True\n",
    "with_subject = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 3315, 'batch_size': 16, 'sample_length': 64, 'n_train_samples': 2000, 'lr': 0.0003, 'weight_decay': 0.001, 'epochs': 10, 'embed_dim': 64, 'encoder_kernel_size': 7, 'decoder_kernel_size': 0, 'with_shared': True, 'with_subject': True}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    k: globals()[k] for k in\n",
    "    [\n",
    "        \"seed\",\n",
    "        \"batch_size\",\n",
    "        \"sample_length\",\n",
    "        \"n_train_samples\",\n",
    "        \"lr\",\n",
    "        \"weight_decay\",\n",
    "        \"epochs\",\n",
    "        \"embed_dim\",\n",
    "        \"encoder_kernel_size\",\n",
    "        \"decoder_kernel_size\",\n",
    "        \"with_shared\",\n",
    "        \"with_subject\",\n",
    "        ]\n",
    "}\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "random_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Algonauts2025Dataset(\n",
    "    friends_train_fmri,\n",
    "    sample_length=sample_length,\n",
    "    num_samples=n_train_samples,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "val_dataset = Algonauts2025Dataset(\n",
    "    friends_val_fmri,\n",
    "    sample_length=None,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_dataset = Algonauts2025Dataset(\n",
    "    movie10_test_fmri,\n",
    "    sample_length=None,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (16, 4, 64, 1000)\n"
     ]
    }
   ],
   "source": [
    "_, sample = next(iter(train_loader))\n",
    "print(\"Sample shape:\", tuple(sample.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CrossSubjectConvLinearEncoderV2(\n",
      "  (shared_encoder): Linear(in_features=1000, out_features=64, bias=True)\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x LinearConv(\n",
      "      (fc): Linear(in_features=1000, out_features=64, bias=True)\n",
      "      (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=same, groups=64)\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (shared_decoder): Linear(in_features=64, out_features=1000, bias=True)\n",
      "  (subject_decoders): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=64, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Num params: 0.65M\n"
     ]
    }
   ],
   "source": [
    "model = CrossSubjectConvLinearEncoderV2(\n",
    "    embed_dim=embed_dim,\n",
    "    encoder_kernel_size=encoder_kernel_size,\n",
    "    decoder_kernel_size=decoder_kernel_size,\n",
    "    with_shared_encoder=with_shared,\n",
    "    with_shared_decoder=with_shared,\n",
    "    with_subject_encoders=with_subject,\n",
    "    with_subject_decoders=with_subject,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Model:\", model)\n",
    "print(f\"Num params: {param_count/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "epoch_batches = n_train_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train friends s1-5\n",
      "Train:   0 [  0/125][     0]  Loss: 0.363 (0.363)  Time: 0.013,0.021 757/s  Mem: 0.19,0.23 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.361 (0.370)  Time: 0.005,0.009 1738/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.342 (0.365)  Time: 0.004,0.008 2057/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.346 (0.359)  Time: 0.004,0.007 2190/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.347 (0.355)  Time: 0.003,0.007 2273/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.350 (0.352)  Time: 0.003,0.007 2323/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.323 (0.349)  Time: 0.003,0.007 2355/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.330 (0.346)  Time: 0.003,0.007 2379/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.332 (0.344)  Time: 0.003,0.007 2401/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.327 (0.342)  Time: 0.003,0.007 2400/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.329 (0.341)  Time: 0.003,0.007 2414/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.322 (0.340)  Time: 0.003,0.007 2424/s  Mem: 0.20,0.27 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.311 (0.338)  Time: 0.003,0.007 2432/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   0  Loss: 0.325  Acc: 0.306,0.308,0.327,0.285 (0.306)  Time: 0.001,0.002 423/s\n",
      "Eval movie10\n",
      "Val:   0  Loss: 0.337  Acc: 0.275,0.253,0.275,0.245 (0.262)  Time: 0.001,0.002 420/s\n",
      "Train friends s1-5\n",
      "Train:   1 [  5/125][   130]  Loss: 0.325 (0.322)  Time: 0.005,0.009 1719/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.318 (0.324)  Time: 0.004,0.008 2062/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.334 (0.324)  Time: 0.004,0.007 2155/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.321 (0.325)  Time: 0.004,0.007 2218/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.319 (0.324)  Time: 0.003,0.007 2262/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.318 (0.323)  Time: 0.003,0.007 2327/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.324 (0.323)  Time: 0.003,0.007 2364/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.324 (0.322)  Time: 0.003,0.007 2397/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.315 (0.322)  Time: 0.003,0.007 2413/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.318 (0.322)  Time: 0.003,0.007 2425/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.323 (0.322)  Time: 0.003,0.007 2429/s  Mem: 0.20,0.27 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.320 (0.322)  Time: 0.003,0.007 2437/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   1  Loss: 0.320  Acc: 0.326,0.327,0.349,0.302 (0.326)  Time: 0.002,0.002 406/s\n",
      "Eval movie10\n",
      "Val:   1  Loss: 0.333  Acc: 0.295,0.270,0.294,0.262 (0.280)  Time: 0.001,0.002 407/s\n",
      "Train friends s1-5\n",
      "Train:   2 [  0/125][   250]  Loss: 0.308 (0.308)  Time: 0.005,0.011 1466/s  Mem: 0.20,0.23 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.317 (0.318)  Time: 0.005,0.008 1902/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.310 (0.318)  Time: 0.004,0.008 2118/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.312 (0.318)  Time: 0.004,0.007 2217/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.299 (0.317)  Time: 0.003,0.007 2290/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.332 (0.318)  Time: 0.003,0.007 2312/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.320 (0.317)  Time: 0.003,0.007 2334/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.316 (0.317)  Time: 0.003,0.007 2349/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.312 (0.318)  Time: 0.003,0.007 2355/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.315 (0.317)  Time: 0.003,0.007 2376/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.310 (0.317)  Time: 0.003,0.007 2394/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.307 (0.317)  Time: 0.003,0.007 2408/s  Mem: 0.20,0.27 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.312 (0.318)  Time: 0.003,0.007 2422/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   2  Loss: 0.318  Acc: 0.334,0.335,0.358,0.309 (0.334)  Time: 0.001,0.002 470/s\n",
      "Eval movie10\n",
      "Val:   2  Loss: 0.331  Acc: 0.303,0.276,0.301,0.267 (0.287)  Time: 0.001,0.002 524/s\n",
      "Train friends s1-5\n",
      "Train:   3 [  5/125][   380]  Loss: 0.309 (0.311)  Time: 0.005,0.011 1472/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.401 (0.320)  Time: 0.004,0.008 2016/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.311 (0.317)  Time: 0.003,0.007 2220/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.310 (0.316)  Time: 0.003,0.007 2333/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.329 (0.316)  Time: 0.003,0.007 2411/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.308 (0.316)  Time: 0.003,0.007 2452/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.317 (0.315)  Time: 0.003,0.006 2467/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.304 (0.316)  Time: 0.003,0.006 2482/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.321 (0.316)  Time: 0.003,0.006 2489/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.323 (0.316)  Time: 0.003,0.006 2499/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.325 (0.316)  Time: 0.003,0.006 2507/s  Mem: 0.20,0.27 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.309 (0.316)  Time: 0.003,0.006 2514/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   3  Loss: 0.316  Acc: 0.339,0.338,0.363,0.313 (0.339)  Time: 0.001,0.002 451/s\n",
      "Eval movie10\n",
      "Val:   3  Loss: 0.330  Acc: 0.308,0.280,0.305,0.271 (0.291)  Time: 0.001,0.002 470/s\n",
      "Train friends s1-5\n",
      "Train:   4 [  0/125][   500]  Loss: 0.308 (0.308)  Time: 0.004,0.010 1602/s  Mem: 0.20,0.23 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.296 (0.311)  Time: 0.003,0.007 2305/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.309 (0.312)  Time: 0.003,0.007 2402/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.314 (0.313)  Time: 0.003,0.007 2427/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.305 (0.314)  Time: 0.003,0.007 2450/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.311 (0.313)  Time: 0.003,0.006 2466/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.313 (0.312)  Time: 0.003,0.006 2477/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.309 (0.312)  Time: 0.003,0.006 2486/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.322 (0.312)  Time: 0.003,0.006 2496/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.314 (0.312)  Time: 0.003,0.006 2505/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.309 (0.313)  Time: 0.003,0.006 2506/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.320 (0.313)  Time: 0.003,0.006 2501/s  Mem: 0.20,0.27 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.316 (0.313)  Time: 0.003,0.006 2505/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   4  Loss: 0.316  Acc: 0.341,0.341,0.367,0.316 (0.341)  Time: 0.001,0.002 455/s\n",
      "Eval movie10\n",
      "Val:   4  Loss: 0.330  Acc: 0.310,0.282,0.309,0.272 (0.293)  Time: 0.001,0.002 506/s\n",
      "Train friends s1-5\n",
      "Train:   5 [  5/125][   630]  Loss: 0.312 (0.316)  Time: 0.005,0.009 1839/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.323 (0.314)  Time: 0.004,0.007 2243/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.309 (0.312)  Time: 0.003,0.007 2402/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.316 (0.312)  Time: 0.003,0.006 2489/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.316 (0.313)  Time: 0.003,0.006 2546/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.309 (0.312)  Time: 0.003,0.006 2576/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.301 (0.312)  Time: 0.003,0.006 2603/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.311 (0.312)  Time: 0.003,0.006 2607/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.302 (0.311)  Time: 0.003,0.006 2601/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.304 (0.311)  Time: 0.003,0.006 2598/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.313 (0.311)  Time: 0.003,0.006 2593/s  Mem: 0.20,0.27 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.311 (0.311)  Time: 0.003,0.006 2590/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   5  Loss: 0.315  Acc: 0.344,0.344,0.368,0.317 (0.343)  Time: 0.001,0.002 442/s\n",
      "Eval movie10\n",
      "Val:   5  Loss: 0.329  Acc: 0.312,0.284,0.309,0.275 (0.295)  Time: 0.001,0.002 463/s\n",
      "Train friends s1-5\n",
      "Train:   6 [  0/125][   750]  Loss: 0.321 (0.321)  Time: 0.004,0.013 1230/s  Mem: 0.20,0.23 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.320 (0.316)  Time: 0.003,0.008 2100/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.313 (0.314)  Time: 0.003,0.007 2287/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.309 (0.314)  Time: 0.003,0.007 2353/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.310 (0.313)  Time: 0.003,0.007 2401/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.315 (0.313)  Time: 0.003,0.007 2425/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.327 (0.312)  Time: 0.003,0.007 2444/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.305 (0.312)  Time: 0.003,0.007 2458/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.316 (0.311)  Time: 0.003,0.006 2472/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.304 (0.311)  Time: 0.003,0.006 2483/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.320 (0.310)  Time: 0.003,0.006 2494/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.302 (0.311)  Time: 0.003,0.006 2499/s  Mem: 0.20,0.27 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.307 (0.311)  Time: 0.003,0.006 2501/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   6  Loss: 0.315  Acc: 0.345,0.346,0.370,0.320 (0.345)  Time: 0.001,0.002 446/s\n",
      "Eval movie10\n",
      "Val:   6  Loss: 0.329  Acc: 0.314,0.287,0.311,0.276 (0.297)  Time: 0.001,0.002 504/s\n",
      "Train friends s1-5\n",
      "Train:   7 [  5/125][   880]  Loss: 0.300 (0.307)  Time: 0.004,0.009 1796/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.294 (0.306)  Time: 0.003,0.007 2287/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.310 (0.307)  Time: 0.003,0.007 2380/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.296 (0.308)  Time: 0.003,0.007 2429/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.316 (0.309)  Time: 0.003,0.006 2464/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.305 (0.309)  Time: 0.003,0.006 2490/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.306 (0.308)  Time: 0.003,0.006 2504/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.318 (0.309)  Time: 0.003,0.006 2512/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.306 (0.308)  Time: 0.003,0.006 2523/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.304 (0.309)  Time: 0.003,0.006 2532/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.302 (0.309)  Time: 0.003,0.006 2534/s  Mem: 0.20,0.27 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.311 (0.309)  Time: 0.003,0.006 2542/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   7  Loss: 0.314  Acc: 0.347,0.346,0.371,0.320 (0.346)  Time: 0.001,0.002 443/s\n",
      "Eval movie10\n",
      "Val:   7  Loss: 0.328  Acc: 0.315,0.287,0.312,0.277 (0.298)  Time: 0.001,0.002 471/s\n",
      "Train friends s1-5\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.317 (0.317)  Time: 0.004,0.012 1289/s  Mem: 0.20,0.23 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.314 (0.308)  Time: 0.004,0.009 1817/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.317 (0.307)  Time: 0.003,0.008 2102/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.303 (0.308)  Time: 0.003,0.007 2230/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.305 (0.308)  Time: 0.003,0.007 2305/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.314 (0.308)  Time: 0.003,0.007 2352/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.304 (0.309)  Time: 0.003,0.007 2389/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.299 (0.308)  Time: 0.003,0.007 2418/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.295 (0.308)  Time: 0.003,0.007 2437/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.305 (0.308)  Time: 0.003,0.007 2453/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.303 (0.308)  Time: 0.003,0.006 2464/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.309 (0.308)  Time: 0.003,0.006 2472/s  Mem: 0.20,0.27 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.306 (0.308)  Time: 0.003,0.006 2483/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   8  Loss: 0.314  Acc: 0.348,0.347,0.372,0.321 (0.347)  Time: 0.001,0.002 459/s\n",
      "Eval movie10\n",
      "Val:   8  Loss: 0.328  Acc: 0.316,0.288,0.312,0.277 (0.298)  Time: 0.001,0.002 480/s\n",
      "Train friends s1-5\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.306 (0.304)  Time: 0.005,0.009 1790/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.310 (0.307)  Time: 0.004,0.007 2170/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.302 (0.309)  Time: 0.003,0.007 2298/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.323 (0.309)  Time: 0.003,0.007 2368/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.299 (0.309)  Time: 0.003,0.007 2417/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.306 (0.309)  Time: 0.003,0.007 2443/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.307 (0.309)  Time: 0.003,0.006 2463/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.302 (0.309)  Time: 0.003,0.006 2476/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.312 (0.309)  Time: 0.003,0.006 2491/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.306 (0.308)  Time: 0.003,0.006 2503/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.308 (0.308)  Time: 0.003,0.006 2511/s  Mem: 0.20,0.27 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.320 (0.309)  Time: 0.003,0.006 2517/s  Mem: 0.20,0.27 GB\n",
      "Eval friends s6\n",
      "Val:   9  Loss: 0.314  Acc: 0.348,0.348,0.373,0.322 (0.348)  Time: 0.001,0.002 450/s\n",
      "Eval movie10\n",
      "Val:   9  Loss: 0.328  Acc: 0.317,0.289,0.312,0.277 (0.299)  Time: 0.001,0.002 470/s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Train friends s1-5\")\n",
    "    train_one_epoch(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        epoch_batches=epoch_batches,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"Eval friends s6\")\n",
    "    val_acc, val_metrics = validate(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"Eval movie10\")\n",
    "    test_acc, test_metrics = validate(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        val_loader=test_loader,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "```\n",
    "seed = 3315\n",
    "batch_size = 16\n",
    "sample_length = 64\n",
    "n_train_samples = 2000\n",
    "lr = 3e-4\n",
    "weight_decay = 0.001\n",
    "epochs = 10\n",
    "\n",
    "embed_dim = 64\n",
    "kernel_size = 0\n",
    "with_shared = True\n",
    "with_subject = True\n",
    "```\n",
    "\n",
    "```\n",
    "Eval friends s6\n",
    "Val:   9  Loss: 0.316  Acc: 0.340,0.339,0.365,0.314 (0.340)  Time: 0.001,0.002 534/s\n",
    "Eval movie10\n",
    "Val:   9  Loss: 0.330  Acc: 0.307,0.282,0.304,0.272 (0.291)  Time: 0.001,0.002 564/s\n",
    "```\n",
    "\n",
    "```\n",
    "embed_dim = 128\n",
    "```\n",
    "```\n",
    "Eval friends s6\n",
    "Val:   9  Loss: 0.316  Acc: 0.341,0.340,0.365,0.312 (0.339)  Time: 0.001,0.002 446/s\n",
    "Eval movie10\n",
    "Val:   9  Loss: 0.330  Acc: 0.307,0.281,0.303,0.269 (0.290)  Time: 0.001,0.002 519/s\n",
    "```\n",
    "\n",
    "```\n",
    "embed_dim = 32\n",
    "```\n",
    "\n",
    "```\n",
    "Eval friends s6\n",
    "Val:   9  Loss: 0.317  Acc: 0.336,0.336,0.362,0.311 (0.336)  Time: 0.001,0.002 514/s\n",
    "Eval movie10\n",
    "Val:   9  Loss: 0.330  Acc: 0.304,0.279,0.303,0.270 (0.289)  Time: 0.001,0.002 582/s\n",
    "```\n",
    "\n",
    "```\n",
    "embed_dim = 64\n",
    "kernel_size = 7\n",
    "```\n",
    "\n",
    "```\n",
    "Eval friends s6\n",
    "Val:   9  Loss: 0.314  Acc: 0.349,0.349,0.375,0.322 (0.349)  Time: 0.001,0.002 433/s\n",
    "Eval movie10\n",
    "Val:   9  Loss: 0.328  Acc: 0.318,0.289,0.314,0.278 (0.300)  Time: 0.001,0.002 472/s\n",
    "```\n",
    "\n",
    "```\n",
    "embed_dim = 32\n",
    "kernel_size = 7\n",
    "```\n",
    "\n",
    "```\n",
    "Eval friends s6\n",
    "Val:   9  Loss: 0.315  Acc: 0.343,0.344,0.369,0.318 (0.343)  Time: 0.001,0.002 436/s\n",
    "Eval movie10\n",
    "Val:   9  Loss: 0.329  Acc: 0.312,0.284,0.310,0.275 (0.295)  Time: 0.001,0.002 452/s\n",
    "```\n",
    "\n",
    "```\n",
    "embed_dim = 64\n",
    "encoder_kernel_size = 7\n",
    "decoder_kernel_size = 7\n",
    "```\n",
    "```\n",
    "Eval friends s6\n",
    "Val:   9  Loss: 0.314  Acc: 0.348,0.348,0.373,0.322 (0.348)  Time: 0.001,0.002 450/s\n",
    "Eval movie10\n",
    "Val:   9  Loss: 0.328  Acc: 0.317,0.289,0.312,0.277 (0.299)  Time: 0.001,0.002 470/s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_dir / \"ckpt.pt\", \"wb\") as f:\n",
    "    torch.save(\n",
    "        {\n",
    "            \"config\": config,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"val_metrics\": val_metrics,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"test_acc\": test_acc,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
