out_dir: output_ense_2/feature_encoding_tt

model:
  embed_dim: 192
  encoder_kernel_size: 45
  decoder_kernel_size: 0
  hidden_model: null
  global_pool: avg
  encoder_causal: false
  encoder_positive: false
  encoder_blockwise: false
  pool_num_heads: 3
  with_shared_decoder: true
  with_subject_decoders: true

transformer:
  num_heads: 3
  depth: 6
  mlp_ratio: 4.0

conv1dnext:
  depth: 6
  kernel_size: 11
  causal: false

features:
  internvl3_8b/layers.20:
    model: internvl3_8b_8bit
    layer: language_model.model.layers.20.post_attention_layernorm
  
  internvl3_8b/layers.10:
    model: internvl3_8b_8bit
    layer: language_model.model.layers.10.post_attention_layernorm

  internvl3_14b/layers.30:
    model: InternVL3_14B
    layer: language_model.model.layers.30.post_attention_layernorm
  
  internvl3_8b_multiframe/layer.20:
    model: InternVL3_8B_multiframe
    layer: language_model.model.layers.20.post_attention_layernorm
    
  qwen-2-5-omni-7b/layers.20:
    model: qwen-2-5-omni-7b
    layer: model.layers.20.post_attention_layernorm
  
  qwen-2-5-omni-3b/layers.20:
    model: qwen2-5_3B
    layer: model.layers.20.post_attention_layernorm

  whisper/layers.12:
    model: whisper
    layer: layers.12.fc2
   
  whisper/layers.31:
    model: whisper
    layer: layers.31.fc2
  
  videomae2/model.blocks.25.mlp.fc2gp:
    model: videomae2
    layer: model.blocks.25.mlp.fc2gp
  
  videomae2/model.blocks.25.mlp.fc2avg:
    model: videomae2
    layer: model.blocks.25.mlp.fc2avg

  MFCC/audio:
    model: MFCC
    layer: audio
  
  bert-base-uncased/language_pooler_output: # Not sure why this is not working
    model: bert-base-uncased
    layer: language_pooler_output
  
  bert-base-uncased/language_last_hidden_state: # Not sure why this is not working
    model: bert-base-uncased
    layer: language_last_hidden_state
  
  modernBert/mean_pooling_output:
    model: modernBert
    layer: mean_pooling_output

  emonet/visual:
    model: emonet
    layer: visual
    stem: _20perTR

  llama_3.2_1B/layers.7:
    model: Llama-3.2-1B
    layer: model.layers.7

  llama_3.2_3B/layers.7:
    model: Llama-3.2-3B
    layer: model.layers.7

  llama_3.2_3B/layers.5:
    model: Llama-3.2-3B
    layer: model.layers.5

  llama_3.2_3B/layers.15:
    model: Llama-3.2-3B
    layer: model.layers.15
  
  llama_3.2_3B/layers.19:
    model: Llama-3.2-3B
    layer: model.layers.19
  
  llama_3.2_3B/layers.11:
    model: Llama-3.2-3B
    layer: model.layers.11

  llama_3.2_1b/layers.7:
    model: meta-llama__Llama-3.2-1B
    layer: model.layers.7
    stem: context-long
  
  llama_3.2_1b/layers.15:
    model: meta-llama__Llama-3.2-1B
    layer: model.layers.15
    stem: context-long

  dinov2/blocks.5:
    model: vit_giant_patch14_reg4_dinov2.lvd142m
    layer: blocks.5

  dinov2/blocks.15:
    model: vit_giant_patch14_reg4_dinov2.lvd142m
    layer: blocks.15

  dinov2/blocks.25:
    model: vit_giant_patch14_reg4_dinov2.lvd142m
    layer: blocks.25

  dinov2/blocks.35:
    model: vit_giant_patch14_reg4_dinov2.lvd142m
    layer: blocks.35

  dinov2/embedding:
    model: vit_giant_patch14_reg4_dinov2.lvd142m
    layer: embedding
  
  dinov2-giant/pooler_output:
    model: dinov2-giant
    layer: pooler_output
    
  vjepa2/encoder.layernorm_avg:
    model: vjepa2_avg_feat
    layer: encoder.layernorm_avg

include_features:
  - llama_3.2_3B/layers.11
  - whisper/layers.12
  - qwen-2-5-omni-3b/layers.20
  - internvl3_14b/layers.30
  - vjepa2/encoder.layernorm_avg

datasets:
  train:
    filter:
      seasons: [1, 2, 3, 4, 5, 6]
      movies: ["wolf","figures"]
    sample_length: 64
    num_samples: 2000
    shuffle: True
    seed: 42

  # val_s6:
  #   filter:
  #     seasons: [6]
  #     movies: []
  #   sample_length: null
  #   num_samples: null
  #   shuffle: false

  val_bourne:
    filter:
      seasons: []
      movies: ["bourne"]
    sample_length: null
    num_samples: null
    shuffle: false

  val_life:
    filter:
      seasons: []
      movies: ["life"]
    sample_length: null
    num_samples: null
    shuffle: false

batch_size: 16
lr: 3e-4
weight_decay: 0.1
epochs: 10

checkpoint: null
freeze_decoder: false

datasets_root: null

seed: 3315
overwrite: true
device: cuda