{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20ce2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e17540",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS = (1, 2, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ea727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to: /home/clane/algonauts2025/long_context_encoding/output\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/home/clane/algonauts_2025.competitors\")\n",
    "feat_dir = Path(\"/home/clane/algonauts2025.huggingface/features\")\n",
    "\n",
    "out_dir = Path(\".\") / \"output\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "print(\"Saving output to:\", out_dir.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ef8db",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Aligned cross-subject fmri data loader. The data loader samples clips of synchronized activity from the same friends episodes across subjects. Each clip is shape `(n_subs, sample_length, dim)`.\n",
    "\n",
    "We also load pre-extracted features of the shape `(sample_length, dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a1b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_friends_run(run: str):\n",
    "    match = re.match(r\"s([0-9]+)e([0-9]+)([a-z])\", run)\n",
    "    if match is None:\n",
    "        raise ValueError(f\"Invalid friends run {run}\")\n",
    "\n",
    "    season = int(match.group(1))\n",
    "    episode = int(match.group(2))\n",
    "    part = match.group(3)\n",
    "    return season, episode, part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9670546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algonauts2025FriendsFmri:\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str | Path,\n",
    "        subjects: list[int] | None = None,\n",
    "        seasons: list[int] | None = None,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.subjects = subjects or SUBJECTS\n",
    "        self.seasons = seasons or list(range(1, 7))\n",
    "\n",
    "        files = {\n",
    "            sub: h5py.File(\n",
    "                Path(root)\n",
    "                / f\"sub-{sub:02d}/func\"\n",
    "                / f\"sub-{sub:02d}_task-friends_space-MNI152NLin2009cAsym_atlas-Schaefer18_parcel-1000Par7Net_desc-s123456_bold.h5\"\n",
    "            )\n",
    "            for sub in self.subjects\n",
    "        }\n",
    "\n",
    "        episode_key_maps = defaultdict(dict)\n",
    "        seasons_set = set(self.seasons)\n",
    "        for sub, file in files.items():\n",
    "            for key in file.keys():\n",
    "                episode = key.split(\"-\")[-1]  # 'ses-066_task-s06e24d'\n",
    "                season, _, _ = parse_friends_run(episode)\n",
    "                if season in seasons_set:\n",
    "                    episode_key_maps[episode][sub] = key\n",
    "\n",
    "        episode_list = sorted(\n",
    "            [\n",
    "                episode for episode, map in episode_key_maps.items()\n",
    "                if len(map) == len(self.subjects)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        data = {}\n",
    "        for episode in episode_list:\n",
    "            samples = []\n",
    "            length = None\n",
    "            for sub in self.subjects:\n",
    "                key = episode_key_maps[episode][sub]\n",
    "                sample = files[sub][key][:]\n",
    "                sub_length = len(sample)\n",
    "                samples.append(sample)\n",
    "                length = min(length, sub_length) if length else sub_length\n",
    "            data[episode] = np.stack([sample[:length] for sample in samples])\n",
    "\n",
    "        self.episode_list = episode_list \n",
    "        self._data = data\n",
    "    \n",
    "    def get(self, episode: str) -> np.ndarray:\n",
    "        return self._data[episode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb73ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data = Algonauts2025FriendsFmri(\n",
    "    root=data_dir / \"fmri\",\n",
    "    seasons=range(1, 7),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc24ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape (NTC): (4, 468, 1000) float32\n"
     ]
    }
   ],
   "source": [
    "sample = fmri_data.get(\"s01e05b\")\n",
    "print(\"Sample shape (NTC):\", sample.shape, sample.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40d92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algonauts2025FriendsFeatures:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str | Path,\n",
    "        layer: str,\n",
    "        episode_list: list[str] | None = None,\n",
    "    ):\n",
    "        self.path = path\n",
    "        self.layer = layer\n",
    "\n",
    "        file = h5py.File(path)\n",
    "\n",
    "        data = {}\n",
    "        episode_set = set(episode_list) if episode_list else None\n",
    "        episode_list = []\n",
    "        for key in file:\n",
    "            if not episode_set or key in episode_set:\n",
    "                data[key] = file[key][layer][:]\n",
    "                episode_list.append(key)\n",
    "\n",
    "        self.episode_list = episode_list\n",
    "        self._data = data\n",
    "\n",
    "    def get(self, episode: str) -> np.ndarray:\n",
    "        return self._data[episode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e53acf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_data = Algonauts2025FriendsFeatures(\n",
    "    path=feat_dir / \"friends/meta-llama__Llama-3.2-1B/context-short_window-16.h5\",\n",
    "    layer=\"model.layers.11\",\n",
    "    episode_list=fmri_data.episode_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c20591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape (TC): (468, 2048) float32\n"
     ]
    }
   ],
   "source": [
    "feat = feat_data.get(\"s01e05b\")\n",
    "print(\"Feature shape (TC):\", feat.shape, feat.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfd7da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algonauts2025FriendsDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_data: Algonauts2025FriendsFeatures,\n",
    "        fmri_data: Algonauts2025FriendsFmri,\n",
    "        seasons: list[int] | None = None,\n",
    "        sample_length: int | None = 128,\n",
    "        num_samples: int | None = None,\n",
    "        shuffle: bool = True,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        self.feat_data = feat_data\n",
    "        self.fmri_data = fmri_data\n",
    "        self.seasons = seasons or list(range(1, 7))\n",
    "\n",
    "        episode_list = []\n",
    "        seasons_set = set(self.seasons)\n",
    "        for episode in fmri_data.episode_list:\n",
    "            season, _, _ = parse_friends_run(episode)\n",
    "            if season in seasons_set:\n",
    "                episode_list.append(episode)\n",
    "        self.episode_list = episode_list\n",
    "\n",
    "        self.sample_length = sample_length\n",
    "        self.num_samples = num_samples\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def _iter_shuffle(self):\n",
    "        sample_idx = 0\n",
    "        while True:\n",
    "            episode_order = self._rng.permutation(len(self.episode_list))\n",
    "\n",
    "            for ii in episode_order:\n",
    "                episode = self.episode_list[ii]\n",
    "                feat = torch.from_numpy(self.feat_data.get(episode))\n",
    "                fmri = torch.from_numpy(self.fmri_data.get(episode))\n",
    "\n",
    "                # Nb, fmri and feature length often off by 1 or 2.\n",
    "                # But assuming time locked to start.\n",
    "                length = min(feat.shape[0], fmri.shape[1])\n",
    "\n",
    "                if self.sample_length:\n",
    "                    # Random segment of run\n",
    "                    offset = int(self._rng.integers(0, length - self.sample_length + 1))\n",
    "                    feat_sample = feat[offset: offset + self.sample_length]\n",
    "                    fmri_sample = fmri[:, offset: offset + self.sample_length]\n",
    "                else:\n",
    "                    # Take full run\n",
    "                    # Nb this only works for batch size 1 since runs are different length\n",
    "                    feat_sample = feat[:length]\n",
    "                    fmri_sample = fmri[:, :length]\n",
    "\n",
    "                yield episode, feat_sample, fmri_sample\n",
    "\n",
    "                sample_idx += 1\n",
    "                if self.num_samples and sample_idx >= self.num_samples:\n",
    "                    return\n",
    "\n",
    "    def _iter_ordered(self):\n",
    "        sample_idx = 0\n",
    "        for episode in self.episode_list:\n",
    "            feat = torch.from_numpy(self.feat_data.get(episode))\n",
    "            fmri = torch.from_numpy(self.fmri_data.get(episode))\n",
    "\n",
    "            length = min(feat.shape[0], fmri.shape[1])\n",
    "            sample_length = self.sample_length or length\n",
    "\n",
    "            for offset in range(0, length - sample_length + 1, sample_length):\n",
    "                feat_sample = feat[offset: offset + sample_length]\n",
    "                fmri_sample = fmri[:, offset: offset + sample_length]\n",
    "                yield episode, feat_sample, fmri_sample\n",
    "\n",
    "                sample_idx += 1\n",
    "                if self.num_samples and sample_idx >= self.num_samples:\n",
    "                    return\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            yield from self._iter_shuffle()\n",
    "        else:\n",
    "            yield from self._iter_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "127dbab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Algonauts2025FriendsDataset(\n",
    "    feat_data,\n",
    "    fmri_data,\n",
    "    seasons=range(1, 6),\n",
    "    sample_length=64,\n",
    "    num_samples=10000,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2087a2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 50584.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time=0.207s, MB/s=71338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_bytes = 0\n",
    "tic = time.monotonic()\n",
    "for task, feat_sample, fmri_sample in tqdm(dataset):\n",
    "    total_bytes += feat_sample.numel() * 4 + fmri_sample.numel() * 4\n",
    "rt = time.monotonic() - tic\n",
    "tput = total_bytes / 1024 ** 2 / rt \n",
    "print(f\"run time={rt:.3f}s, MB/s={tput:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7dcdd5",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db161ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b2f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    \"\"\"Conv1d layer with a causal mask, to only \"attend\" to past time points.\"\"\"\n",
    "    attn_mask: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: str | int = 0,\n",
    "        dilation: int = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        assert kernel_size % 2 == 1, \"causal conv requires odd kernel size\"\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        attn_mask = torch.zeros(kernel_size)\n",
    "        attn_mask[:kernel_size // 2 + 1] = 1.0\n",
    "        self.weight.data.mul_(attn_mask)\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        weight = self.weight * self.attn_mask\n",
    "        return F.conv1d(\n",
    "            input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28955ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        kernel_size: int = 11,\n",
    "        causal: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        conv_layer = CausalConv1d if causal else nn.Conv1d\n",
    "        self.conv = conv_layer(\n",
    "            in_features,\n",
    "            in_features,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            groups=in_features,\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (N, L, C)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        kernel_size: int = 11,\n",
    "        causal: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        conv_layer = CausalConv1d if causal else nn.Conv1d\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.conv = conv_layer(\n",
    "            out_features,\n",
    "            out_features,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            groups=out_features,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (N, L, C)\n",
    "        x = self.fc(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "441bbe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLinear(\n",
      "  (conv): CausalConv1d(1000, 1000, kernel_size=(11,), stride=(1,), padding=same, groups=1000)\n",
      "  (fc): Linear(in_features=1000, out_features=256, bias=True)\n",
      ")\n",
      "torch.Size([16, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "encoder = ConvLinear(\n",
    "    in_features=1000,\n",
    "    out_features=256,\n",
    "    causal=True\n",
    ")\n",
    "print(encoder)\n",
    "\n",
    "# (N, L, C)\n",
    "x = torch.randn(16, 64, 1000)\n",
    "embed = encoder.forward(x)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70199949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSubjectConvLinearEncoder(nn.Module):\n",
    "    weight: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_subjects: int = 4,\n",
    "        feat_dim: int = 2048,\n",
    "        embed_dim: int = 256,\n",
    "        target_dim: int = 1000,\n",
    "        kernel_size: int = 11,\n",
    "        causal: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_subjects = num_subjects\n",
    "\n",
    "        self.norm = nn.LayerNorm(feat_dim)\n",
    "        self.feat_embed = nn.Linear(feat_dim, embed_dim)\n",
    "\n",
    "        self.shared_encoder = ConvLinear(\n",
    "            embed_dim, target_dim, kernel_size=kernel_size, causal=causal,\n",
    "        )\n",
    "        self.subject_encoders = nn.ModuleList(\n",
    "            [\n",
    "                ConvLinear(\n",
    "                    embed_dim, target_dim, kernel_size=kernel_size, causal=causal\n",
    "                )\n",
    "                for _ in range(num_subjects)\n",
    "            ]\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # input: (N, L, D)\n",
    "        # output: (N, S, L, C)\n",
    "        embed = self.feat_embed(self.norm(input))\n",
    "        shared_output = self.shared_encoder(embed)\n",
    "        subject_output = torch.stack(\n",
    "            [encoder(embed) for encoder in self.subject_encoders],\n",
    "            dim=1,\n",
    "        )\n",
    "        output = subject_output + shared_output[:, None]\n",
    "        return output\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "        nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83409095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = MultiSubjectConvLinearEncoder()\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c200f6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 64, 1000])\n"
     ]
    }
   ],
   "source": [
    "# (N, L, C)\n",
    "x = torch.randn(16, 64, 2048)\n",
    "z = encoder.forward(x)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6f492",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49557a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.utils import AverageMeter, random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "108d94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    *,\n",
    "    epoch: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch_batches: int | None,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.train()\n",
    "    \n",
    "    use_cuda = device.type == \"cuda\"\n",
    "    if use_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    step_time_m = AverageMeter()\n",
    "\n",
    "    epoch_batches = len(train_loader) if epoch_batches is None else epoch_batches\n",
    "    first_step = epoch * epoch_batches\n",
    "\n",
    "    end = time.monotonic()\n",
    "    for batch_idx, (_, feat, sample) in enumerate(train_loader):\n",
    "        step = first_step + batch_idx\n",
    "        feat = feat.to(device)\n",
    "        sample = sample.to(device)\n",
    "        batch_size = feat.size(0)\n",
    "        data_time = time.monotonic() - end\n",
    "\n",
    "        # forward pass\n",
    "        output = model(feat)\n",
    "        loss = F.mse_loss(output, sample)\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        if math.isnan(loss_item) or math.isinf(loss_item):\n",
    "            raise RuntimeError(\"NaN/Inf loss encountered on step %d; exiting\", step)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # end of iteration timing\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        step_time = time.monotonic() - end\n",
    "\n",
    "        loss_m.update(loss_item, batch_size)\n",
    "        data_time_m.update(data_time, batch_size)\n",
    "        step_time_m.update(step_time, batch_size)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            tput = batch_size / step_time_m.avg\n",
    "            if use_cuda:\n",
    "                alloc_mem_gb = torch.cuda.max_memory_allocated() / 1e9\n",
    "                res_mem_gb = torch.cuda.max_memory_reserved() / 1e9\n",
    "            else:\n",
    "                alloc_mem_gb = res_mem_gb = 0.0\n",
    "\n",
    "            print(\n",
    "                f\"Train: {epoch:>3d} [{batch_idx:>3d}/{epoch_batches}][{step:>6d}]\"\n",
    "                f\"  Loss: {loss_m.val:#.3g} ({loss_m.avg:#.3g})\"\n",
    "                f\"  Time: {data_time_m.avg:.3f},{step_time_m.avg:.3f} {tput:.0f}/s\"\n",
    "                f\"  Mem: {alloc_mem_gb:.2f},{res_mem_gb:.2f} GB\"\n",
    "            )\n",
    "\n",
    "        # Restart timer for next iteration\n",
    "        end = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05d84900",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(\n",
    "    *,\n",
    "    epoch: int,\n",
    "    model: torch.nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    use_cuda = device.type == \"cuda\"\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    step_time_m = AverageMeter()\n",
    "\n",
    "    samples = []\n",
    "    outputs = []\n",
    "\n",
    "    end = time.monotonic()\n",
    "    for batch_idx, (_, feat, sample) in enumerate(val_loader):\n",
    "        feat = feat.to(device)\n",
    "        sample = sample.to(device)\n",
    "        batch_size = feat.size(0)\n",
    "        data_time = time.monotonic() - end\n",
    "\n",
    "        # forward pass\n",
    "        output = model(feat)\n",
    "        loss = F.mse_loss(output, sample)\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        # end of iteration timing\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        step_time = time.monotonic() - end\n",
    "\n",
    "        loss_m.update(loss_item, batch_size)\n",
    "        data_time_m.update(data_time, batch_size)\n",
    "        step_time_m.update(step_time, batch_size)\n",
    "\n",
    "        N, S, L, C = sample.shape\n",
    "        assert N, S == (1, 4)\n",
    "        samples.append(sample.cpu().numpy().swapaxes(0, 1).reshape((S, N*L, C)))\n",
    "        outputs.append(output.cpu().numpy().swapaxes(0, 1).reshape((S, N*L, C)))\n",
    "\n",
    "        # Reset timer\n",
    "        end = time.monotonic()\n",
    "\n",
    "    # (S, N, C)\n",
    "    samples = np.concatenate(samples, axis=1)\n",
    "    outputs = np.concatenate(outputs, axis=1)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Encoding accuracy metrics\n",
    "    dim = samples.shape[-1]\n",
    "    acc = 0.0\n",
    "    acc_map = np.zeros(dim)\n",
    "    for ii, sub in enumerate(SUBJECTS):\n",
    "        y_true = samples[ii].reshape(-1, dim)\n",
    "        y_pred = outputs[ii].reshape(-1, dim)\n",
    "        metrics[f\"acc_map_sub-{sub}\"] = acc_map_i = pearsonr_score(y_true, y_pred)\n",
    "        metrics[f\"acc_sub-{sub}\"] = acc_i = np.mean(acc_map_i)\n",
    "        acc_map += acc_map_i / len(SUBJECTS)\n",
    "        acc += acc_i / len(SUBJECTS)\n",
    "\n",
    "    metrics[\"acc_map_avg\"] = acc_map\n",
    "    metrics[\"acc_avg\"] = acc\n",
    "    accs_fmt = \",\".join(\n",
    "        f\"{val:.3f}\" for key, val in metrics.items() if key.startswith(\"acc_sub-\")\n",
    "    )\n",
    "\n",
    "    tput = batch_size / step_time_m.avg\n",
    "    print(\n",
    "        f\"Val: {epoch:>3d}\"\n",
    "        f\"  Loss: {loss_m.avg:#.3g}\"\n",
    "        f\"  Acc: {accs_fmt} ({acc:.3f})\"\n",
    "        f\"  Time: {data_time_m.avg:.3f},{step_time_m.avg:.3f} {tput:.0f}/s\"\n",
    "    )\n",
    "\n",
    "    return acc, metrics\n",
    "\n",
    "\n",
    "def pearsonr_score(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-7\n",
    ") -> np.ndarray:\n",
    "    assert y_true.ndim == y_pred.ndim == 2\n",
    "\n",
    "    y_true = y_true - y_true.mean(axis=0)\n",
    "    y_true = y_true / (np.linalg.norm(y_true, axis=0) + eps)\n",
    "\n",
    "    y_pred = y_pred - y_pred.mean(axis=0)\n",
    "    y_pred = y_pred / (np.linalg.norm(y_pred, axis=0) + eps)\n",
    "\n",
    "    score = (y_true * y_pred).sum(axis=0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d47be765",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3315\n",
    "batch_size = 16\n",
    "sample_length = 64\n",
    "n_train_samples = 2000\n",
    "embed_dim = 256\n",
    "kernel_size = 11\n",
    "causal = False\n",
    "lr = 3e-4\n",
    "weight_decay = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad94b127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4adf563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loaders(\n",
    "    model_name: str = \"meta-llama/Llama-3.2-1B\",\n",
    "    layer: str = \"model.layers.11\",\n",
    "    context: str = \"short\",\n",
    "    window: int = 16,\n",
    "    summary: bool = False,\n",
    "):\n",
    "    model_str = model_name.replace('/', '__')\n",
    "    if context == \"short\":\n",
    "        feat_path = f\"{model_str}/context-{context}_window-{window}.h5\"\n",
    "    else:\n",
    "        feat_path = f\"{model_str}/context-{context}_summary-{int(summary)}.h5\"\n",
    "    feat_path = feat_dir / \"friends\" / feat_path\n",
    "\n",
    "    feat_data = Algonauts2025FriendsFeatures(\n",
    "        path=feat_path,\n",
    "        layer=layer,\n",
    "        episode_list=fmri_data.episode_list,\n",
    "    )\n",
    "\n",
    "    train_dataset = Algonauts2025FriendsDataset(\n",
    "        feat_data,\n",
    "        fmri_data,\n",
    "        seasons=range(1, 6),\n",
    "        sample_length=sample_length,\n",
    "        num_samples=n_train_samples,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    val_dataset = Algonauts2025FriendsDataset(\n",
    "        feat_data,\n",
    "        fmri_data,\n",
    "        seasons=[6],\n",
    "        sample_length=None,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e103e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = make_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc6e48c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat shape: (16, 64, 2048)\n",
      "Sample shape: (16, 4, 64, 1000)\n"
     ]
    }
   ],
   "source": [
    "_, feat, sample = next(iter(train_loader))\n",
    "print(\"feat shape:\", tuple(feat.shape))\n",
    "print(\"Sample shape:\", tuple(sample.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7e59ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model_name: str = \"meta-llama/Llama-3.2-1B\",\n",
    "    layer: str = \"model.layers.11\",\n",
    "    context: str = \"short\",\n",
    "    window: int = 16,\n",
    "    summary: bool = False,\n",
    "    overwrite: bool = False,\n",
    "):\n",
    "    random_seed(seed)\n",
    "\n",
    "    model_str = model_name.replace('/', '__')\n",
    "    if context == \"short\":\n",
    "        out_path = f\"{model_str}/context-{context}_window-{window}/{layer}.pt\"\n",
    "    else:\n",
    "        out_path = f\"{model_str}/context-{context}_summary-{int(summary)}/{layer}.pt\"\n",
    "\n",
    "    out_path = out_dir / \"results\" / out_path\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if out_path.exists() and not overwrite:\n",
    "        ckpt = torch.load(out_path, map_location=device, weights_only=False)\n",
    "        return ckpt[\"acc\"]\n",
    "    \n",
    "    train_loader, val_loader = make_data_loaders(\n",
    "        model_name=model_name,\n",
    "        layer=layer,\n",
    "        context=context,\n",
    "        window=window,\n",
    "        summary=summary,\n",
    "    )\n",
    "\n",
    "    model = MultiSubjectConvLinearEncoder(\n",
    "        embed_dim=embed_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        causal=causal,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Model:\", model)\n",
    "    print(f\"Num params: {param_count/1e6:.2f}M\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    epoch_batches = n_train_samples // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch(\n",
    "            epoch=epoch,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch_batches=epoch_batches,\n",
    "            device=device,\n",
    "        )\n",
    "        acc, metrics = validate(\n",
    "            epoch=epoch,\n",
    "            model=model,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "        )\n",
    "    \n",
    "    acc = float(acc)\n",
    "\n",
    "    with out_path.open(\"wb\") as f:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"metrics\": metrics,\n",
    "                \"acc\": acc,\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66c2b0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.014,0.059 269/s  Mem: 0.22,0.25 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.018,0.034 471/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.381 (0.375)  Time: 0.016,0.029 557/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.359 (0.371)  Time: 0.014,0.025 640/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.373 (0.370)  Time: 0.013,0.023 691/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.352 (0.368)  Time: 0.013,0.022 724/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.359 (0.367)  Time: 0.012,0.022 744/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.364 (0.366)  Time: 0.012,0.021 768/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.343 (0.366)  Time: 0.012,0.020 781/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.356 (0.365)  Time: 0.012,0.020 791/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.357 (0.365)  Time: 0.012,0.020 802/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.351 (0.364)  Time: 0.011,0.020 813/s  Mem: 0.25,0.27 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.352 (0.364)  Time: 0.011,0.020 818/s  Mem: 0.25,0.27 GB\n",
      "Val:   0  Loss: 0.359  Acc: 0.144,0.157,0.164,0.141 (0.151)  Time: 0.006,0.007 136/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.361 (0.357)  Time: 0.014,0.029 561/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.361 (0.358)  Time: 0.013,0.024 655/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.351 (0.357)  Time: 0.012,0.022 736/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.360 (0.358)  Time: 0.011,0.021 775/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.360 (0.359)  Time: 0.011,0.020 801/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.341 (0.358)  Time: 0.011,0.020 820/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.358 (0.358)  Time: 0.011,0.019 835/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.359 (0.358)  Time: 0.011,0.019 833/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.349 (0.357)  Time: 0.011,0.019 837/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.360 (0.358)  Time: 0.011,0.019 843/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.358 (0.358)  Time: 0.011,0.019 846/s  Mem: 0.25,0.27 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.356 (0.357)  Time: 0.011,0.019 846/s  Mem: 0.25,0.27 GB\n",
      "Val:   1  Loss: 0.357  Acc: 0.156,0.172,0.179,0.153 (0.165)  Time: 0.005,0.007 143/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.354 (0.354)  Time: 0.012,0.037 437/s  Mem: 0.23,0.25 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.368 (0.357)  Time: 0.014,0.028 578/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.363 (0.358)  Time: 0.012,0.023 693/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.344 (0.357)  Time: 0.011,0.021 755/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.359 (0.357)  Time: 0.011,0.021 764/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.356 (0.357)  Time: 0.011,0.021 778/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.362 (0.357)  Time: 0.011,0.020 788/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.362 (0.357)  Time: 0.011,0.020 786/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.345 (0.357)  Time: 0.011,0.020 793/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.362 (0.357)  Time: 0.011,0.020 803/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.356 (0.357)  Time: 0.011,0.020 814/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.349 (0.357)  Time: 0.011,0.020 816/s  Mem: 0.25,0.27 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.363 (0.357)  Time: 0.011,0.020 817/s  Mem: 0.25,0.27 GB\n",
      "Val:   2  Loss: 0.356  Acc: 0.161,0.178,0.184,0.158 (0.170)  Time: 0.005,0.007 139/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.363 (0.354)  Time: 0.015,0.032 506/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.355 (0.353)  Time: 0.013,0.025 629/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.348 (0.355)  Time: 0.012,0.023 695/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.354 (0.354)  Time: 0.011,0.022 733/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.371 (0.355)  Time: 0.011,0.021 757/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.357 (0.355)  Time: 0.011,0.020 791/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.358 (0.355)  Time: 0.010,0.019 830/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.353 (0.355)  Time: 0.010,0.019 865/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.358 (0.355)  Time: 0.010,0.018 889/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.361 (0.355)  Time: 0.010,0.018 906/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.360 (0.355)  Time: 0.009,0.017 919/s  Mem: 0.25,0.27 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.358 (0.355)  Time: 0.009,0.017 935/s  Mem: 0.25,0.27 GB\n",
      "Val:   3  Loss: 0.356  Acc: 0.165,0.182,0.189,0.162 (0.174)  Time: 0.005,0.006 158/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.362 (0.362)  Time: 0.013,0.034 476/s  Mem: 0.23,0.25 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.355 (0.355)  Time: 0.012,0.023 693/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.354 (0.355)  Time: 0.011,0.021 777/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.348 (0.354)  Time: 0.011,0.020 795/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.363 (0.355)  Time: 0.011,0.020 808/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.350 (0.355)  Time: 0.010,0.019 831/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.353 (0.355)  Time: 0.010,0.019 836/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.341 (0.354)  Time: 0.010,0.019 841/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.348 (0.354)  Time: 0.010,0.019 850/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.359 (0.354)  Time: 0.010,0.019 849/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.360 (0.354)  Time: 0.010,0.019 853/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.356 (0.354)  Time: 0.010,0.019 856/s  Mem: 0.25,0.27 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.354 (0.355)  Time: 0.010,0.019 862/s  Mem: 0.25,0.27 GB\n",
      "Val:   4  Loss: 0.355  Acc: 0.169,0.185,0.191,0.164 (0.177)  Time: 0.006,0.007 137/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.354 (0.350)  Time: 0.016,0.030 534/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.356 (0.353)  Time: 0.014,0.023 687/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.360 (0.354)  Time: 0.012,0.021 769/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.353 (0.354)  Time: 0.012,0.020 805/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.354 (0.354)  Time: 0.011,0.019 835/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.352 (0.354)  Time: 0.011,0.019 852/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.360 (0.354)  Time: 0.011,0.019 860/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.348 (0.354)  Time: 0.011,0.018 870/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.362 (0.354)  Time: 0.011,0.018 883/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.350 (0.353)  Time: 0.011,0.018 890/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.351 (0.353)  Time: 0.011,0.018 894/s  Mem: 0.25,0.27 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.358 (0.353)  Time: 0.011,0.018 900/s  Mem: 0.25,0.27 GB\n",
      "Val:   5  Loss: 0.355  Acc: 0.169,0.187,0.192,0.165 (0.178)  Time: 0.006,0.007 137/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.374 (0.374)  Time: 0.011,0.037 434/s  Mem: 0.23,0.25 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.356 (0.353)  Time: 0.012,0.024 677/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.348 (0.354)  Time: 0.011,0.021 753/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.344 (0.354)  Time: 0.011,0.020 791/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.334 (0.354)  Time: 0.011,0.020 814/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.364 (0.354)  Time: 0.011,0.020 819/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.357 (0.354)  Time: 0.011,0.019 831/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.344 (0.354)  Time: 0.011,0.019 839/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.344 (0.354)  Time: 0.010,0.019 848/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.347 (0.354)  Time: 0.010,0.019 853/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.344 (0.353)  Time: 0.010,0.019 856/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.368 (0.354)  Time: 0.010,0.019 862/s  Mem: 0.25,0.27 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.365 (0.354)  Time: 0.010,0.018 871/s  Mem: 0.25,0.27 GB\n",
      "Val:   6  Loss: 0.355  Acc: 0.169,0.186,0.192,0.165 (0.178)  Time: 0.005,0.006 155/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.347 (0.353)  Time: 0.015,0.028 572/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.361 (0.353)  Time: 0.013,0.023 685/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.363 (0.354)  Time: 0.012,0.021 751/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.359 (0.354)  Time: 0.011,0.020 813/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.340 (0.354)  Time: 0.011,0.019 847/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.356 (0.354)  Time: 0.010,0.019 863/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.354 (0.354)  Time: 0.010,0.018 887/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.358 (0.354)  Time: 0.010,0.018 894/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.339 (0.354)  Time: 0.010,0.018 893/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.354 (0.354)  Time: 0.010,0.018 891/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.357 (0.354)  Time: 0.010,0.018 896/s  Mem: 0.25,0.27 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.364 (0.354)  Time: 0.010,0.018 900/s  Mem: 0.25,0.27 GB\n",
      "Val:   7  Loss: 0.355  Acc: 0.172,0.188,0.193,0.166 (0.180)  Time: 0.005,0.007 141/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.350 (0.350)  Time: 0.011,0.032 494/s  Mem: 0.23,0.25 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.352 (0.352)  Time: 0.013,0.024 678/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.346 (0.353)  Time: 0.012,0.022 738/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.339 (0.354)  Time: 0.011,0.020 781/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.354 (0.354)  Time: 0.011,0.020 799/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.350 (0.353)  Time: 0.011,0.020 820/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.352 (0.353)  Time: 0.011,0.019 834/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.346 (0.353)  Time: 0.011,0.019 847/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.346 (0.353)  Time: 0.011,0.019 853/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.357 (0.353)  Time: 0.011,0.019 856/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.362 (0.353)  Time: 0.011,0.018 866/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.341 (0.353)  Time: 0.011,0.018 869/s  Mem: 0.25,0.27 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.355 (0.352)  Time: 0.010,0.018 877/s  Mem: 0.25,0.27 GB\n",
      "Val:   8  Loss: 0.355  Acc: 0.169,0.186,0.190,0.164 (0.177)  Time: 0.005,0.007 140/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.352 (0.353)  Time: 0.011,0.022 737/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.349 (0.353)  Time: 0.011,0.021 763/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.353 (0.352)  Time: 0.011,0.020 793/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.347 (0.351)  Time: 0.011,0.019 831/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.344 (0.350)  Time: 0.011,0.019 863/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.353 (0.351)  Time: 0.010,0.018 883/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.346 (0.352)  Time: 0.010,0.018 893/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.349 (0.352)  Time: 0.010,0.018 894/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.351 (0.352)  Time: 0.010,0.018 890/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.344 (0.352)  Time: 0.010,0.018 887/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.356 (0.352)  Time: 0.010,0.018 890/s  Mem: 0.25,0.27 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.338 (0.352)  Time: 0.010,0.018 892/s  Mem: 0.25,0.27 GB\n",
      "Val:   9  Loss: 0.355  Acc: 0.169,0.187,0.190,0.164 (0.178)  Time: 0.005,0.007 145/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17757554352283478"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "468eda84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.037,0.069 232/s  Mem: 0.27,0.31 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.017,0.033 486/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.380 (0.375)  Time: 0.015,0.028 578/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.359 (0.371)  Time: 0.013,0.024 673/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.373 (0.370)  Time: 0.012,0.022 743/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.352 (0.368)  Time: 0.011,0.020 796/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.359 (0.367)  Time: 0.011,0.019 821/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.364 (0.366)  Time: 0.010,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.343 (0.365)  Time: 0.010,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.356 (0.365)  Time: 0.010,0.017 941/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.357 (0.364)  Time: 0.009,0.016 971/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.350 (0.364)  Time: 0.009,0.016 998/s  Mem: 0.30,0.33 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.352 (0.364)  Time: 0.009,0.016 1027/s  Mem: 0.30,0.33 GB\n",
      "Val:   0  Loss: 0.358  Acc: 0.144,0.157,0.164,0.141 (0.151)  Time: 0.005,0.006 164/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.361 (0.357)  Time: 0.013,0.025 643/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.361 (0.358)  Time: 0.011,0.021 772/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.351 (0.357)  Time: 0.010,0.019 849/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.360 (0.358)  Time: 0.010,0.018 882/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.359 (0.359)  Time: 0.010,0.018 907/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.341 (0.358)  Time: 0.010,0.017 921/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.358 (0.358)  Time: 0.010,0.017 940/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.359 (0.358)  Time: 0.010,0.017 943/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.348 (0.357)  Time: 0.010,0.017 955/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.359 (0.357)  Time: 0.010,0.017 956/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.357 (0.357)  Time: 0.010,0.017 961/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.356 (0.357)  Time: 0.010,0.017 960/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.357  Acc: 0.156,0.171,0.178,0.154 (0.165)  Time: 0.005,0.007 150/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.355 (0.355)  Time: 0.015,0.040 402/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.367 (0.357)  Time: 0.012,0.024 681/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.362 (0.358)  Time: 0.011,0.021 762/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.344 (0.357)  Time: 0.011,0.020 805/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.359 (0.357)  Time: 0.011,0.019 842/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.355 (0.357)  Time: 0.011,0.018 867/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.361 (0.357)  Time: 0.010,0.018 886/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.361 (0.357)  Time: 0.010,0.018 893/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.344 (0.357)  Time: 0.011,0.018 896/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.361 (0.357)  Time: 0.011,0.018 895/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.355 (0.357)  Time: 0.011,0.018 894/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.349 (0.356)  Time: 0.011,0.018 898/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.363 (0.357)  Time: 0.011,0.018 903/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.356  Acc: 0.160,0.177,0.183,0.158 (0.170)  Time: 0.005,0.007 142/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.364 (0.353)  Time: 0.013,0.025 636/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.355 (0.352)  Time: 0.011,0.021 774/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.348 (0.354)  Time: 0.011,0.019 822/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.354 (0.354)  Time: 0.011,0.019 840/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.370 (0.355)  Time: 0.011,0.019 845/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.355 (0.355)  Time: 0.011,0.019 842/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.358 (0.354)  Time: 0.011,0.019 851/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.352 (0.354)  Time: 0.011,0.019 864/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.358 (0.355)  Time: 0.011,0.018 869/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.361 (0.355)  Time: 0.011,0.018 865/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.359 (0.355)  Time: 0.011,0.018 869/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.357 (0.355)  Time: 0.011,0.018 876/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.356  Acc: 0.163,0.180,0.187,0.162 (0.173)  Time: 0.005,0.007 139/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.361 (0.361)  Time: 0.017,0.045 355/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.354 (0.355)  Time: 0.014,0.026 615/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.353 (0.354)  Time: 0.013,0.023 709/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.348 (0.354)  Time: 0.013,0.022 717/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.363 (0.355)  Time: 0.012,0.021 751/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.349 (0.354)  Time: 0.012,0.021 771/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.352 (0.355)  Time: 0.012,0.020 791/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.341 (0.354)  Time: 0.012,0.020 808/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.347 (0.354)  Time: 0.012,0.020 819/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.358 (0.353)  Time: 0.011,0.019 830/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.360 (0.353)  Time: 0.011,0.019 837/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.355 (0.354)  Time: 0.011,0.019 848/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.354 (0.354)  Time: 0.011,0.019 858/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.355  Acc: 0.167,0.183,0.189,0.164 (0.176)  Time: 0.006,0.007 139/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.353 (0.350)  Time: 0.014,0.027 594/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.354 (0.352)  Time: 0.013,0.024 659/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.360 (0.354)  Time: 0.012,0.022 736/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.353 (0.353)  Time: 0.012,0.020 783/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.354 (0.354)  Time: 0.011,0.020 812/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.351 (0.354)  Time: 0.011,0.019 826/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.358 (0.354)  Time: 0.011,0.019 839/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.349 (0.353)  Time: 0.011,0.019 838/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.361 (0.353)  Time: 0.011,0.019 855/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.349 (0.353)  Time: 0.011,0.018 878/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.350 (0.353)  Time: 0.010,0.018 905/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.358 (0.353)  Time: 0.010,0.017 934/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.355  Acc: 0.167,0.184,0.190,0.164 (0.176)  Time: 0.005,0.006 161/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.374 (0.374)  Time: 0.015,0.037 438/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.354 (0.352)  Time: 0.011,0.020 801/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.348 (0.354)  Time: 0.010,0.019 836/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.343 (0.354)  Time: 0.010,0.018 879/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.334 (0.353)  Time: 0.010,0.018 894/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.363 (0.353)  Time: 0.010,0.018 899/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.357 (0.353)  Time: 0.010,0.017 927/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.343 (0.353)  Time: 0.009,0.017 953/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.343 (0.353)  Time: 0.009,0.016 973/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.346 (0.353)  Time: 0.009,0.016 993/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.344 (0.353)  Time: 0.009,0.016 1009/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.368 (0.353)  Time: 0.009,0.016 997/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.365 (0.353)  Time: 0.009,0.016 1006/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.356  Acc: 0.167,0.184,0.190,0.164 (0.177)  Time: 0.005,0.006 156/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.346 (0.352)  Time: 0.013,0.026 616/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.361 (0.352)  Time: 0.011,0.021 758/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.363 (0.353)  Time: 0.011,0.020 819/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.359 (0.353)  Time: 0.010,0.019 855/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.340 (0.353)  Time: 0.010,0.018 887/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.356 (0.353)  Time: 0.010,0.018 908/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.353 (0.353)  Time: 0.010,0.017 920/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.357 (0.354)  Time: 0.010,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.338 (0.353)  Time: 0.010,0.017 939/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.353 (0.353)  Time: 0.010,0.017 944/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.357 (0.353)  Time: 0.010,0.017 934/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.362 (0.353)  Time: 0.010,0.017 940/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.355  Acc: 0.169,0.184,0.191,0.165 (0.177)  Time: 0.005,0.007 143/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.349 (0.349)  Time: 0.015,0.035 456/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.351 (0.351)  Time: 0.010,0.018 904/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.346 (0.352)  Time: 0.009,0.017 958/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.338 (0.353)  Time: 0.009,0.017 959/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.353 (0.353)  Time: 0.009,0.016 976/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.350 (0.353)  Time: 0.009,0.016 987/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.350 (0.352)  Time: 0.009,0.016 997/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.345 (0.352)  Time: 0.009,0.016 1003/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.344 (0.352)  Time: 0.009,0.016 987/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.357 (0.352)  Time: 0.010,0.016 985/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.361 (0.352)  Time: 0.010,0.016 991/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.341 (0.352)  Time: 0.009,0.016 999/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.354 (0.352)  Time: 0.009,0.016 1010/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.355  Acc: 0.166,0.184,0.188,0.163 (0.175)  Time: 0.005,0.006 157/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.351 (0.353)  Time: 0.015,0.027 602/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.349 (0.352)  Time: 0.013,0.022 732/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.352 (0.352)  Time: 0.012,0.020 809/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.347 (0.350)  Time: 0.011,0.019 860/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.344 (0.350)  Time: 0.011,0.019 857/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.352 (0.350)  Time: 0.011,0.019 856/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.345 (0.352)  Time: 0.011,0.018 876/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.349 (0.352)  Time: 0.011,0.018 891/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.349 (0.352)  Time: 0.011,0.018 903/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.343 (0.351)  Time: 0.011,0.018 909/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.353 (0.351)  Time: 0.011,0.017 916/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.338 (0.351)  Time: 0.011,0.017 919/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.356  Acc: 0.166,0.183,0.188,0.162 (0.175)  Time: 0.006,0.007 138/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"short\", \"window\": 16, \"summary\": false, \"layer\": \"model.layers.7\", \"acc\": 0.174943745136261}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.017,0.057 281/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.015,0.030 530/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.381 (0.375)  Time: 0.013,0.025 645/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.359 (0.371)  Time: 0.012,0.023 710/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.373 (0.370)  Time: 0.012,0.021 749/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.352 (0.368)  Time: 0.011,0.021 779/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.359 (0.367)  Time: 0.011,0.020 809/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.364 (0.366)  Time: 0.011,0.019 830/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.343 (0.366)  Time: 0.011,0.019 847/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.356 (0.365)  Time: 0.011,0.019 860/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.357 (0.365)  Time: 0.011,0.018 870/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.351 (0.364)  Time: 0.011,0.018 881/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.352 (0.364)  Time: 0.010,0.018 902/s  Mem: 0.31,0.34 GB\n",
      "Val:   0  Loss: 0.359  Acc: 0.144,0.157,0.164,0.141 (0.151)  Time: 0.005,0.007 152/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.361 (0.357)  Time: 0.014,0.028 576/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.361 (0.358)  Time: 0.013,0.025 647/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.351 (0.357)  Time: 0.012,0.022 711/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.360 (0.358)  Time: 0.012,0.021 749/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.360 (0.359)  Time: 0.011,0.020 788/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.341 (0.358)  Time: 0.011,0.020 812/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.358 (0.358)  Time: 0.011,0.019 833/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.359 (0.358)  Time: 0.011,0.019 847/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.349 (0.357)  Time: 0.011,0.019 846/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.360 (0.358)  Time: 0.011,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.358 (0.358)  Time: 0.011,0.018 883/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.356 (0.357)  Time: 0.010,0.018 903/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.357  Acc: 0.156,0.172,0.179,0.153 (0.165)  Time: 0.004,0.006 176/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.354 (0.354)  Time: 0.010,0.029 560/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.368 (0.357)  Time: 0.008,0.017 933/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.363 (0.358)  Time: 0.008,0.016 1009/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.344 (0.357)  Time: 0.008,0.015 1070/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.359 (0.357)  Time: 0.008,0.015 1085/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.356 (0.357)  Time: 0.008,0.015 1092/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.362 (0.357)  Time: 0.008,0.014 1109/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.362 (0.357)  Time: 0.008,0.014 1132/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.345 (0.357)  Time: 0.008,0.014 1164/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.362 (0.357)  Time: 0.008,0.013 1195/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.356 (0.357)  Time: 0.008,0.013 1220/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.349 (0.357)  Time: 0.007,0.013 1253/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.363 (0.357)  Time: 0.007,0.012 1283/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.356  Acc: 0.161,0.178,0.184,0.158 (0.170)  Time: 0.004,0.005 199/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.363 (0.354)  Time: 0.011,0.023 686/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.355 (0.353)  Time: 0.011,0.020 781/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.348 (0.355)  Time: 0.011,0.019 826/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.354 (0.354)  Time: 0.011,0.019 847/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.371 (0.355)  Time: 0.011,0.019 862/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.357 (0.355)  Time: 0.011,0.018 865/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.358 (0.355)  Time: 0.011,0.018 872/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.353 (0.355)  Time: 0.011,0.018 877/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.358 (0.355)  Time: 0.011,0.018 876/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.361 (0.355)  Time: 0.011,0.018 877/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.360 (0.355)  Time: 0.011,0.018 883/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.358 (0.355)  Time: 0.011,0.018 883/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.356  Acc: 0.165,0.182,0.189,0.162 (0.174)  Time: 0.006,0.007 135/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.362 (0.362)  Time: 0.017,0.040 398/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.355 (0.355)  Time: 0.014,0.026 606/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.354 (0.355)  Time: 0.013,0.023 696/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.348 (0.354)  Time: 0.012,0.021 773/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.363 (0.355)  Time: 0.011,0.019 830/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.350 (0.355)  Time: 0.011,0.018 869/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.353 (0.355)  Time: 0.010,0.018 907/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.341 (0.354)  Time: 0.010,0.017 930/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.348 (0.354)  Time: 0.010,0.017 955/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.359 (0.354)  Time: 0.010,0.016 983/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.360 (0.354)  Time: 0.009,0.016 1008/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.356 (0.354)  Time: 0.009,0.016 1031/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.354 (0.355)  Time: 0.009,0.015 1051/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.355  Acc: 0.169,0.185,0.191,0.164 (0.177)  Time: 0.005,0.006 158/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.354 (0.350)  Time: 0.012,0.026 626/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.356 (0.353)  Time: 0.011,0.021 757/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.360 (0.354)  Time: 0.011,0.020 781/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.353 (0.354)  Time: 0.011,0.020 810/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.354 (0.354)  Time: 0.011,0.020 812/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.352 (0.354)  Time: 0.011,0.020 815/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.360 (0.354)  Time: 0.011,0.019 824/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.348 (0.354)  Time: 0.011,0.019 837/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.362 (0.354)  Time: 0.011,0.019 842/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.350 (0.353)  Time: 0.011,0.019 845/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.351 (0.353)  Time: 0.011,0.019 837/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.358 (0.353)  Time: 0.011,0.019 840/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.355  Acc: 0.169,0.187,0.192,0.165 (0.178)  Time: 0.006,0.007 138/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.374 (0.374)  Time: 0.013,0.041 390/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.356 (0.353)  Time: 0.013,0.026 608/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.348 (0.354)  Time: 0.012,0.023 693/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.344 (0.354)  Time: 0.012,0.022 735/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.334 (0.354)  Time: 0.012,0.021 763/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.364 (0.354)  Time: 0.011,0.020 784/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.357 (0.354)  Time: 0.011,0.020 809/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.344 (0.354)  Time: 0.011,0.019 833/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.344 (0.354)  Time: 0.011,0.019 850/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.347 (0.354)  Time: 0.011,0.018 869/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.344 (0.353)  Time: 0.010,0.018 884/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.368 (0.354)  Time: 0.010,0.018 898/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.365 (0.354)  Time: 0.010,0.018 913/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.355  Acc: 0.169,0.186,0.192,0.165 (0.178)  Time: 0.005,0.007 151/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.347 (0.353)  Time: 0.011,0.021 779/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.361 (0.353)  Time: 0.011,0.020 787/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.363 (0.354)  Time: 0.011,0.019 823/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.359 (0.354)  Time: 0.011,0.019 839/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.340 (0.354)  Time: 0.011,0.019 846/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.356 (0.354)  Time: 0.011,0.019 856/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.354 (0.354)  Time: 0.011,0.019 864/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.358 (0.354)  Time: 0.011,0.019 862/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.339 (0.354)  Time: 0.011,0.018 867/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.354 (0.354)  Time: 0.011,0.018 870/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.357 (0.354)  Time: 0.011,0.018 871/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.364 (0.354)  Time: 0.011,0.018 867/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.355  Acc: 0.172,0.188,0.193,0.166 (0.180)  Time: 0.005,0.007 144/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.350 (0.350)  Time: 0.013,0.032 503/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.352 (0.352)  Time: 0.012,0.023 708/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.346 (0.353)  Time: 0.011,0.020 800/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.339 (0.354)  Time: 0.011,0.019 835/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.354 (0.354)  Time: 0.011,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.350 (0.353)  Time: 0.011,0.018 876/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.352 (0.353)  Time: 0.010,0.018 891/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.346 (0.353)  Time: 0.010,0.018 898/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.346 (0.353)  Time: 0.010,0.018 907/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.357 (0.353)  Time: 0.010,0.018 913/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.362 (0.353)  Time: 0.010,0.018 914/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.341 (0.353)  Time: 0.010,0.017 917/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.355 (0.352)  Time: 0.010,0.017 923/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.355  Acc: 0.169,0.186,0.190,0.164 (0.177)  Time: 0.005,0.007 144/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.352 (0.353)  Time: 0.008,0.016 1014/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.349 (0.353)  Time: 0.008,0.015 1063/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.353 (0.352)  Time: 0.008,0.015 1082/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.347 (0.351)  Time: 0.008,0.015 1095/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.344 (0.350)  Time: 0.008,0.014 1113/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.353 (0.351)  Time: 0.008,0.014 1104/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.346 (0.352)  Time: 0.008,0.015 1101/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.349 (0.352)  Time: 0.008,0.015 1101/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.351 (0.352)  Time: 0.008,0.014 1113/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.344 (0.352)  Time: 0.008,0.014 1128/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.356 (0.352)  Time: 0.008,0.014 1142/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.338 (0.352)  Time: 0.008,0.014 1156/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.355  Acc: 0.169,0.187,0.190,0.164 (0.178)  Time: 0.004,0.006 165/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"short\", \"window\": 16, \"summary\": false, \"layer\": \"model.layers.11\", \"acc\": 0.17757554352283478}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.015,0.039 409/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.367 (0.375)  Time: 0.013,0.025 636/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.381 (0.375)  Time: 0.012,0.022 731/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.360 (0.372)  Time: 0.011,0.020 793/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.374 (0.371)  Time: 0.011,0.019 836/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.353 (0.369)  Time: 0.011,0.019 832/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.359 (0.368)  Time: 0.011,0.019 838/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.364 (0.367)  Time: 0.011,0.019 844/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.343 (0.366)  Time: 0.011,0.019 852/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.356 (0.366)  Time: 0.011,0.019 859/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.357 (0.365)  Time: 0.011,0.019 860/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.351 (0.364)  Time: 0.011,0.019 862/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.352 (0.364)  Time: 0.011,0.018 869/s  Mem: 0.31,0.34 GB\n",
      "Val:   0  Loss: 0.359  Acc: 0.144,0.158,0.164,0.141 (0.152)  Time: 0.006,0.007 139/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.361 (0.357)  Time: 0.015,0.028 573/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.361 (0.358)  Time: 0.011,0.020 804/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.351 (0.357)  Time: 0.010,0.018 901/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.361 (0.358)  Time: 0.009,0.016 984/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.360 (0.359)  Time: 0.009,0.016 1031/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.341 (0.359)  Time: 0.008,0.015 1068/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.358 (0.358)  Time: 0.008,0.015 1097/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.360 (0.358)  Time: 0.008,0.014 1119/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.350 (0.358)  Time: 0.008,0.014 1146/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.361 (0.358)  Time: 0.008,0.014 1164/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.359 (0.358)  Time: 0.008,0.014 1168/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.357 (0.358)  Time: 0.008,0.014 1184/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.358  Acc: 0.155,0.171,0.177,0.151 (0.163)  Time: 0.004,0.006 165/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.355 (0.355)  Time: 0.009,0.024 653/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.368 (0.358)  Time: 0.010,0.020 800/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.364 (0.358)  Time: 0.009,0.018 888/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.345 (0.358)  Time: 0.009,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.361 (0.358)  Time: 0.009,0.017 968/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.356 (0.358)  Time: 0.009,0.016 975/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.362 (0.358)  Time: 0.009,0.016 992/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.362 (0.358)  Time: 0.009,0.016 997/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.346 (0.358)  Time: 0.009,0.016 1006/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.362 (0.358)  Time: 0.009,0.016 1013/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.357 (0.358)  Time: 0.009,0.016 1030/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.350 (0.357)  Time: 0.009,0.016 1021/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.363 (0.358)  Time: 0.009,0.016 1026/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.356  Acc: 0.159,0.175,0.181,0.154 (0.168)  Time: 0.005,0.007 153/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.363 (0.354)  Time: 0.009,0.018 909/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.356 (0.354)  Time: 0.008,0.016 1011/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.349 (0.355)  Time: 0.008,0.015 1053/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.355 (0.355)  Time: 0.008,0.015 1074/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.370 (0.356)  Time: 0.008,0.015 1092/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.357 (0.356)  Time: 0.008,0.015 1095/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.359 (0.355)  Time: 0.008,0.015 1092/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.354 (0.355)  Time: 0.008,0.015 1098/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.358 (0.356)  Time: 0.008,0.014 1109/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.362 (0.356)  Time: 0.008,0.014 1127/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.360 (0.356)  Time: 0.008,0.014 1150/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.359 (0.356)  Time: 0.008,0.014 1170/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.356  Acc: 0.164,0.179,0.186,0.159 (0.172)  Time: 0.005,0.007 144/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.363 (0.363)  Time: 0.009,0.025 634/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.355 (0.356)  Time: 0.010,0.019 848/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.354 (0.355)  Time: 0.009,0.017 922/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.348 (0.355)  Time: 0.009,0.017 959/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.363 (0.356)  Time: 0.009,0.016 973/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.351 (0.355)  Time: 0.009,0.016 995/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.353 (0.356)  Time: 0.009,0.016 1017/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.342 (0.355)  Time: 0.009,0.016 1031/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.349 (0.355)  Time: 0.009,0.015 1042/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.359 (0.354)  Time: 0.009,0.015 1052/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.361 (0.354)  Time: 0.009,0.015 1071/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.357 (0.355)  Time: 0.009,0.015 1077/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.355 (0.355)  Time: 0.009,0.015 1061/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.355  Acc: 0.168,0.182,0.188,0.162 (0.175)  Time: 0.005,0.006 156/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.353 (0.351)  Time: 0.011,0.023 692/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.356 (0.353)  Time: 0.010,0.020 818/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.360 (0.355)  Time: 0.009,0.018 875/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.353 (0.354)  Time: 0.009,0.017 930/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.355 (0.355)  Time: 0.009,0.017 969/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.352 (0.355)  Time: 0.009,0.016 998/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.359 (0.355)  Time: 0.009,0.016 1023/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.349 (0.355)  Time: 0.009,0.015 1033/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.363 (0.354)  Time: 0.008,0.015 1055/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.351 (0.354)  Time: 0.008,0.015 1084/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.352 (0.354)  Time: 0.008,0.014 1110/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.358 (0.354)  Time: 0.008,0.014 1106/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.355  Acc: 0.168,0.184,0.190,0.163 (0.176)  Time: 0.005,0.007 152/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.375 (0.375)  Time: 0.009,0.025 653/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.357 (0.353)  Time: 0.010,0.018 894/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.349 (0.355)  Time: 0.009,0.017 966/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.344 (0.355)  Time: 0.009,0.016 994/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.336 (0.355)  Time: 0.009,0.016 1017/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.365 (0.355)  Time: 0.009,0.015 1040/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.357 (0.355)  Time: 0.009,0.015 1053/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.345 (0.354)  Time: 0.009,0.015 1056/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.344 (0.354)  Time: 0.009,0.015 1064/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.348 (0.354)  Time: 0.008,0.015 1080/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.344 (0.354)  Time: 0.008,0.015 1101/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.370 (0.354)  Time: 0.008,0.014 1105/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.366 (0.355)  Time: 0.008,0.014 1109/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.356  Acc: 0.168,0.184,0.189,0.163 (0.176)  Time: 0.005,0.006 162/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.347 (0.353)  Time: 0.011,0.020 813/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.363 (0.354)  Time: 0.010,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.364 (0.355)  Time: 0.009,0.017 956/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.360 (0.354)  Time: 0.009,0.016 980/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.341 (0.354)  Time: 0.009,0.016 1006/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.358 (0.354)  Time: 0.009,0.016 1009/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.355 (0.354)  Time: 0.009,0.016 1022/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.358 (0.355)  Time: 0.009,0.016 1032/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.340 (0.355)  Time: 0.009,0.015 1038/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.355 (0.354)  Time: 0.009,0.015 1054/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.359 (0.354)  Time: 0.009,0.015 1072/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.366 (0.354)  Time: 0.009,0.015 1089/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.355  Acc: 0.169,0.184,0.189,0.164 (0.177)  Time: 0.005,0.007 139/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.351 (0.351)  Time: 0.012,0.030 533/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.353 (0.352)  Time: 0.011,0.020 787/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.347 (0.354)  Time: 0.010,0.018 883/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.339 (0.355)  Time: 0.009,0.017 943/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.355 (0.355)  Time: 0.009,0.016 971/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.350 (0.354)  Time: 0.009,0.016 984/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.352 (0.354)  Time: 0.009,0.016 1000/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.346 (0.353)  Time: 0.009,0.016 1023/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.346 (0.353)  Time: 0.009,0.016 1025/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.357 (0.354)  Time: 0.009,0.015 1042/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.362 (0.353)  Time: 0.009,0.015 1067/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.343 (0.353)  Time: 0.008,0.015 1088/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.355 (0.353)  Time: 0.008,0.014 1106/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.356  Acc: 0.167,0.184,0.188,0.162 (0.175)  Time: 0.005,0.007 147/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.352 (0.353)  Time: 0.009,0.019 841/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.350 (0.353)  Time: 0.008,0.016 987/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.355 (0.353)  Time: 0.008,0.015 1033/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.348 (0.352)  Time: 0.008,0.015 1075/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.345 (0.351)  Time: 0.008,0.015 1069/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.353 (0.352)  Time: 0.008,0.015 1067/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.347 (0.353)  Time: 0.008,0.015 1072/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.350 (0.353)  Time: 0.008,0.015 1082/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.351 (0.353)  Time: 0.008,0.015 1085/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.345 (0.353)  Time: 0.008,0.015 1103/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.357 (0.353)  Time: 0.008,0.014 1128/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.339 (0.352)  Time: 0.008,0.014 1145/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.356  Acc: 0.166,0.183,0.186,0.160 (0.174)  Time: 0.005,0.006 160/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"short\", \"window\": 16, \"summary\": false, \"layer\": \"model.layers.15\", \"acc\": 0.17384329438209534}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.015,0.044 362/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.011,0.023 682/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.380 (0.374)  Time: 0.012,0.023 692/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.359 (0.371)  Time: 0.012,0.022 741/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.372 (0.370)  Time: 0.012,0.021 767/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.352 (0.368)  Time: 0.011,0.020 814/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.358 (0.367)  Time: 0.011,0.019 833/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.363 (0.366)  Time: 0.011,0.019 855/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.343 (0.365)  Time: 0.010,0.018 876/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.355 (0.365)  Time: 0.010,0.018 884/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.355 (0.364)  Time: 0.010,0.018 895/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.349 (0.363)  Time: 0.010,0.018 908/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.351 (0.363)  Time: 0.010,0.017 919/s  Mem: 0.31,0.34 GB\n",
      "Val:   0  Loss: 0.357  Acc: 0.151,0.165,0.172,0.149 (0.159)  Time: 0.004,0.006 172/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.360 (0.356)  Time: 0.011,0.020 782/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.360 (0.357)  Time: 0.010,0.019 863/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.350 (0.356)  Time: 0.010,0.017 928/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.359 (0.357)  Time: 0.009,0.017 948/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.359 (0.358)  Time: 0.009,0.016 970/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.339 (0.357)  Time: 0.009,0.016 978/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.356 (0.357)  Time: 0.010,0.017 948/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.358 (0.357)  Time: 0.010,0.017 955/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.347 (0.356)  Time: 0.010,0.017 962/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.358 (0.356)  Time: 0.010,0.017 967/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.357 (0.356)  Time: 0.010,0.017 965/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.356 (0.356)  Time: 0.009,0.016 974/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.356  Acc: 0.164,0.179,0.186,0.162 (0.173)  Time: 0.005,0.006 166/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.353 (0.353)  Time: 0.012,0.040 403/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.366 (0.356)  Time: 0.012,0.025 634/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.360 (0.357)  Time: 0.012,0.023 707/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.343 (0.356)  Time: 0.012,0.021 744/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.358 (0.356)  Time: 0.011,0.021 766/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.354 (0.356)  Time: 0.011,0.020 797/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.360 (0.356)  Time: 0.011,0.020 820/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.359 (0.356)  Time: 0.011,0.019 826/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.343 (0.356)  Time: 0.011,0.019 836/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.360 (0.356)  Time: 0.011,0.019 842/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.353 (0.356)  Time: 0.011,0.019 849/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.347 (0.355)  Time: 0.011,0.019 859/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.362 (0.355)  Time: 0.011,0.019 863/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.354  Acc: 0.170,0.186,0.192,0.167 (0.179)  Time: 0.005,0.007 142/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.362 (0.351)  Time: 0.010,0.020 795/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.353 (0.351)  Time: 0.010,0.018 886/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.346 (0.353)  Time: 0.009,0.017 942/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.353 (0.353)  Time: 0.009,0.016 971/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.368 (0.354)  Time: 0.009,0.016 979/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.354 (0.354)  Time: 0.009,0.016 990/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.357 (0.353)  Time: 0.009,0.016 1001/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.351 (0.353)  Time: 0.009,0.016 1011/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.356 (0.353)  Time: 0.009,0.016 1022/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.359 (0.353)  Time: 0.009,0.015 1044/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.358 (0.353)  Time: 0.009,0.015 1058/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.355 (0.353)  Time: 0.009,0.015 1071/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.354  Acc: 0.173,0.190,0.196,0.171 (0.182)  Time: 0.005,0.006 164/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.360 (0.360)  Time: 0.011,0.030 537/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.352 (0.353)  Time: 0.010,0.021 750/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.351 (0.353)  Time: 0.010,0.019 835/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.347 (0.352)  Time: 0.010,0.019 853/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.361 (0.353)  Time: 0.010,0.018 888/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.347 (0.353)  Time: 0.010,0.018 913/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.350 (0.353)  Time: 0.009,0.017 958/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.339 (0.352)  Time: 0.009,0.016 993/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.346 (0.352)  Time: 0.009,0.016 1025/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.358 (0.352)  Time: 0.009,0.015 1058/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.358 (0.352)  Time: 0.008,0.015 1097/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.354 (0.352)  Time: 0.008,0.014 1128/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.352 (0.353)  Time: 0.008,0.014 1154/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.354  Acc: 0.178,0.191,0.198,0.174 (0.185)  Time: 0.004,0.005 206/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.352 (0.348)  Time: 0.009,0.019 850/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.354 (0.351)  Time: 0.008,0.016 1030/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.358 (0.352)  Time: 0.008,0.014 1114/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.351 (0.351)  Time: 0.008,0.014 1137/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.353 (0.352)  Time: 0.008,0.014 1164/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.349 (0.352)  Time: 0.008,0.014 1185/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.357 (0.352)  Time: 0.007,0.013 1205/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.347 (0.352)  Time: 0.008,0.014 1150/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.359 (0.351)  Time: 0.008,0.014 1150/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.348 (0.351)  Time: 0.008,0.014 1157/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.349 (0.351)  Time: 0.008,0.014 1161/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.356 (0.351)  Time: 0.008,0.014 1165/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.353  Acc: 0.177,0.192,0.200,0.174 (0.186)  Time: 0.004,0.005 185/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.372 (0.372)  Time: 0.012,0.029 555/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.352 (0.350)  Time: 0.013,0.025 643/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.346 (0.352)  Time: 0.012,0.022 723/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.341 (0.352)  Time: 0.011,0.021 776/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.333 (0.352)  Time: 0.011,0.020 811/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.361 (0.352)  Time: 0.012,0.021 765/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.354 (0.352)  Time: 0.012,0.021 768/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.341 (0.351)  Time: 0.012,0.020 786/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.342 (0.351)  Time: 0.012,0.020 808/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.344 (0.351)  Time: 0.011,0.019 827/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.341 (0.351)  Time: 0.011,0.019 844/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.366 (0.352)  Time: 0.011,0.019 857/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.362 (0.352)  Time: 0.011,0.018 868/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.354  Acc: 0.178,0.193,0.199,0.174 (0.186)  Time: 0.005,0.007 145/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.344 (0.351)  Time: 0.011,0.021 747/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.360 (0.350)  Time: 0.010,0.018 883/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.362 (0.352)  Time: 0.009,0.017 953/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.357 (0.351)  Time: 0.009,0.017 950/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.338 (0.351)  Time: 0.009,0.016 975/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.354 (0.351)  Time: 0.009,0.016 982/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.352 (0.351)  Time: 0.009,0.016 984/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.356 (0.352)  Time: 0.009,0.016 990/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.337 (0.352)  Time: 0.009,0.016 995/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.351 (0.351)  Time: 0.009,0.016 1017/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.354 (0.351)  Time: 0.009,0.015 1035/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.360 (0.351)  Time: 0.009,0.015 1051/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.353  Acc: 0.180,0.194,0.200,0.175 (0.187)  Time: 0.004,0.006 165/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.348 (0.348)  Time: 0.012,0.038 421/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.350 (0.349)  Time: 0.011,0.022 714/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.344 (0.350)  Time: 0.011,0.020 793/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.337 (0.351)  Time: 0.011,0.020 793/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.353 (0.352)  Time: 0.014,0.023 699/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.348 (0.351)  Time: 0.013,0.022 723/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.349 (0.351)  Time: 0.013,0.021 747/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.344 (0.350)  Time: 0.013,0.021 753/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.344 (0.350)  Time: 0.012,0.021 769/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.355 (0.351)  Time: 0.012,0.020 781/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.360 (0.351)  Time: 0.012,0.020 794/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.340 (0.350)  Time: 0.012,0.020 804/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.353 (0.350)  Time: 0.012,0.020 812/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.354  Acc: 0.175,0.192,0.195,0.171 (0.183)  Time: 0.006,0.007 138/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.349 (0.350)  Time: 0.015,0.028 565/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.346 (0.350)  Time: 0.013,0.024 669/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.351 (0.350)  Time: 0.012,0.021 766/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.345 (0.348)  Time: 0.011,0.019 821/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.341 (0.348)  Time: 0.011,0.019 855/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.350 (0.348)  Time: 0.010,0.018 884/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.343 (0.350)  Time: 0.010,0.018 898/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.348 (0.350)  Time: 0.010,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.348 (0.350)  Time: 0.010,0.018 891/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.342 (0.350)  Time: 0.010,0.018 892/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.354 (0.349)  Time: 0.010,0.018 885/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.336 (0.349)  Time: 0.010,0.018 894/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.354  Acc: 0.178,0.194,0.197,0.172 (0.185)  Time: 0.006,0.007 135/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"short\", \"window\": 32, \"summary\": false, \"layer\": \"model.layers.7\", \"acc\": 0.1852087676525116}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.030,0.065 246/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.017,0.032 502/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.380 (0.374)  Time: 0.014,0.026 626/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.359 (0.371)  Time: 0.012,0.023 694/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.372 (0.370)  Time: 0.012,0.022 736/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.352 (0.368)  Time: 0.012,0.021 763/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.358 (0.367)  Time: 0.011,0.020 794/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.364 (0.366)  Time: 0.011,0.019 827/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.343 (0.365)  Time: 0.011,0.019 853/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.355 (0.365)  Time: 0.010,0.018 875/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.355 (0.364)  Time: 0.010,0.018 888/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.350 (0.363)  Time: 0.010,0.018 904/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.351 (0.363)  Time: 0.010,0.017 916/s  Mem: 0.31,0.34 GB\n",
      "Val:   0  Loss: 0.358  Acc: 0.149,0.162,0.170,0.147 (0.157)  Time: 0.005,0.007 150/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.359 (0.356)  Time: 0.012,0.021 756/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.359 (0.357)  Time: 0.011,0.019 832/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.350 (0.356)  Time: 0.010,0.018 878/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.359 (0.357)  Time: 0.010,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.359 (0.358)  Time: 0.010,0.017 915/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.339 (0.357)  Time: 0.010,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.357 (0.357)  Time: 0.010,0.017 942/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.358 (0.357)  Time: 0.009,0.017 950/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.348 (0.356)  Time: 0.010,0.017 949/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.359 (0.357)  Time: 0.009,0.017 968/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.358 (0.356)  Time: 0.009,0.016 992/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.356 (0.356)  Time: 0.009,0.016 1025/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.356  Acc: 0.164,0.179,0.187,0.160 (0.173)  Time: 0.003,0.005 205/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.353 (0.353)  Time: 0.010,0.026 614/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.367 (0.356)  Time: 0.010,0.019 852/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.361 (0.357)  Time: 0.009,0.017 944/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.343 (0.356)  Time: 0.009,0.016 988/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.358 (0.356)  Time: 0.009,0.016 987/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.354 (0.356)  Time: 0.009,0.016 1014/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.360 (0.356)  Time: 0.009,0.016 1032/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.360 (0.356)  Time: 0.008,0.015 1044/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.344 (0.356)  Time: 0.008,0.015 1053/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.361 (0.356)  Time: 0.008,0.015 1060/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.355 (0.356)  Time: 0.008,0.015 1067/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.347 (0.355)  Time: 0.008,0.015 1081/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.361 (0.356)  Time: 0.008,0.015 1083/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.355  Acc: 0.170,0.186,0.193,0.166 (0.179)  Time: 0.005,0.006 158/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.361 (0.352)  Time: 0.011,0.022 743/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.353 (0.351)  Time: 0.009,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.347 (0.353)  Time: 0.009,0.017 952/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.353 (0.353)  Time: 0.009,0.016 975/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.368 (0.354)  Time: 0.009,0.016 986/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.356 (0.354)  Time: 0.009,0.016 1026/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.357 (0.353)  Time: 0.009,0.015 1045/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.351 (0.353)  Time: 0.009,0.016 1024/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.356 (0.354)  Time: 0.009,0.016 1026/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.359 (0.354)  Time: 0.009,0.015 1036/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.358 (0.354)  Time: 0.009,0.015 1056/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.357 (0.354)  Time: 0.009,0.015 1070/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.355  Acc: 0.173,0.190,0.196,0.170 (0.182)  Time: 0.004,0.006 163/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.361 (0.361)  Time: 0.011,0.027 593/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.352 (0.354)  Time: 0.011,0.021 771/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.352 (0.353)  Time: 0.011,0.020 808/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.346 (0.353)  Time: 0.010,0.019 863/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.361 (0.353)  Time: 0.010,0.018 897/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.348 (0.353)  Time: 0.010,0.017 925/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.351 (0.353)  Time: 0.011,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.340 (0.353)  Time: 0.010,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.347 (0.353)  Time: 0.010,0.018 911/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.358 (0.352)  Time: 0.010,0.018 914/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.358 (0.352)  Time: 0.010,0.017 919/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.354 (0.353)  Time: 0.010,0.017 925/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.353 (0.353)  Time: 0.010,0.017 933/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.354  Acc: 0.178,0.192,0.199,0.173 (0.185)  Time: 0.005,0.006 157/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.352 (0.349)  Time: 0.013,0.025 641/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.354 (0.351)  Time: 0.010,0.019 831/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.358 (0.352)  Time: 0.010,0.018 886/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.351 (0.352)  Time: 0.012,0.020 809/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.353 (0.352)  Time: 0.012,0.020 796/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.350 (0.353)  Time: 0.012,0.020 808/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.358 (0.352)  Time: 0.012,0.019 822/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.347 (0.352)  Time: 0.011,0.019 841/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.360 (0.352)  Time: 0.011,0.019 854/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.348 (0.351)  Time: 0.011,0.019 859/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.348 (0.352)  Time: 0.011,0.018 867/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.356 (0.352)  Time: 0.011,0.018 876/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.354  Acc: 0.177,0.193,0.200,0.172 (0.186)  Time: 0.005,0.007 145/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.373 (0.373)  Time: 0.011,0.025 633/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.354 (0.351)  Time: 0.011,0.019 846/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.346 (0.353)  Time: 0.011,0.019 857/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.342 (0.353)  Time: 0.010,0.018 913/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.334 (0.353)  Time: 0.010,0.017 937/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.362 (0.352)  Time: 0.010,0.017 956/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.355 (0.352)  Time: 0.010,0.016 981/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.342 (0.352)  Time: 0.009,0.016 993/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.342 (0.352)  Time: 0.009,0.016 984/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.345 (0.352)  Time: 0.009,0.016 994/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.341 (0.352)  Time: 0.009,0.016 998/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.365 (0.352)  Time: 0.009,0.016 1013/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.362 (0.352)  Time: 0.009,0.016 1027/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.354  Acc: 0.177,0.194,0.199,0.173 (0.186)  Time: 0.004,0.006 173/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.345 (0.351)  Time: 0.015,0.031 524/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.359 (0.351)  Time: 0.012,0.023 687/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.361 (0.352)  Time: 0.011,0.021 755/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.357 (0.352)  Time: 0.011,0.020 794/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.339 (0.352)  Time: 0.011,0.020 800/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.353 (0.352)  Time: 0.011,0.020 809/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.353 (0.352)  Time: 0.011,0.019 821/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.356 (0.352)  Time: 0.011,0.019 830/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.337 (0.352)  Time: 0.011,0.019 841/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.352 (0.352)  Time: 0.011,0.019 842/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.356 (0.352)  Time: 0.011,0.019 830/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.361 (0.352)  Time: 0.011,0.019 832/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.353  Acc: 0.180,0.195,0.201,0.174 (0.188)  Time: 0.006,0.007 136/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.348 (0.348)  Time: 0.012,0.031 521/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.349 (0.349)  Time: 0.013,0.024 658/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.345 (0.351)  Time: 0.011,0.019 823/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.337 (0.352)  Time: 0.010,0.017 930/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.353 (0.352)  Time: 0.009,0.016 993/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.348 (0.351)  Time: 0.009,0.016 1028/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.350 (0.351)  Time: 0.009,0.015 1058/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.344 (0.351)  Time: 0.008,0.015 1101/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.345 (0.351)  Time: 0.008,0.014 1124/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.356 (0.351)  Time: 0.008,0.014 1137/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.360 (0.351)  Time: 0.008,0.014 1171/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.340 (0.351)  Time: 0.008,0.013 1193/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.353 (0.351)  Time: 0.008,0.013 1224/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.354  Acc: 0.175,0.194,0.197,0.170 (0.184)  Time: 0.004,0.005 210/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.349 (0.350)  Time: 0.011,0.020 790/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.346 (0.351)  Time: 0.009,0.017 933/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.351 (0.350)  Time: 0.009,0.016 1029/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.346 (0.349)  Time: 0.009,0.015 1047/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.342 (0.348)  Time: 0.008,0.015 1072/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.350 (0.349)  Time: 0.009,0.015 1080/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.345 (0.350)  Time: 0.008,0.015 1090/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.347 (0.350)  Time: 0.008,0.015 1090/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.349 (0.350)  Time: 0.008,0.015 1099/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.343 (0.350)  Time: 0.008,0.015 1092/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.355 (0.350)  Time: 0.009,0.015 1086/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.337 (0.350)  Time: 0.009,0.015 1060/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.354  Acc: 0.178,0.195,0.199,0.170 (0.185)  Time: 0.005,0.007 143/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"short\", \"window\": 32, \"summary\": false, \"layer\": \"model.layers.11\", \"acc\": 0.1853727251291275}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.019,0.051 316/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.367 (0.375)  Time: 0.017,0.032 499/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.381 (0.375)  Time: 0.015,0.028 572/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.360 (0.372)  Time: 0.013,0.024 654/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.373 (0.371)  Time: 0.012,0.022 713/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.353 (0.369)  Time: 0.012,0.021 757/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.359 (0.368)  Time: 0.011,0.020 785/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.364 (0.367)  Time: 0.011,0.020 812/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.343 (0.366)  Time: 0.011,0.019 839/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.355 (0.365)  Time: 0.010,0.018 871/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.356 (0.365)  Time: 0.010,0.018 899/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.350 (0.364)  Time: 0.010,0.017 920/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.351 (0.364)  Time: 0.010,0.017 930/s  Mem: 0.31,0.34 GB\n",
      "Val:   0  Loss: 0.359  Acc: 0.147,0.161,0.167,0.144 (0.155)  Time: 0.005,0.006 160/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.361 (0.357)  Time: 0.012,0.026 624/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.360 (0.358)  Time: 0.011,0.021 751/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.351 (0.356)  Time: 0.010,0.019 827/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.360 (0.358)  Time: 0.010,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.360 (0.359)  Time: 0.010,0.018 884/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.340 (0.358)  Time: 0.010,0.018 883/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.357 (0.358)  Time: 0.010,0.018 891/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.359 (0.357)  Time: 0.010,0.018 901/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.348 (0.357)  Time: 0.010,0.018 904/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.360 (0.357)  Time: 0.010,0.018 911/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.359 (0.357)  Time: 0.010,0.018 912/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.356 (0.357)  Time: 0.010,0.017 920/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.357  Acc: 0.159,0.175,0.182,0.156 (0.168)  Time: 0.005,0.007 143/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.354 (0.354)  Time: 0.008,0.023 685/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.367 (0.357)  Time: 0.009,0.017 947/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.362 (0.357)  Time: 0.008,0.016 990/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.344 (0.357)  Time: 0.008,0.016 1016/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.360 (0.357)  Time: 0.008,0.015 1050/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.355 (0.357)  Time: 0.008,0.015 1079/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.361 (0.357)  Time: 0.008,0.015 1089/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.361 (0.357)  Time: 0.008,0.014 1109/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.345 (0.357)  Time: 0.008,0.014 1128/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.362 (0.357)  Time: 0.008,0.014 1130/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.356 (0.357)  Time: 0.008,0.014 1105/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.348 (0.356)  Time: 0.008,0.014 1121/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.362 (0.357)  Time: 0.008,0.014 1140/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.356  Acc: 0.163,0.180,0.187,0.159 (0.172)  Time: 0.005,0.006 161/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.361 (0.353)  Time: 0.010,0.018 872/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.354 (0.352)  Time: 0.009,0.016 984/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.349 (0.354)  Time: 0.009,0.016 1017/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.354 (0.354)  Time: 0.008,0.015 1044/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.367 (0.355)  Time: 0.008,0.015 1054/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.356 (0.355)  Time: 0.008,0.015 1061/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.357 (0.354)  Time: 0.008,0.015 1051/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.353 (0.354)  Time: 0.009,0.015 1045/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.357 (0.355)  Time: 0.008,0.015 1069/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.359 (0.355)  Time: 0.008,0.015 1084/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.359 (0.355)  Time: 0.009,0.015 1056/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.357 (0.354)  Time: 0.009,0.015 1058/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.356  Acc: 0.167,0.184,0.191,0.164 (0.177)  Time: 0.005,0.006 155/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.362 (0.362)  Time: 0.009,0.023 695/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.353 (0.355)  Time: 0.010,0.019 840/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.353 (0.354)  Time: 0.009,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.347 (0.354)  Time: 0.009,0.017 937/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.360 (0.354)  Time: 0.009,0.017 946/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.350 (0.354)  Time: 0.009,0.017 942/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.352 (0.354)  Time: 0.009,0.017 939/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.342 (0.354)  Time: 0.009,0.017 941/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.348 (0.353)  Time: 0.010,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.358 (0.353)  Time: 0.010,0.017 926/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.359 (0.353)  Time: 0.010,0.017 924/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.355 (0.354)  Time: 0.010,0.017 927/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.355 (0.354)  Time: 0.010,0.017 924/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.355  Acc: 0.172,0.187,0.195,0.167 (0.180)  Time: 0.006,0.007 135/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.351 (0.349)  Time: 0.014,0.027 597/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.355 (0.352)  Time: 0.012,0.022 741/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.359 (0.353)  Time: 0.011,0.020 816/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.351 (0.353)  Time: 0.011,0.019 835/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.353 (0.353)  Time: 0.011,0.019 852/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.352 (0.353)  Time: 0.010,0.018 895/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.358 (0.353)  Time: 0.010,0.017 933/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.347 (0.353)  Time: 0.009,0.017 957/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.361 (0.353)  Time: 0.009,0.016 981/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.348 (0.352)  Time: 0.009,0.016 1016/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.351 (0.352)  Time: 0.009,0.015 1036/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.357 (0.353)  Time: 0.009,0.015 1049/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.355  Acc: 0.171,0.188,0.195,0.167 (0.180)  Time: 0.005,0.006 155/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.375 (0.375)  Time: 0.011,0.028 569/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.355 (0.352)  Time: 0.010,0.019 837/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.346 (0.353)  Time: 0.009,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.342 (0.353)  Time: 0.009,0.017 962/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.335 (0.353)  Time: 0.009,0.016 1010/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.362 (0.353)  Time: 0.008,0.015 1050/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.356 (0.353)  Time: 0.008,0.015 1067/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.344 (0.353)  Time: 0.009,0.015 1065/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.343 (0.353)  Time: 0.009,0.015 1056/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.347 (0.353)  Time: 0.009,0.015 1054/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.341 (0.353)  Time: 0.009,0.015 1061/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.366 (0.353)  Time: 0.009,0.015 1060/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.364 (0.353)  Time: 0.009,0.015 1062/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.355  Acc: 0.171,0.189,0.194,0.167 (0.180)  Time: 0.005,0.006 158/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.345 (0.352)  Time: 0.012,0.022 730/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.362 (0.352)  Time: 0.010,0.019 852/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.362 (0.353)  Time: 0.010,0.018 913/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.359 (0.353)  Time: 0.010,0.017 917/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.339 (0.353)  Time: 0.010,0.017 926/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.355 (0.353)  Time: 0.011,0.019 865/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.354 (0.353)  Time: 0.011,0.018 879/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.357 (0.353)  Time: 0.010,0.018 886/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.337 (0.353)  Time: 0.010,0.018 892/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.353 (0.353)  Time: 0.010,0.018 896/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.357 (0.353)  Time: 0.010,0.018 896/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.364 (0.352)  Time: 0.010,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.355  Acc: 0.174,0.190,0.196,0.169 (0.182)  Time: 0.005,0.007 141/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.349 (0.349)  Time: 0.010,0.026 610/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.350 (0.351)  Time: 0.010,0.020 802/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.345 (0.352)  Time: 0.009,0.018 898/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.338 (0.353)  Time: 0.009,0.017 957/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.353 (0.353)  Time: 0.009,0.016 990/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.348 (0.352)  Time: 0.009,0.016 1013/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.350 (0.352)  Time: 0.009,0.016 1027/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.344 (0.352)  Time: 0.009,0.016 1031/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.345 (0.352)  Time: 0.009,0.016 1022/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.357 (0.352)  Time: 0.009,0.016 1023/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.362 (0.352)  Time: 0.009,0.015 1040/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.342 (0.352)  Time: 0.009,0.015 1052/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.354 (0.351)  Time: 0.009,0.015 1061/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.356  Acc: 0.170,0.188,0.192,0.165 (0.179)  Time: 0.005,0.007 149/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.350 (0.351)  Time: 0.011,0.021 776/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.348 (0.351)  Time: 0.010,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.353 (0.351)  Time: 0.009,0.017 956/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.347 (0.350)  Time: 0.009,0.016 976/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.343 (0.349)  Time: 0.009,0.016 987/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.351 (0.350)  Time: 0.009,0.016 984/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.345 (0.351)  Time: 0.009,0.016 996/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.347 (0.351)  Time: 0.009,0.016 1001/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.350 (0.351)  Time: 0.009,0.016 980/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.344 (0.351)  Time: 0.009,0.016 977/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.357 (0.351)  Time: 0.009,0.016 977/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.337 (0.351)  Time: 0.009,0.016 972/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.355  Acc: 0.172,0.189,0.192,0.164 (0.179)  Time: 0.005,0.007 143/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"short\", \"window\": 32, \"summary\": false, \"layer\": \"model.layers.15\", \"acc\": 0.17925724387168884}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.030,0.057 280/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.019,0.034 475/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.379 (0.374)  Time: 0.016,0.028 580/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.357 (0.371)  Time: 0.014,0.025 650/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.369 (0.369)  Time: 0.013,0.023 695/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.350 (0.366)  Time: 0.013,0.022 727/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.354 (0.365)  Time: 0.012,0.021 747/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.361 (0.364)  Time: 0.012,0.021 767/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.339 (0.363)  Time: 0.012,0.021 780/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.352 (0.362)  Time: 0.012,0.020 783/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.351 (0.362)  Time: 0.012,0.020 795/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.346 (0.361)  Time: 0.012,0.020 802/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.346 (0.360)  Time: 0.012,0.020 807/s  Mem: 0.31,0.34 GB\n",
      "Val:   0  Loss: 0.354  Acc: 0.172,0.189,0.194,0.168 (0.181)  Time: 0.006,0.007 135/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.356 (0.353)  Time: 0.015,0.029 546/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.356 (0.353)  Time: 0.012,0.024 676/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.344 (0.352)  Time: 0.011,0.021 760/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.356 (0.353)  Time: 0.011,0.020 791/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.356 (0.354)  Time: 0.011,0.020 791/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.335 (0.353)  Time: 0.011,0.020 816/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.351 (0.353)  Time: 0.011,0.019 833/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.355 (0.353)  Time: 0.011,0.019 850/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.343 (0.352)  Time: 0.010,0.019 857/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.354 (0.352)  Time: 0.010,0.018 866/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.352 (0.352)  Time: 0.010,0.018 877/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.352 (0.352)  Time: 0.010,0.018 884/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.352  Acc: 0.185,0.203,0.207,0.181 (0.194)  Time: 0.005,0.007 149/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.349 (0.349)  Time: 0.010,0.024 678/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.361 (0.352)  Time: 0.009,0.017 930/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.357 (0.352)  Time: 0.008,0.016 1021/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.339 (0.352)  Time: 0.008,0.015 1067/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.352 (0.352)  Time: 0.008,0.015 1091/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.350 (0.351)  Time: 0.008,0.014 1104/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.356 (0.352)  Time: 0.008,0.014 1115/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.354 (0.352)  Time: 0.008,0.014 1116/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.339 (0.351)  Time: 0.009,0.015 1046/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.357 (0.352)  Time: 0.009,0.015 1053/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.349 (0.351)  Time: 0.009,0.015 1084/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.344 (0.351)  Time: 0.009,0.014 1108/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.356 (0.351)  Time: 0.008,0.014 1134/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.351  Acc: 0.190,0.209,0.212,0.187 (0.199)  Time: 0.004,0.005 194/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.357 (0.346)  Time: 0.013,0.026 604/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.348 (0.346)  Time: 0.011,0.021 753/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.342 (0.348)  Time: 0.011,0.020 799/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.348 (0.348)  Time: 0.011,0.019 833/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.362 (0.349)  Time: 0.010,0.019 858/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.350 (0.349)  Time: 0.010,0.019 864/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.352 (0.348)  Time: 0.010,0.018 888/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.347 (0.348)  Time: 0.010,0.018 905/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.351 (0.349)  Time: 0.010,0.017 926/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.354 (0.349)  Time: 0.010,0.017 936/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.353 (0.349)  Time: 0.009,0.017 945/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.352 (0.348)  Time: 0.009,0.017 952/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.351  Acc: 0.192,0.212,0.215,0.189 (0.202)  Time: 0.005,0.007 153/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.355 (0.355)  Time: 0.014,0.036 450/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.348 (0.349)  Time: 0.012,0.022 738/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.347 (0.348)  Time: 0.011,0.020 808/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.341 (0.348)  Time: 0.011,0.019 833/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.354 (0.348)  Time: 0.011,0.019 857/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.343 (0.348)  Time: 0.010,0.018 871/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.345 (0.348)  Time: 0.011,0.018 869/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.336 (0.348)  Time: 0.011,0.018 874/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.342 (0.347)  Time: 0.010,0.018 886/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.353 (0.347)  Time: 0.011,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.353 (0.347)  Time: 0.010,0.018 894/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.349 (0.348)  Time: 0.010,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.347 (0.348)  Time: 0.010,0.018 901/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.351  Acc: 0.194,0.212,0.215,0.189 (0.202)  Time: 0.006,0.007 137/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.347 (0.343)  Time: 0.010,0.021 757/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.349 (0.346)  Time: 0.009,0.017 941/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.351 (0.347)  Time: 0.009,0.016 993/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.346 (0.347)  Time: 0.009,0.016 1008/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.350 (0.347)  Time: 0.009,0.016 1030/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.345 (0.347)  Time: 0.009,0.015 1043/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.353 (0.347)  Time: 0.009,0.015 1036/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.341 (0.347)  Time: 0.009,0.016 1027/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.354 (0.347)  Time: 0.009,0.015 1037/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.342 (0.346)  Time: 0.009,0.015 1050/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.342 (0.346)  Time: 0.009,0.015 1073/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.352 (0.346)  Time: 0.008,0.015 1088/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.351  Acc: 0.193,0.213,0.216,0.189 (0.203)  Time: 0.004,0.006 166/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.365 (0.365)  Time: 0.011,0.023 681/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.348 (0.345)  Time: 0.009,0.018 887/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.341 (0.347)  Time: 0.008,0.016 992/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.335 (0.347)  Time: 0.008,0.015 1033/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.327 (0.347)  Time: 0.008,0.015 1058/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.357 (0.347)  Time: 0.008,0.015 1063/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.348 (0.347)  Time: 0.008,0.015 1081/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.338 (0.346)  Time: 0.010,0.017 959/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.337 (0.346)  Time: 0.010,0.017 955/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.339 (0.346)  Time: 0.010,0.017 950/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.334 (0.346)  Time: 0.010,0.017 953/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.358 (0.346)  Time: 0.010,0.017 950/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.357 (0.347)  Time: 0.010,0.017 950/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.351  Acc: 0.192,0.211,0.214,0.188 (0.201)  Time: 0.005,0.007 138/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.339 (0.345)  Time: 0.011,0.021 758/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.354 (0.345)  Time: 0.010,0.019 858/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.356 (0.347)  Time: 0.010,0.018 909/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.350 (0.346)  Time: 0.010,0.018 904/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.332 (0.346)  Time: 0.010,0.018 894/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.348 (0.346)  Time: 0.010,0.018 905/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.347 (0.346)  Time: 0.010,0.018 912/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.350 (0.346)  Time: 0.010,0.018 903/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.332 (0.346)  Time: 0.010,0.017 914/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.348 (0.346)  Time: 0.010,0.017 917/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.349 (0.346)  Time: 0.010,0.017 921/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.357 (0.346)  Time: 0.010,0.017 924/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.351  Acc: 0.192,0.210,0.212,0.186 (0.200)  Time: 0.005,0.007 138/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.343 (0.343)  Time: 0.011,0.032 499/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.343 (0.344)  Time: 0.014,0.024 657/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.342 (0.345)  Time: 0.013,0.022 738/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.332 (0.346)  Time: 0.012,0.020 792/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.347 (0.346)  Time: 0.012,0.020 818/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.342 (0.346)  Time: 0.011,0.019 839/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.344 (0.345)  Time: 0.011,0.019 851/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.340 (0.345)  Time: 0.011,0.019 860/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.341 (0.345)  Time: 0.011,0.018 866/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.351 (0.345)  Time: 0.011,0.018 875/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.355 (0.345)  Time: 0.011,0.018 880/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.334 (0.345)  Time: 0.011,0.018 885/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.347 (0.345)  Time: 0.011,0.018 884/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.352  Acc: 0.191,0.210,0.209,0.183 (0.198)  Time: 0.006,0.007 135/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.344 (0.344)  Time: 0.015,0.026 605/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.342 (0.345)  Time: 0.013,0.024 681/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.344 (0.344)  Time: 0.012,0.021 770/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.341 (0.343)  Time: 0.011,0.020 805/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.336 (0.342)  Time: 0.011,0.019 836/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.345 (0.343)  Time: 0.011,0.019 857/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.339 (0.344)  Time: 0.011,0.018 876/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.343 (0.344)  Time: 0.011,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.343 (0.344)  Time: 0.010,0.018 889/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.337 (0.344)  Time: 0.010,0.018 909/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.347 (0.344)  Time: 0.010,0.017 919/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.330 (0.344)  Time: 0.010,0.017 950/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.352  Acc: 0.191,0.209,0.210,0.183 (0.198)  Time: 0.004,0.005 193/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"long\", \"window\": 16, \"summary\": false, \"layer\": \"model.layers.7\", \"acc\": 0.19815652072429657}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.366 (0.366)  Time: 0.019,0.040 398/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.016,0.029 548/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.380 (0.374)  Time: 0.014,0.025 634/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.358 (0.371)  Time: 0.013,0.023 705/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.371 (0.369)  Time: 0.012,0.021 745/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.351 (0.367)  Time: 0.012,0.021 766/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.355 (0.366)  Time: 0.012,0.020 787/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.362 (0.365)  Time: 0.012,0.020 803/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.341 (0.364)  Time: 0.012,0.020 816/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.353 (0.363)  Time: 0.011,0.019 826/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.353 (0.363)  Time: 0.011,0.019 831/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.349 (0.362)  Time: 0.011,0.019 833/s  Mem: 0.31,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.348 (0.361)  Time: 0.011,0.019 833/s  Mem: 0.31,0.34 GB\n",
      "Val:   0  Loss: 0.355  Acc: 0.165,0.182,0.187,0.162 (0.174)  Time: 0.006,0.007 134/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.357 (0.354)  Time: 0.013,0.027 600/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.358 (0.355)  Time: 0.012,0.023 696/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.346 (0.353)  Time: 0.012,0.022 744/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.358 (0.355)  Time: 0.011,0.021 775/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.357 (0.356)  Time: 0.011,0.020 796/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.337 (0.355)  Time: 0.011,0.020 801/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.354 (0.355)  Time: 0.011,0.020 810/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.356 (0.354)  Time: 0.011,0.019 827/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.344 (0.354)  Time: 0.011,0.019 840/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.355 (0.354)  Time: 0.011,0.019 853/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.355 (0.354)  Time: 0.011,0.019 858/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.354 (0.354)  Time: 0.011,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.353  Acc: 0.179,0.197,0.201,0.174 (0.188)  Time: 0.005,0.007 140/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.350 (0.350)  Time: 0.015,0.039 410/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.364 (0.354)  Time: 0.014,0.026 604/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.359 (0.354)  Time: 0.012,0.022 721/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.340 (0.353)  Time: 0.012,0.020 788/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.354 (0.353)  Time: 0.011,0.020 806/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.351 (0.353)  Time: 0.011,0.020 808/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.357 (0.353)  Time: 0.011,0.019 827/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.356 (0.353)  Time: 0.011,0.019 842/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.341 (0.353)  Time: 0.011,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.358 (0.353)  Time: 0.011,0.018 873/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.352 (0.353)  Time: 0.010,0.018 888/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.345 (0.353)  Time: 0.010,0.018 899/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.358 (0.353)  Time: 0.010,0.018 904/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.352  Acc: 0.185,0.204,0.207,0.179 (0.194)  Time: 0.005,0.007 137/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.359 (0.349)  Time: 0.012,0.025 629/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.350 (0.348)  Time: 0.012,0.021 758/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.345 (0.350)  Time: 0.011,0.019 834/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.349 (0.350)  Time: 0.011,0.019 855/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.364 (0.351)  Time: 0.011,0.019 846/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.352 (0.351)  Time: 0.011,0.019 851/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.353 (0.350)  Time: 0.011,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.348 (0.350)  Time: 0.011,0.018 875/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.353 (0.350)  Time: 0.011,0.018 880/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.355 (0.350)  Time: 0.011,0.018 888/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.354 (0.350)  Time: 0.010,0.018 894/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.354 (0.350)  Time: 0.010,0.018 901/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.352  Acc: 0.187,0.207,0.209,0.181 (0.196)  Time: 0.006,0.007 138/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.357 (0.357)  Time: 0.010,0.028 567/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.349 (0.350)  Time: 0.009,0.017 943/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.349 (0.350)  Time: 0.009,0.016 985/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.342 (0.349)  Time: 0.009,0.016 1022/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.356 (0.350)  Time: 0.009,0.016 1027/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.346 (0.350)  Time: 0.009,0.016 989/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.348 (0.350)  Time: 0.010,0.016 984/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.338 (0.349)  Time: 0.010,0.016 984/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.344 (0.349)  Time: 0.010,0.016 977/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.354 (0.349)  Time: 0.010,0.017 969/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.354 (0.349)  Time: 0.010,0.017 967/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.352 (0.350)  Time: 0.010,0.016 970/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.350 (0.350)  Time: 0.010,0.016 971/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.352  Acc: 0.189,0.208,0.210,0.182 (0.197)  Time: 0.005,0.007 140/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.347 (0.345)  Time: 0.009,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.351 (0.348)  Time: 0.008,0.015 1045/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.353 (0.349)  Time: 0.008,0.015 1065/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.349 (0.348)  Time: 0.008,0.014 1109/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.349 (0.349)  Time: 0.008,0.014 1134/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.345 (0.349)  Time: 0.008,0.014 1123/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.353 (0.349)  Time: 0.009,0.015 1094/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.342 (0.349)  Time: 0.009,0.015 1093/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.356 (0.348)  Time: 0.008,0.014 1118/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.344 (0.348)  Time: 0.008,0.014 1149/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.344 (0.348)  Time: 0.008,0.014 1177/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.354 (0.348)  Time: 0.008,0.013 1200/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.352  Acc: 0.189,0.209,0.211,0.181 (0.197)  Time: 0.004,0.005 204/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.367 (0.367)  Time: 0.007,0.025 630/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.350 (0.347)  Time: 0.009,0.017 916/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.342 (0.349)  Time: 0.008,0.016 1019/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.338 (0.349)  Time: 0.008,0.015 1073/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.330 (0.349)  Time: 0.008,0.015 1094/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.357 (0.349)  Time: 0.008,0.014 1120/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.350 (0.348)  Time: 0.008,0.014 1124/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.339 (0.348)  Time: 0.008,0.014 1144/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.339 (0.348)  Time: 0.008,0.014 1136/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.342 (0.348)  Time: 0.008,0.014 1160/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.336 (0.348)  Time: 0.008,0.013 1198/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.361 (0.348)  Time: 0.007,0.013 1232/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.359 (0.349)  Time: 0.007,0.013 1269/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.353  Acc: 0.187,0.208,0.210,0.180 (0.196)  Time: 0.004,0.005 191/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.341 (0.348)  Time: 0.015,0.031 523/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.357 (0.347)  Time: 0.012,0.023 692/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.358 (0.349)  Time: 0.011,0.021 771/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.352 (0.348)  Time: 0.011,0.020 809/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.334 (0.348)  Time: 0.011,0.019 835/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.349 (0.348)  Time: 0.011,0.019 851/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.349 (0.348)  Time: 0.011,0.019 844/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.353 (0.348)  Time: 0.011,0.019 846/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.334 (0.348)  Time: 0.011,0.019 854/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.349 (0.348)  Time: 0.011,0.019 862/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.353 (0.348)  Time: 0.011,0.018 867/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.359 (0.348)  Time: 0.011,0.018 879/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.352  Acc: 0.187,0.207,0.207,0.178 (0.195)  Time: 0.006,0.007 137/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.344 (0.344)  Time: 0.008,0.022 735/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.347 (0.346)  Time: 0.010,0.018 873/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.343 (0.348)  Time: 0.009,0.017 949/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.334 (0.348)  Time: 0.009,0.017 966/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.348 (0.348)  Time: 0.009,0.016 999/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.344 (0.348)  Time: 0.009,0.016 1006/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.345 (0.348)  Time: 0.009,0.016 1018/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.341 (0.347)  Time: 0.009,0.016 1027/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.343 (0.347)  Time: 0.009,0.016 1026/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.351 (0.347)  Time: 0.009,0.016 1032/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.357 (0.347)  Time: 0.009,0.015 1046/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.337 (0.347)  Time: 0.009,0.015 1063/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.350 (0.347)  Time: 0.009,0.015 1069/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.352  Acc: 0.186,0.208,0.206,0.177 (0.194)  Time: 0.005,0.007 153/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.346 (0.347)  Time: 0.011,0.022 712/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.344 (0.347)  Time: 0.010,0.019 849/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.348 (0.346)  Time: 0.009,0.018 902/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.343 (0.345)  Time: 0.010,0.017 927/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.339 (0.344)  Time: 0.009,0.017 950/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.346 (0.345)  Time: 0.009,0.017 954/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.340 (0.346)  Time: 0.010,0.017 948/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.345 (0.346)  Time: 0.010,0.017 946/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.345 (0.346)  Time: 0.010,0.017 942/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.340 (0.346)  Time: 0.010,0.017 929/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.351 (0.346)  Time: 0.010,0.017 935/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.333 (0.346)  Time: 0.010,0.017 927/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.353  Acc: 0.187,0.207,0.207,0.177 (0.194)  Time: 0.005,0.007 137/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"long\", \"window\": 16, \"summary\": false, \"layer\": \"model.layers.11\", \"acc\": 0.19445377588272095}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.019,0.047 342/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.016,0.029 552/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.381 (0.375)  Time: 0.014,0.025 630/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.359 (0.372)  Time: 0.013,0.023 703/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.373 (0.370)  Time: 0.012,0.021 753/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.352 (0.368)  Time: 0.012,0.021 770/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.359 (0.367)  Time: 0.012,0.020 789/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.363 (0.366)  Time: 0.011,0.020 808/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.342 (0.365)  Time: 0.011,0.020 819/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.355 (0.365)  Time: 0.011,0.019 828/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.356 (0.364)  Time: 0.011,0.019 833/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.350 (0.364)  Time: 0.011,0.019 837/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.351 (0.363)  Time: 0.011,0.019 836/s  Mem: 0.30,0.34 GB\n",
      "Val:   0  Loss: 0.358  Acc: 0.150,0.166,0.172,0.146 (0.158)  Time: 0.006,0.008 128/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.359 (0.356)  Time: 0.014,0.027 600/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.360 (0.357)  Time: 0.012,0.022 743/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.349 (0.356)  Time: 0.011,0.020 796/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.360 (0.357)  Time: 0.011,0.019 832/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.360 (0.358)  Time: 0.011,0.019 849/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.339 (0.358)  Time: 0.011,0.019 857/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.357 (0.357)  Time: 0.010,0.018 876/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.359 (0.357)  Time: 0.010,0.018 875/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.347 (0.357)  Time: 0.011,0.018 875/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.358 (0.357)  Time: 0.010,0.018 879/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.357 (0.357)  Time: 0.010,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.357 (0.356)  Time: 0.010,0.018 902/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.356  Acc: 0.163,0.180,0.186,0.160 (0.172)  Time: 0.005,0.007 140/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.352 (0.352)  Time: 0.012,0.026 613/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.366 (0.356)  Time: 0.010,0.019 822/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.362 (0.357)  Time: 0.010,0.018 909/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.344 (0.356)  Time: 0.009,0.017 945/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.359 (0.356)  Time: 0.009,0.016 974/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.353 (0.356)  Time: 0.010,0.017 923/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.360 (0.356)  Time: 0.010,0.018 912/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.359 (0.356)  Time: 0.010,0.018 914/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.345 (0.356)  Time: 0.010,0.017 917/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.361 (0.356)  Time: 0.010,0.017 921/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.355 (0.356)  Time: 0.010,0.017 930/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.348 (0.356)  Time: 0.010,0.017 926/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.361 (0.356)  Time: 0.010,0.017 933/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.355  Acc: 0.170,0.188,0.194,0.165 (0.179)  Time: 0.005,0.006 166/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.362 (0.352)  Time: 0.010,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.354 (0.351)  Time: 0.009,0.017 943/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.348 (0.353)  Time: 0.009,0.016 1032/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.353 (0.353)  Time: 0.008,0.015 1090/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.367 (0.354)  Time: 0.008,0.014 1130/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.354 (0.354)  Time: 0.008,0.014 1147/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.356 (0.353)  Time: 0.008,0.014 1165/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.351 (0.353)  Time: 0.008,0.014 1147/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.356 (0.354)  Time: 0.008,0.014 1157/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.360 (0.354)  Time: 0.008,0.014 1171/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.357 (0.354)  Time: 0.008,0.014 1176/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.357 (0.353)  Time: 0.008,0.014 1177/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.354  Acc: 0.174,0.192,0.196,0.169 (0.183)  Time: 0.005,0.006 159/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.361 (0.361)  Time: 0.008,0.029 555/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.352 (0.354)  Time: 0.011,0.020 792/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.353 (0.353)  Time: 0.010,0.018 875/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.346 (0.353)  Time: 0.010,0.018 904/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.359 (0.353)  Time: 0.010,0.017 916/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.349 (0.353)  Time: 0.010,0.017 916/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.351 (0.353)  Time: 0.010,0.017 915/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.340 (0.353)  Time: 0.010,0.017 925/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.348 (0.353)  Time: 0.010,0.018 914/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.357 (0.352)  Time: 0.010,0.017 915/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.359 (0.352)  Time: 0.010,0.017 920/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.355 (0.353)  Time: 0.010,0.017 929/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.355 (0.353)  Time: 0.010,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.354  Acc: 0.175,0.194,0.197,0.171 (0.184)  Time: 0.005,0.006 159/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.351 (0.349)  Time: 0.011,0.021 768/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.354 (0.351)  Time: 0.010,0.018 889/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.357 (0.352)  Time: 0.010,0.017 919/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.352 (0.352)  Time: 0.010,0.017 938/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.353 (0.353)  Time: 0.010,0.017 950/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.349 (0.353)  Time: 0.010,0.017 953/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.358 (0.352)  Time: 0.010,0.017 966/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.347 (0.352)  Time: 0.010,0.017 931/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.360 (0.352)  Time: 0.010,0.017 928/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.347 (0.351)  Time: 0.010,0.017 933/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.348 (0.352)  Time: 0.010,0.017 938/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.357 (0.352)  Time: 0.010,0.017 937/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.354  Acc: 0.176,0.195,0.199,0.171 (0.185)  Time: 0.005,0.007 144/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.372 (0.372)  Time: 0.008,0.028 575/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.354 (0.351)  Time: 0.009,0.018 908/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.345 (0.353)  Time: 0.009,0.016 1007/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.342 (0.353)  Time: 0.009,0.015 1034/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.335 (0.352)  Time: 0.008,0.015 1049/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.362 (0.352)  Time: 0.008,0.015 1065/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.354 (0.352)  Time: 0.008,0.015 1077/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.343 (0.352)  Time: 0.008,0.015 1060/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.343 (0.352)  Time: 0.009,0.016 998/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.346 (0.352)  Time: 0.009,0.016 996/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.340 (0.352)  Time: 0.009,0.016 1002/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.366 (0.352)  Time: 0.009,0.016 1006/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.362 (0.352)  Time: 0.009,0.016 1004/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.354  Acc: 0.176,0.196,0.199,0.172 (0.186)  Time: 0.005,0.007 152/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.345 (0.351)  Time: 0.010,0.021 778/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.361 (0.351)  Time: 0.010,0.018 898/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.361 (0.352)  Time: 0.009,0.017 922/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.358 (0.352)  Time: 0.010,0.018 914/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.338 (0.352)  Time: 0.010,0.017 917/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.355 (0.352)  Time: 0.010,0.017 919/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.352 (0.352)  Time: 0.010,0.018 908/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.356 (0.352)  Time: 0.010,0.018 909/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.337 (0.352)  Time: 0.010,0.017 915/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.351 (0.352)  Time: 0.010,0.018 912/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.358 (0.352)  Time: 0.010,0.018 914/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.363 (0.352)  Time: 0.010,0.018 914/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.354  Acc: 0.176,0.196,0.199,0.172 (0.186)  Time: 0.006,0.007 138/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.347 (0.347)  Time: 0.013,0.035 452/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.350 (0.350)  Time: 0.013,0.024 670/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.345 (0.351)  Time: 0.011,0.021 761/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.337 (0.352)  Time: 0.011,0.020 806/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.351 (0.352)  Time: 0.011,0.020 809/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.347 (0.351)  Time: 0.011,0.020 820/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.349 (0.351)  Time: 0.011,0.019 837/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.344 (0.351)  Time: 0.011,0.019 858/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.346 (0.351)  Time: 0.011,0.018 868/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.354 (0.351)  Time: 0.011,0.018 878/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.361 (0.351)  Time: 0.011,0.018 884/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.341 (0.351)  Time: 0.011,0.018 887/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.353 (0.351)  Time: 0.011,0.018 893/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.354  Acc: 0.176,0.196,0.196,0.170 (0.184)  Time: 0.006,0.007 137/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.349 (0.351)  Time: 0.010,0.020 816/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.348 (0.351)  Time: 0.009,0.017 961/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.351 (0.350)  Time: 0.009,0.016 987/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.346 (0.349)  Time: 0.009,0.016 1017/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.343 (0.348)  Time: 0.009,0.016 1022/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.351 (0.349)  Time: 0.009,0.016 1015/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.344 (0.350)  Time: 0.009,0.016 999/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.349 (0.350)  Time: 0.009,0.016 999/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.349 (0.350)  Time: 0.009,0.016 1000/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.343 (0.350)  Time: 0.009,0.016 999/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.356 (0.350)  Time: 0.009,0.016 989/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.336 (0.350)  Time: 0.009,0.016 978/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.354  Acc: 0.177,0.196,0.198,0.171 (0.185)  Time: 0.005,0.007 147/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"long\", \"window\": 16, \"summary\": false, \"layer\": \"model.layers.15\", \"acc\": 0.18533802032470703}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.033,0.059 273/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.017,0.031 518/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.379 (0.374)  Time: 0.015,0.027 602/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.357 (0.371)  Time: 0.013,0.024 680/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.369 (0.369)  Time: 0.012,0.022 732/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.350 (0.366)  Time: 0.012,0.020 783/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.354 (0.365)  Time: 0.011,0.019 834/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.360 (0.364)  Time: 0.010,0.018 877/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.339 (0.363)  Time: 0.010,0.017 925/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.351 (0.362)  Time: 0.009,0.016 971/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.351 (0.362)  Time: 0.009,0.016 1011/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.346 (0.361)  Time: 0.009,0.015 1038/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.346 (0.360)  Time: 0.009,0.015 1064/s  Mem: 0.30,0.34 GB\n",
      "Val:   0  Loss: 0.354  Acc: 0.172,0.190,0.194,0.168 (0.181)  Time: 0.004,0.006 169/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.356 (0.353)  Time: 0.008,0.017 944/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.356 (0.353)  Time: 0.008,0.016 980/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.344 (0.352)  Time: 0.008,0.016 990/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.356 (0.353)  Time: 0.008,0.016 1013/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.356 (0.354)  Time: 0.008,0.016 1011/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.335 (0.353)  Time: 0.008,0.016 1010/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.351 (0.353)  Time: 0.009,0.016 985/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.355 (0.353)  Time: 0.009,0.016 978/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.343 (0.352)  Time: 0.009,0.016 973/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.354 (0.352)  Time: 0.009,0.016 972/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.352 (0.352)  Time: 0.009,0.017 962/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.352 (0.352)  Time: 0.009,0.017 958/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.352  Acc: 0.185,0.203,0.207,0.181 (0.194)  Time: 0.006,0.007 140/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.349 (0.349)  Time: 0.012,0.036 440/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.361 (0.352)  Time: 0.011,0.022 742/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.357 (0.352)  Time: 0.010,0.019 843/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.339 (0.352)  Time: 0.010,0.019 830/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.352 (0.352)  Time: 0.010,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.350 (0.351)  Time: 0.010,0.019 851/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.355 (0.351)  Time: 0.010,0.018 871/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.354 (0.352)  Time: 0.010,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.339 (0.351)  Time: 0.010,0.018 898/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.357 (0.351)  Time: 0.010,0.018 907/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.349 (0.351)  Time: 0.010,0.018 909/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.343 (0.351)  Time: 0.010,0.017 918/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.356 (0.351)  Time: 0.010,0.017 926/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.351  Acc: 0.190,0.210,0.213,0.186 (0.200)  Time: 0.005,0.007 138/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.357 (0.346)  Time: 0.012,0.025 631/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.348 (0.346)  Time: 0.011,0.020 789/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.342 (0.348)  Time: 0.010,0.019 828/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.347 (0.348)  Time: 0.010,0.019 858/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.362 (0.349)  Time: 0.010,0.018 869/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.349 (0.349)  Time: 0.010,0.018 872/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.351 (0.348)  Time: 0.010,0.018 883/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.346 (0.348)  Time: 0.010,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.351 (0.349)  Time: 0.010,0.018 897/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.354 (0.349)  Time: 0.010,0.018 898/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.353 (0.349)  Time: 0.010,0.018 903/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.352 (0.348)  Time: 0.010,0.018 908/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.351  Acc: 0.192,0.213,0.215,0.189 (0.202)  Time: 0.005,0.007 142/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.355 (0.355)  Time: 0.011,0.023 684/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.347 (0.349)  Time: 0.009,0.017 923/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.346 (0.348)  Time: 0.009,0.016 977/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.341 (0.348)  Time: 0.009,0.016 1002/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.354 (0.348)  Time: 0.009,0.016 1018/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.343 (0.348)  Time: 0.009,0.016 1014/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.345 (0.348)  Time: 0.009,0.016 1002/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.335 (0.348)  Time: 0.009,0.016 996/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.342 (0.347)  Time: 0.009,0.016 992/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.353 (0.347)  Time: 0.009,0.016 985/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.353 (0.347)  Time: 0.009,0.016 981/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.349 (0.348)  Time: 0.009,0.016 981/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.347 (0.348)  Time: 0.009,0.016 982/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.351  Acc: 0.194,0.213,0.215,0.189 (0.203)  Time: 0.005,0.006 161/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.346 (0.343)  Time: 0.011,0.022 742/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.349 (0.346)  Time: 0.010,0.018 895/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.351 (0.347)  Time: 0.010,0.017 929/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.346 (0.346)  Time: 0.009,0.017 966/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.350 (0.347)  Time: 0.009,0.016 977/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.344 (0.347)  Time: 0.009,0.016 986/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.353 (0.347)  Time: 0.009,0.016 982/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.341 (0.347)  Time: 0.009,0.017 948/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.354 (0.346)  Time: 0.010,0.017 938/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.342 (0.346)  Time: 0.010,0.017 938/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.342 (0.346)  Time: 0.010,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.352 (0.346)  Time: 0.010,0.017 921/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.351  Acc: 0.194,0.214,0.216,0.188 (0.203)  Time: 0.005,0.007 140/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.365 (0.365)  Time: 0.011,0.037 435/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.348 (0.345)  Time: 0.014,0.025 631/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.341 (0.347)  Time: 0.012,0.021 747/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.335 (0.347)  Time: 0.011,0.020 818/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.327 (0.347)  Time: 0.011,0.020 816/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.357 (0.347)  Time: 0.011,0.019 826/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.348 (0.347)  Time: 0.011,0.019 846/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.337 (0.346)  Time: 0.011,0.019 859/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.337 (0.346)  Time: 0.011,0.019 863/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.339 (0.346)  Time: 0.011,0.018 866/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.334 (0.346)  Time: 0.011,0.019 864/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.358 (0.346)  Time: 0.011,0.018 871/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.357 (0.347)  Time: 0.010,0.018 887/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.351  Acc: 0.192,0.212,0.214,0.188 (0.201)  Time: 0.005,0.007 143/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.339 (0.345)  Time: 0.013,0.025 647/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.354 (0.345)  Time: 0.012,0.021 746/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.356 (0.347)  Time: 0.011,0.020 808/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.350 (0.346)  Time: 0.011,0.019 850/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.332 (0.346)  Time: 0.010,0.018 880/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.347 (0.346)  Time: 0.010,0.018 891/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.347 (0.346)  Time: 0.010,0.017 916/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.349 (0.346)  Time: 0.010,0.017 943/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.332 (0.346)  Time: 0.010,0.017 961/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.348 (0.346)  Time: 0.009,0.017 969/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.348 (0.346)  Time: 0.009,0.016 988/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.356 (0.346)  Time: 0.009,0.016 1003/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.351  Acc: 0.192,0.211,0.211,0.186 (0.200)  Time: 0.005,0.008 130/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.344 (0.344)  Time: 0.011,0.026 622/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.342 (0.344)  Time: 0.010,0.019 827/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.342 (0.345)  Time: 0.009,0.017 918/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.333 (0.346)  Time: 0.009,0.017 949/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.347 (0.346)  Time: 0.009,0.016 996/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.342 (0.346)  Time: 0.009,0.016 1026/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.344 (0.345)  Time: 0.009,0.015 1046/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.340 (0.345)  Time: 0.008,0.015 1071/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.341 (0.345)  Time: 0.008,0.015 1102/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.350 (0.345)  Time: 0.008,0.014 1133/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.354 (0.345)  Time: 0.008,0.014 1161/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.334 (0.345)  Time: 0.008,0.014 1184/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.346 (0.345)  Time: 0.007,0.013 1208/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.352  Acc: 0.190,0.211,0.209,0.182 (0.198)  Time: 0.004,0.006 175/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.344 (0.344)  Time: 0.012,0.025 630/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.342 (0.345)  Time: 0.011,0.021 758/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.344 (0.344)  Time: 0.010,0.019 864/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.341 (0.343)  Time: 0.010,0.017 915/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.336 (0.342)  Time: 0.009,0.017 941/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.345 (0.343)  Time: 0.009,0.017 961/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.339 (0.344)  Time: 0.009,0.017 968/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.343 (0.344)  Time: 0.009,0.016 986/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.343 (0.344)  Time: 0.009,0.016 1007/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.337 (0.344)  Time: 0.009,0.016 1029/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.347 (0.344)  Time: 0.009,0.015 1049/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.330 (0.343)  Time: 0.009,0.015 1067/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.352  Acc: 0.190,0.210,0.210,0.182 (0.198)  Time: 0.005,0.006 159/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"long\", \"window\": 16, \"summary\": true, \"layer\": \"model.layers.7\", \"acc\": 0.19817279279232025}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.366 (0.366)  Time: 0.030,0.054 295/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.017,0.031 513/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.380 (0.374)  Time: 0.014,0.026 627/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.358 (0.371)  Time: 0.013,0.023 702/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.371 (0.369)  Time: 0.012,0.021 756/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.351 (0.367)  Time: 0.012,0.020 792/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.355 (0.366)  Time: 0.011,0.020 819/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.362 (0.365)  Time: 0.011,0.019 842/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.341 (0.364)  Time: 0.011,0.019 863/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.353 (0.363)  Time: 0.011,0.018 874/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.353 (0.363)  Time: 0.010,0.018 885/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.349 (0.362)  Time: 0.010,0.018 905/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.348 (0.362)  Time: 0.010,0.017 922/s  Mem: 0.30,0.34 GB\n",
      "Val:   0  Loss: 0.355  Acc: 0.165,0.182,0.187,0.161 (0.174)  Time: 0.005,0.007 150/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.357 (0.354)  Time: 0.013,0.027 602/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.358 (0.355)  Time: 0.011,0.022 726/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.346 (0.353)  Time: 0.011,0.021 756/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.358 (0.355)  Time: 0.011,0.020 788/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.357 (0.356)  Time: 0.011,0.020 811/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.337 (0.355)  Time: 0.011,0.019 825/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.354 (0.355)  Time: 0.011,0.019 834/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.357 (0.354)  Time: 0.011,0.019 846/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.344 (0.354)  Time: 0.010,0.019 847/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.355 (0.354)  Time: 0.011,0.019 846/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.355 (0.354)  Time: 0.011,0.019 845/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.354 (0.354)  Time: 0.010,0.019 847/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.353  Acc: 0.179,0.197,0.201,0.174 (0.188)  Time: 0.006,0.007 137/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.350 (0.350)  Time: 0.014,0.034 472/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.364 (0.354)  Time: 0.015,0.026 607/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.359 (0.354)  Time: 0.013,0.023 702/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.340 (0.353)  Time: 0.012,0.021 749/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.354 (0.353)  Time: 0.011,0.020 788/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.351 (0.353)  Time: 0.011,0.019 825/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.357 (0.353)  Time: 0.011,0.019 830/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.356 (0.353)  Time: 0.011,0.019 821/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.341 (0.353)  Time: 0.011,0.020 819/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.358 (0.353)  Time: 0.011,0.020 818/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.352 (0.353)  Time: 0.011,0.019 830/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.345 (0.352)  Time: 0.011,0.019 834/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.358 (0.353)  Time: 0.011,0.019 845/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.352  Acc: 0.185,0.205,0.208,0.179 (0.194)  Time: 0.005,0.007 145/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.359 (0.348)  Time: 0.015,0.030 531/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.350 (0.348)  Time: 0.014,0.025 643/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.345 (0.350)  Time: 0.013,0.023 705/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.349 (0.350)  Time: 0.014,0.023 690/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.364 (0.351)  Time: 0.013,0.022 734/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.352 (0.351)  Time: 0.012,0.021 768/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.352 (0.350)  Time: 0.012,0.020 781/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.348 (0.350)  Time: 0.012,0.020 801/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.353 (0.350)  Time: 0.012,0.020 815/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.355 (0.350)  Time: 0.011,0.019 825/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.354 (0.350)  Time: 0.011,0.019 843/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.354 (0.350)  Time: 0.011,0.019 857/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.352  Acc: 0.188,0.209,0.210,0.181 (0.197)  Time: 0.005,0.007 140/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.357 (0.357)  Time: 0.008,0.020 801/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.348 (0.350)  Time: 0.011,0.021 772/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.349 (0.350)  Time: 0.011,0.020 798/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.342 (0.349)  Time: 0.011,0.019 835/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.356 (0.350)  Time: 0.011,0.019 839/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.346 (0.350)  Time: 0.010,0.019 862/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.348 (0.350)  Time: 0.010,0.018 867/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.337 (0.349)  Time: 0.010,0.018 884/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.344 (0.349)  Time: 0.010,0.018 897/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.354 (0.349)  Time: 0.010,0.018 900/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.354 (0.349)  Time: 0.010,0.017 915/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.352 (0.349)  Time: 0.010,0.017 928/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.350 (0.350)  Time: 0.010,0.017 940/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.352  Acc: 0.190,0.209,0.210,0.182 (0.198)  Time: 0.004,0.005 189/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.347 (0.345)  Time: 0.008,0.014 1110/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.351 (0.348)  Time: 0.008,0.015 1102/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.353 (0.349)  Time: 0.010,0.017 949/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.348 (0.348)  Time: 0.010,0.017 968/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.349 (0.349)  Time: 0.009,0.016 1012/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.346 (0.349)  Time: 0.009,0.015 1037/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.354 (0.349)  Time: 0.009,0.015 1045/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.342 (0.349)  Time: 0.009,0.015 1043/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.356 (0.348)  Time: 0.009,0.015 1045/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.344 (0.348)  Time: 0.009,0.015 1046/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.344 (0.348)  Time: 0.009,0.015 1060/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.354 (0.348)  Time: 0.009,0.015 1064/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.352  Acc: 0.189,0.211,0.211,0.181 (0.198)  Time: 0.004,0.006 168/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.367 (0.367)  Time: 0.011,0.030 534/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.350 (0.347)  Time: 0.010,0.018 878/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.342 (0.349)  Time: 0.009,0.017 961/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.338 (0.349)  Time: 0.009,0.016 1002/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.331 (0.349)  Time: 0.009,0.016 998/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.357 (0.348)  Time: 0.009,0.016 996/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.350 (0.348)  Time: 0.009,0.017 941/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.339 (0.348)  Time: 0.010,0.018 905/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.339 (0.348)  Time: 0.010,0.018 904/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.341 (0.348)  Time: 0.010,0.018 895/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.336 (0.348)  Time: 0.010,0.018 890/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.361 (0.348)  Time: 0.010,0.018 886/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.359 (0.349)  Time: 0.010,0.018 886/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.352  Acc: 0.188,0.210,0.210,0.180 (0.197)  Time: 0.005,0.007 146/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.340 (0.347)  Time: 0.010,0.018 866/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.356 (0.347)  Time: 0.009,0.016 990/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.358 (0.348)  Time: 0.009,0.016 1024/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.352 (0.348)  Time: 0.008,0.015 1060/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.334 (0.348)  Time: 0.008,0.015 1078/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.349 (0.348)  Time: 0.008,0.015 1085/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.349 (0.348)  Time: 0.008,0.015 1055/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.352 (0.348)  Time: 0.009,0.016 1017/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.334 (0.348)  Time: 0.009,0.016 990/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.349 (0.348)  Time: 0.009,0.016 979/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.353 (0.348)  Time: 0.009,0.016 971/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.359 (0.348)  Time: 0.009,0.017 969/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.352  Acc: 0.187,0.208,0.207,0.178 (0.195)  Time: 0.005,0.007 136/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.344 (0.344)  Time: 0.011,0.031 510/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.346 (0.346)  Time: 0.011,0.022 712/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.343 (0.347)  Time: 0.011,0.021 773/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.334 (0.348)  Time: 0.011,0.020 791/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.348 (0.348)  Time: 0.011,0.020 815/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.343 (0.348)  Time: 0.011,0.020 812/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.345 (0.347)  Time: 0.011,0.020 812/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.341 (0.347)  Time: 0.011,0.019 829/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.343 (0.347)  Time: 0.011,0.019 836/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.351 (0.347)  Time: 0.011,0.019 843/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.356 (0.347)  Time: 0.011,0.019 854/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.337 (0.347)  Time: 0.011,0.019 859/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.349 (0.347)  Time: 0.011,0.019 861/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.352  Acc: 0.187,0.209,0.206,0.177 (0.195)  Time: 0.005,0.007 145/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.346 (0.347)  Time: 0.015,0.027 582/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.344 (0.347)  Time: 0.012,0.021 760/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.347 (0.346)  Time: 0.011,0.019 824/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.343 (0.345)  Time: 0.011,0.018 871/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.339 (0.344)  Time: 0.010,0.018 891/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.346 (0.345)  Time: 0.010,0.018 889/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.340 (0.346)  Time: 0.011,0.018 887/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.345 (0.346)  Time: 0.011,0.018 893/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.345 (0.346)  Time: 0.010,0.018 904/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.340 (0.346)  Time: 0.010,0.017 917/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.351 (0.346)  Time: 0.010,0.017 917/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.333 (0.346)  Time: 0.010,0.017 918/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.353  Acc: 0.187,0.209,0.207,0.178 (0.195)  Time: 0.005,0.007 142/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"long\", \"window\": 16, \"summary\": true, \"layer\": \"model.layers.11\", \"acc\": 0.19514086842536926}\n",
      "Model: MultiSubjectConvLinearEncoder(\n",
      "  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (feat_embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (shared_encoder): ConvLinear(\n",
      "    (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  )\n",
      "  (subject_encoders): ModuleList(\n",
      "    (0-3): 4 x ConvLinear(\n",
      "      (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=same, groups=256)\n",
      "      (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num params: 1.83M\n",
      "Train:   0 [  0/125][     0]  Loss: 0.367 (0.367)  Time: 0.038,0.070 227/s  Mem: 0.28,0.32 GB\n",
      "Train:   0 [ 10/125][    10]  Loss: 0.366 (0.375)  Time: 0.018,0.033 485/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 20/125][    20]  Loss: 0.380 (0.375)  Time: 0.015,0.028 573/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 30/125][    30]  Loss: 0.359 (0.371)  Time: 0.013,0.024 665/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 40/125][    40]  Loss: 0.373 (0.370)  Time: 0.012,0.022 730/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 50/125][    50]  Loss: 0.352 (0.368)  Time: 0.012,0.021 762/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 60/125][    60]  Loss: 0.359 (0.367)  Time: 0.011,0.020 793/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 70/125][    70]  Loss: 0.363 (0.366)  Time: 0.011,0.020 815/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 80/125][    80]  Loss: 0.343 (0.365)  Time: 0.011,0.019 839/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [ 90/125][    90]  Loss: 0.355 (0.365)  Time: 0.011,0.019 856/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [100/125][   100]  Loss: 0.356 (0.364)  Time: 0.010,0.018 872/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [110/125][   110]  Loss: 0.350 (0.363)  Time: 0.010,0.018 889/s  Mem: 0.30,0.34 GB\n",
      "Train:   0 [120/125][   120]  Loss: 0.351 (0.363)  Time: 0.010,0.018 903/s  Mem: 0.30,0.34 GB\n",
      "Val:   0  Loss: 0.358  Acc: 0.150,0.166,0.172,0.146 (0.159)  Time: 0.005,0.006 158/s\n",
      "Train:   1 [  5/125][   130]  Loss: 0.359 (0.356)  Time: 0.015,0.031 509/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 15/125][   140]  Loss: 0.360 (0.357)  Time: 0.012,0.024 679/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 25/125][   150]  Loss: 0.349 (0.356)  Time: 0.011,0.022 735/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 35/125][   160]  Loss: 0.360 (0.357)  Time: 0.011,0.021 771/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 45/125][   170]  Loss: 0.360 (0.358)  Time: 0.011,0.020 796/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 55/125][   180]  Loss: 0.339 (0.358)  Time: 0.011,0.020 817/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 65/125][   190]  Loss: 0.357 (0.357)  Time: 0.010,0.019 827/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 75/125][   200]  Loss: 0.359 (0.357)  Time: 0.011,0.020 814/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 85/125][   210]  Loss: 0.347 (0.356)  Time: 0.011,0.019 821/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [ 95/125][   220]  Loss: 0.358 (0.357)  Time: 0.011,0.019 829/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [105/125][   230]  Loss: 0.357 (0.357)  Time: 0.011,0.019 826/s  Mem: 0.30,0.33 GB\n",
      "Train:   1 [115/125][   240]  Loss: 0.357 (0.356)  Time: 0.010,0.019 842/s  Mem: 0.30,0.33 GB\n",
      "Val:   1  Loss: 0.356  Acc: 0.163,0.181,0.187,0.160 (0.173)  Time: 0.005,0.006 155/s\n",
      "Train:   2 [  0/125][   250]  Loss: 0.353 (0.353)  Time: 0.008,0.019 829/s  Mem: 0.29,0.31 GB\n",
      "Train:   2 [ 10/125][   260]  Loss: 0.366 (0.356)  Time: 0.008,0.017 965/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 20/125][   270]  Loss: 0.362 (0.357)  Time: 0.009,0.016 980/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 30/125][   280]  Loss: 0.344 (0.356)  Time: 0.009,0.016 979/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 40/125][   290]  Loss: 0.359 (0.356)  Time: 0.009,0.016 988/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 50/125][   300]  Loss: 0.353 (0.356)  Time: 0.009,0.016 1009/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 60/125][   310]  Loss: 0.360 (0.356)  Time: 0.009,0.016 1022/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 70/125][   320]  Loss: 0.359 (0.356)  Time: 0.009,0.016 1008/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 80/125][   330]  Loss: 0.345 (0.356)  Time: 0.009,0.016 1006/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [ 90/125][   340]  Loss: 0.362 (0.356)  Time: 0.009,0.016 1028/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [100/125][   350]  Loss: 0.355 (0.356)  Time: 0.009,0.015 1059/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [110/125][   360]  Loss: 0.348 (0.356)  Time: 0.008,0.015 1094/s  Mem: 0.30,0.33 GB\n",
      "Train:   2 [120/125][   370]  Loss: 0.361 (0.356)  Time: 0.008,0.014 1119/s  Mem: 0.30,0.33 GB\n",
      "Val:   2  Loss: 0.355  Acc: 0.170,0.189,0.194,0.166 (0.180)  Time: 0.004,0.005 187/s\n",
      "Train:   3 [  5/125][   380]  Loss: 0.362 (0.352)  Time: 0.011,0.021 759/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 15/125][   390]  Loss: 0.355 (0.351)  Time: 0.010,0.019 849/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 25/125][   400]  Loss: 0.348 (0.353)  Time: 0.010,0.018 908/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 35/125][   410]  Loss: 0.353 (0.353)  Time: 0.009,0.017 954/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 45/125][   420]  Loss: 0.367 (0.354)  Time: 0.009,0.016 977/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 55/125][   430]  Loss: 0.354 (0.354)  Time: 0.009,0.017 968/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 65/125][   440]  Loss: 0.356 (0.353)  Time: 0.009,0.016 981/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 75/125][   450]  Loss: 0.351 (0.353)  Time: 0.009,0.016 994/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 85/125][   460]  Loss: 0.356 (0.354)  Time: 0.009,0.016 979/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [ 95/125][   470]  Loss: 0.360 (0.354)  Time: 0.009,0.016 981/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [105/125][   480]  Loss: 0.357 (0.354)  Time: 0.009,0.016 986/s  Mem: 0.30,0.33 GB\n",
      "Train:   3 [115/125][   490]  Loss: 0.357 (0.353)  Time: 0.009,0.016 996/s  Mem: 0.30,0.33 GB\n",
      "Val:   3  Loss: 0.354  Acc: 0.174,0.192,0.197,0.169 (0.183)  Time: 0.005,0.006 158/s\n",
      "Train:   4 [  0/125][   500]  Loss: 0.361 (0.361)  Time: 0.013,0.027 593/s  Mem: 0.29,0.31 GB\n",
      "Train:   4 [ 10/125][   510]  Loss: 0.351 (0.354)  Time: 0.009,0.017 935/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 20/125][   520]  Loss: 0.353 (0.353)  Time: 0.009,0.017 968/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 30/125][   530]  Loss: 0.346 (0.353)  Time: 0.009,0.016 987/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 40/125][   540]  Loss: 0.360 (0.353)  Time: 0.009,0.016 994/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 50/125][   550]  Loss: 0.349 (0.353)  Time: 0.009,0.016 994/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 60/125][   560]  Loss: 0.351 (0.353)  Time: 0.009,0.016 987/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 70/125][   570]  Loss: 0.340 (0.353)  Time: 0.009,0.016 991/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 80/125][   580]  Loss: 0.348 (0.353)  Time: 0.010,0.016 979/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [ 90/125][   590]  Loss: 0.357 (0.352)  Time: 0.010,0.016 970/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [100/125][   600]  Loss: 0.359 (0.352)  Time: 0.010,0.016 971/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [110/125][   610]  Loss: 0.355 (0.353)  Time: 0.010,0.016 971/s  Mem: 0.30,0.33 GB\n",
      "Train:   4 [120/125][   620]  Loss: 0.355 (0.353)  Time: 0.010,0.017 964/s  Mem: 0.30,0.33 GB\n",
      "Val:   4  Loss: 0.354  Acc: 0.176,0.194,0.198,0.170 (0.184)  Time: 0.005,0.007 146/s\n",
      "Train:   5 [  5/125][   630]  Loss: 0.351 (0.349)  Time: 0.012,0.023 704/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 15/125][   640]  Loss: 0.354 (0.351)  Time: 0.011,0.019 847/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 25/125][   650]  Loss: 0.357 (0.352)  Time: 0.010,0.018 886/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 35/125][   660]  Loss: 0.352 (0.352)  Time: 0.010,0.017 917/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 45/125][   670]  Loss: 0.352 (0.352)  Time: 0.010,0.017 920/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 55/125][   680]  Loss: 0.349 (0.353)  Time: 0.010,0.017 932/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 65/125][   690]  Loss: 0.358 (0.352)  Time: 0.010,0.017 935/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 75/125][   700]  Loss: 0.346 (0.352)  Time: 0.010,0.017 946/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 85/125][   710]  Loss: 0.360 (0.352)  Time: 0.010,0.017 958/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [ 95/125][   720]  Loss: 0.347 (0.351)  Time: 0.010,0.017 966/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [105/125][   730]  Loss: 0.348 (0.352)  Time: 0.009,0.016 982/s  Mem: 0.30,0.33 GB\n",
      "Train:   5 [115/125][   740]  Loss: 0.357 (0.352)  Time: 0.009,0.016 989/s  Mem: 0.30,0.33 GB\n",
      "Val:   5  Loss: 0.354  Acc: 0.176,0.196,0.200,0.171 (0.186)  Time: 0.005,0.006 159/s\n",
      "Train:   6 [  0/125][   750]  Loss: 0.372 (0.372)  Time: 0.012,0.038 422/s  Mem: 0.29,0.31 GB\n",
      "Train:   6 [ 10/125][   760]  Loss: 0.354 (0.351)  Time: 0.013,0.025 652/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 20/125][   770]  Loss: 0.345 (0.353)  Time: 0.011,0.022 742/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 30/125][   780]  Loss: 0.341 (0.352)  Time: 0.011,0.020 781/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 40/125][   790]  Loss: 0.335 (0.352)  Time: 0.012,0.021 758/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 50/125][   800]  Loss: 0.361 (0.352)  Time: 0.012,0.021 759/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 60/125][   810]  Loss: 0.354 (0.352)  Time: 0.012,0.021 774/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 70/125][   820]  Loss: 0.343 (0.352)  Time: 0.012,0.020 789/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 80/125][   830]  Loss: 0.343 (0.352)  Time: 0.011,0.020 807/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [ 90/125][   840]  Loss: 0.346 (0.352)  Time: 0.011,0.019 821/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [100/125][   850]  Loss: 0.340 (0.352)  Time: 0.011,0.019 832/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [110/125][   860]  Loss: 0.365 (0.352)  Time: 0.011,0.019 840/s  Mem: 0.30,0.33 GB\n",
      "Train:   6 [120/125][   870]  Loss: 0.362 (0.352)  Time: 0.011,0.019 849/s  Mem: 0.30,0.33 GB\n",
      "Val:   6  Loss: 0.354  Acc: 0.177,0.196,0.200,0.172 (0.186)  Time: 0.005,0.007 143/s\n",
      "Train:   7 [  5/125][   880]  Loss: 0.344 (0.351)  Time: 0.009,0.017 954/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 15/125][   890]  Loss: 0.361 (0.351)  Time: 0.008,0.015 1047/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 25/125][   900]  Loss: 0.362 (0.352)  Time: 0.009,0.016 992/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 35/125][   910]  Loss: 0.358 (0.352)  Time: 0.009,0.016 999/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 45/125][   920]  Loss: 0.337 (0.352)  Time: 0.009,0.016 1018/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 55/125][   930]  Loss: 0.355 (0.352)  Time: 0.009,0.016 1003/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 65/125][   940]  Loss: 0.353 (0.352)  Time: 0.009,0.016 1006/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 75/125][   950]  Loss: 0.356 (0.352)  Time: 0.009,0.016 1011/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 85/125][   960]  Loss: 0.337 (0.352)  Time: 0.009,0.016 1010/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [ 95/125][   970]  Loss: 0.351 (0.352)  Time: 0.009,0.016 1012/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [105/125][   980]  Loss: 0.358 (0.352)  Time: 0.009,0.016 1025/s  Mem: 0.30,0.33 GB\n",
      "Train:   7 [115/125][   990]  Loss: 0.363 (0.352)  Time: 0.009,0.015 1034/s  Mem: 0.30,0.33 GB\n",
      "Val:   7  Loss: 0.354  Acc: 0.177,0.196,0.199,0.172 (0.186)  Time: 0.005,0.006 158/s\n",
      "Train:   8 [  0/125][  1000]  Loss: 0.347 (0.347)  Time: 0.019,0.041 388/s  Mem: 0.29,0.31 GB\n",
      "Train:   8 [ 10/125][  1010]  Loss: 0.350 (0.350)  Time: 0.015,0.028 576/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 20/125][  1020]  Loss: 0.345 (0.351)  Time: 0.013,0.025 648/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 30/125][  1030]  Loss: 0.337 (0.352)  Time: 0.012,0.023 702/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 40/125][  1040]  Loss: 0.351 (0.352)  Time: 0.012,0.022 735/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 50/125][  1050]  Loss: 0.347 (0.351)  Time: 0.012,0.022 742/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 60/125][  1060]  Loss: 0.349 (0.351)  Time: 0.012,0.021 745/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 70/125][  1070]  Loss: 0.344 (0.351)  Time: 0.012,0.022 744/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 80/125][  1080]  Loss: 0.345 (0.351)  Time: 0.012,0.021 747/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [ 90/125][  1090]  Loss: 0.355 (0.351)  Time: 0.012,0.021 754/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [100/125][  1100]  Loss: 0.361 (0.351)  Time: 0.012,0.022 724/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [110/125][  1110]  Loss: 0.342 (0.351)  Time: 0.012,0.022 732/s  Mem: 0.30,0.33 GB\n",
      "Train:   8 [120/125][  1120]  Loss: 0.353 (0.351)  Time: 0.012,0.022 735/s  Mem: 0.30,0.33 GB\n",
      "Val:   8  Loss: 0.354  Acc: 0.176,0.196,0.197,0.169 (0.185)  Time: 0.006,0.008 123/s\n",
      "Train:   9 [  5/125][  1130]  Loss: 0.349 (0.351)  Time: 0.015,0.027 584/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 15/125][  1140]  Loss: 0.348 (0.351)  Time: 0.014,0.026 620/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 25/125][  1150]  Loss: 0.351 (0.350)  Time: 0.013,0.024 673/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 35/125][  1160]  Loss: 0.346 (0.349)  Time: 0.013,0.023 709/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 45/125][  1170]  Loss: 0.342 (0.348)  Time: 0.012,0.022 721/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 55/125][  1180]  Loss: 0.351 (0.349)  Time: 0.012,0.022 732/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 65/125][  1190]  Loss: 0.344 (0.350)  Time: 0.012,0.022 742/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 75/125][  1200]  Loss: 0.348 (0.350)  Time: 0.012,0.021 756/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 85/125][  1210]  Loss: 0.348 (0.350)  Time: 0.012,0.020 782/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [ 95/125][  1220]  Loss: 0.343 (0.350)  Time: 0.011,0.020 801/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [105/125][  1230]  Loss: 0.356 (0.350)  Time: 0.011,0.020 815/s  Mem: 0.30,0.33 GB\n",
      "Train:   9 [115/125][  1240]  Loss: 0.336 (0.350)  Time: 0.011,0.019 830/s  Mem: 0.30,0.33 GB\n",
      "Val:   9  Loss: 0.354  Acc: 0.177,0.196,0.199,0.170 (0.185)  Time: 0.005,0.007 149/s\n",
      "{\"model\": \"meta-llama/Llama-3.2-1B\", \"context\": \"long\", \"window\": 16, \"summary\": true, \"layer\": \"model.layers.15\", \"acc\": 0.1853857934474945}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "cfgs = [\n",
    "    (\"short\", 16, False),\n",
    "    (\"short\", 32, False),\n",
    "    (\"long\", 16, False),\n",
    "    (\"long\", 16, True),\n",
    "]\n",
    "\n",
    "layers = [f\"model.layers.{ii}\" for ii in [7, 11, 15]]\n",
    "\n",
    "for context, window, summary in cfgs:\n",
    "    for layer in layers:\n",
    "        acc = run_experiment(\n",
    "            model_name=model_name,\n",
    "            layer=layer,\n",
    "            context=context,\n",
    "            window=window,\n",
    "            summary=summary,\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"context\": context,\n",
    "                \"window\": window,\n",
    "                \"summary\": summary,\n",
    "                \"layer\": layer,\n",
    "                \"acc\": acc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(json.dumps(results[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56351e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "23897f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d280dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f04b3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"index_name level0\" >layer</th>\n",
       "      <th id=\"T_f04b3_level0_col0\" class=\"col_heading level0 col0\" >model.layers.7</th>\n",
       "      <th id=\"T_f04b3_level0_col1\" class=\"col_heading level0 col1\" >model.layers.11</th>\n",
       "      <th id=\"T_f04b3_level0_col2\" class=\"col_heading level0 col2\" >model.layers.15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >context</th>\n",
       "      <th class=\"index_name level1\" >window</th>\n",
       "      <th class=\"index_name level2\" >summary</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f04b3_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"2\">long</th>\n",
       "      <th id=\"T_f04b3_level1_row0\" class=\"row_heading level1 row0\" rowspan=\"2\">16</th>\n",
       "      <th id=\"T_f04b3_level2_row0\" class=\"row_heading level2 row0\" >False</th>\n",
       "      <td id=\"T_f04b3_row0_col0\" class=\"data row0 col0\" >0.198</td>\n",
       "      <td id=\"T_f04b3_row0_col1\" class=\"data row0 col1\" >0.194</td>\n",
       "      <td id=\"T_f04b3_row0_col2\" class=\"data row0 col2\" >0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f04b3_level2_row1\" class=\"row_heading level2 row1\" >True</th>\n",
       "      <td id=\"T_f04b3_row1_col0\" class=\"data row1 col0\" >0.198</td>\n",
       "      <td id=\"T_f04b3_row1_col1\" class=\"data row1 col1\" >0.195</td>\n",
       "      <td id=\"T_f04b3_row1_col2\" class=\"data row1 col2\" >0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f04b3_level0_row2\" class=\"row_heading level0 row2\" rowspan=\"2\">short</th>\n",
       "      <th id=\"T_f04b3_level1_row2\" class=\"row_heading level1 row2\" >16</th>\n",
       "      <th id=\"T_f04b3_level2_row2\" class=\"row_heading level2 row2\" >False</th>\n",
       "      <td id=\"T_f04b3_row2_col0\" class=\"data row2 col0\" >0.175</td>\n",
       "      <td id=\"T_f04b3_row2_col1\" class=\"data row2 col1\" >0.178</td>\n",
       "      <td id=\"T_f04b3_row2_col2\" class=\"data row2 col2\" >0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f04b3_level1_row3\" class=\"row_heading level1 row3\" >32</th>\n",
       "      <th id=\"T_f04b3_level2_row3\" class=\"row_heading level2 row3\" >False</th>\n",
       "      <td id=\"T_f04b3_row3_col0\" class=\"data row3 col0\" >0.185</td>\n",
       "      <td id=\"T_f04b3_row3_col1\" class=\"data row3 col1\" >0.185</td>\n",
       "      <td id=\"T_f04b3_row3_col2\" class=\"data row3 col2\" >0.179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff9d8173a40>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table = results_df.iloc[:, 1:].pivot_table(\n",
    "    \"acc\", index=[\"context\", \"window\", \"summary\"], columns=[\"layer\"]\n",
    ")\n",
    "\n",
    "results_table = results_table.iloc[:, [2, 0, 1]]\n",
    "results_table.style.format(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a36904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
